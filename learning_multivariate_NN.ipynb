{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate regression to predict the wind speed field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "from metrics import MetricsLogger\n",
    "import metrics\n",
    "from data_loaders import get_wake_dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAFRAME_FILEPATH = \"data/discr_factors_x2_50_y-1_1_step0.125\"\n",
    "BEST_MODEL_PATH = \"models/multivariate_NN\"\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# hyperparameters\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 500\n",
    "LR = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_dataloader = None\n",
    "train_dataloader, valid_dataloader, test_dataloader = get_wake_dataloaders(DATAFRAME_FILEPATH,\n",
    "                                                          consider_ws=False,\n",
    "                                                          coords_as_input=False,\n",
    "                                                          train_perc=0.6,\n",
    "                                                          test_perc=0.2,\n",
    "                                                          validation_perc=0.2,\n",
    "                                                          batch_size=BATCH_SIZE)\n",
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 2]) torch.Size([8, 6144])\n"
     ]
    }
   ],
   "source": [
    "for b in train_dataloader:\n",
    "    print(b[0].shape, b[1].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_space=2\toutput_space=6144\n",
      "Logging MultivariateNN training\n",
      "Epoch 0 ->\tTraining loss (MSE)=0.00290805902360812\tTraining RMSE=0.05313171618790538\tValidation loss (MSE)=0.002095712135390689\tValidation RMSE=0.04296489611819938\t\n",
      "Epoch 1 ->\tTraining loss (MSE)=0.0015598001500396173\tTraining RMSE=0.03848772214288697\tValidation loss (MSE)=0.0014291675103811064\tValidation RMSE=0.03335208214681457\t\n",
      "Epoch 2 ->\tTraining loss (MSE)=0.0011504341768787851\tTraining RMSE=0.032570105218501004\tValidation loss (MSE)=0.0011844260249442111\tValidation RMSE=0.029095626229213342\t\n",
      "Epoch 3 ->\tTraining loss (MSE)=0.0009870681291723475\tTraining RMSE=0.029775888768289193\tValidation loss (MSE)=0.0010607242716273672\tValidation RMSE=0.02685109570760418\t\n",
      "Epoch 4 ->\tTraining loss (MSE)=0.0008944935964084127\tTraining RMSE=0.02818481348179005\tValidation loss (MSE)=0.0009774719841920564\tValidation RMSE=0.025470731969646835\t\n",
      "Epoch 5 ->\tTraining loss (MSE)=0.0008310464584127781\tTraining RMSE=0.027303486510559364\tValidation loss (MSE)=0.0009155520617948949\tValidation RMSE=0.024658302811008913\t\n",
      "Epoch 6 ->\tTraining loss (MSE)=0.0007838417999956607\tTraining RMSE=0.026558593266218164\tValidation loss (MSE)=0.0008674964096658018\tValidation RMSE=0.02393642695689643\t\n",
      "Epoch 7 ->\tTraining loss (MSE)=0.0007681975050083431\tTraining RMSE=0.025988634411291577\tValidation loss (MSE)=0.0008308062832737858\tValidation RMSE=0.02347535146745267\t\n",
      "Epoch 8 ->\tTraining loss (MSE)=0.0007190994318214642\tTraining RMSE=0.025475736500488386\tValidation loss (MSE)=0.0007961091518895786\tValidation RMSE=0.022963268147712504\t\n",
      "Epoch 9 ->\tTraining loss (MSE)=0.0006950470172644904\tTraining RMSE=0.02490075483320304\tValidation loss (MSE)=0.0007727853645283849\tValidation RMSE=0.022673778598093323\t\n",
      "Epoch 10 ->\tTraining loss (MSE)=0.0006894446416908191\tTraining RMSE=0.024826551505858885\tValidation loss (MSE)=0.0007538328052895075\tValidation RMSE=0.02245663044353326\t\n",
      "Epoch 11 ->\tTraining loss (MSE)=0.0006611089088746217\tTraining RMSE=0.024209536547358666\tValidation loss (MSE)=0.0007390991693817476\tValidation RMSE=0.022313087040351495\t\n",
      "Epoch 12 ->\tTraining loss (MSE)=0.0006521159882623455\tTraining RMSE=0.024074912065479123\tValidation loss (MSE)=0.0007253186290822837\tValidation RMSE=0.02212188020348549\t\n",
      "Epoch 13 ->\tTraining loss (MSE)=0.0006395249775205566\tTraining RMSE=0.023817201530169925\tValidation loss (MSE)=0.0007145082038025268\tValidation RMSE=0.021984259649697278\t\n",
      "Epoch 14 ->\tTraining loss (MSE)=0.0006307433078616691\tTraining RMSE=0.02384980205722429\tValidation loss (MSE)=0.0007062868105465357\tValidation RMSE=0.021850112463451095\t\n",
      "Epoch 15 ->\tTraining loss (MSE)=0.0006235878861192715\tTraining RMSE=0.023705299949434436\tValidation loss (MSE)=0.0006982247432461008\tValidation RMSE=0.021744701959606674\t\n",
      "Epoch 16 ->\tTraining loss (MSE)=0.000621846420898815\tTraining RMSE=0.023646774529306976\tValidation loss (MSE)=0.0006912706362695813\tValidation RMSE=0.021659708254177262\t\n",
      "Epoch 17 ->\tTraining loss (MSE)=0.0006280094672592334\tTraining RMSE=0.023643222465007392\tValidation loss (MSE)=0.0006843406344058975\tValidation RMSE=0.02156392169288463\t\n",
      "Epoch 18 ->\tTraining loss (MSE)=0.0006071996588470489\tTraining RMSE=0.023515990423613492\tValidation loss (MSE)=0.0006775159896211268\tValidation RMSE=0.021494136088424258\t\n",
      "Epoch 19 ->\tTraining loss (MSE)=0.0006031119909617521\tTraining RMSE=0.023302790582363987\tValidation loss (MSE)=0.0006735574601474649\tValidation RMSE=0.021408868160236766\t\n",
      "Epoch 20 ->\tTraining loss (MSE)=0.0006008646470698655\tTraining RMSE=0.023306955175221335\tValidation loss (MSE)=0.0006691842126774309\tValidation RMSE=0.021352771859340095\t\n",
      "Epoch 21 ->\tTraining loss (MSE)=0.0005945465655477838\tTraining RMSE=0.023034115166713793\tValidation loss (MSE)=0.000664335584722856\tValidation RMSE=0.021302622725704202\t\n",
      "Epoch 22 ->\tTraining loss (MSE)=0.0005913525342798999\tTraining RMSE=0.023134755459326653\tValidation loss (MSE)=0.0006609532899380645\tValidation RMSE=0.02122159295335964\t\n",
      "Epoch 23 ->\tTraining loss (MSE)=0.0005879979390657456\tTraining RMSE=0.023147495394503628\tValidation loss (MSE)=0.0006584412938671583\tValidation RMSE=0.02117536234220973\t\n",
      "Epoch 24 ->\tTraining loss (MSE)=0.0005847859858117716\tTraining RMSE=0.022833376685962265\tValidation loss (MSE)=0.0006532369549892826\tValidation RMSE=0.021112860404644853\t\n",
      "Epoch 25 ->\tTraining loss (MSE)=0.0005817199204035138\tTraining RMSE=0.02283784806912328\tValidation loss (MSE)=0.0006509953185471844\tValidation RMSE=0.021056525840389508\t\n",
      "Epoch 26 ->\tTraining loss (MSE)=0.0005809675268760601\tTraining RMSE=0.022943531398741918\tValidation loss (MSE)=0.000647649247252115\tValidation RMSE=0.02100810484477767\t\n",
      "Epoch 27 ->\tTraining loss (MSE)=0.000576639527373769\tTraining RMSE=0.02269940220831353\tValidation loss (MSE)=0.000644904240524974\tValidation RMSE=0.02095884289937439\t\n",
      "Epoch 28 ->\tTraining loss (MSE)=0.000574628589899397\tTraining RMSE=0.022688186192816054\tValidation loss (MSE)=0.0006423150278149276\tValidation RMSE=0.020909232294393912\t\n",
      "Epoch 29 ->\tTraining loss (MSE)=0.0005728715465168388\tTraining RMSE=0.022649131169151744\tValidation loss (MSE)=0.000639149769186689\tValidation RMSE=0.02085306899001201\t\n",
      "Epoch 30 ->\tTraining loss (MSE)=0.0005686938987276301\tTraining RMSE=0.022727952864779917\tValidation loss (MSE)=0.0006362949778256869\tValidation RMSE=0.02081051035956652\t\n",
      "Epoch 31 ->\tTraining loss (MSE)=0.0005680923457049052\tTraining RMSE=0.02272967008275934\tValidation loss (MSE)=0.0006345556989631236\tValidation RMSE=0.02077247473169808\t\n",
      "Epoch 32 ->\tTraining loss (MSE)=0.0005655591477629913\tTraining RMSE=0.022677070840641304\tValidation loss (MSE)=0.0006319594778890152\tValidation RMSE=0.020723999150235345\t\n",
      "Epoch 33 ->\tTraining loss (MSE)=0.0005682327475908635\tTraining RMSE=0.022535347750548412\tValidation loss (MSE)=0.0006292838083431383\tValidation RMSE=0.020673078812520812\t\n",
      "Epoch 34 ->\tTraining loss (MSE)=0.0005620078879507707\tTraining RMSE=0.022597887185344726\tValidation loss (MSE)=0.0006272171705387774\tValidation RMSE=0.020637955671797197\t\n",
      "Epoch 35 ->\tTraining loss (MSE)=0.0005597696797422818\tTraining RMSE=0.022364969589506036\tValidation loss (MSE)=0.00062507464624812\tValidation RMSE=0.020592989296548896\t\n",
      "Epoch 36 ->\tTraining loss (MSE)=0.000557083987385138\tTraining RMSE=0.022324858020623155\tValidation loss (MSE)=0.0006232025639123835\tValidation RMSE=0.02056239833365436\t\n",
      "Epoch 37 ->\tTraining loss (MSE)=0.0005559099507216295\tTraining RMSE=0.02260562485474863\tValidation loss (MSE)=0.0006210587423454315\tValidation RMSE=0.02052306192409661\t\n",
      "Epoch 38 ->\tTraining loss (MSE)=0.0005689488373909514\tTraining RMSE=0.022676628370435884\tValidation loss (MSE)=0.0006194669784033775\tValidation RMSE=0.02049067121482006\t\n",
      "Epoch 39 ->\tTraining loss (MSE)=0.0005715653069161402\tTraining RMSE=0.022360171616813283\tValidation loss (MSE)=0.0006173989628829683\tValidation RMSE=0.020455645966447063\t\n",
      "Epoch 40 ->\tTraining loss (MSE)=0.0005525552545872482\tTraining RMSE=0.022420206015216715\tValidation loss (MSE)=0.0006157089511462902\tValidation RMSE=0.02042823929891542\t\n",
      "Epoch 41 ->\tTraining loss (MSE)=0.0005489507174736642\tTraining RMSE=0.02221420249775236\tValidation loss (MSE)=0.0006143336874999848\tValidation RMSE=0.020398726428134575\t\n",
      "Epoch 42 ->\tTraining loss (MSE)=0.000547699741555671\tTraining RMSE=0.02216269418505239\tValidation loss (MSE)=0.0006130743158455179\tValidation RMSE=0.020365043888213457\t\n",
      "Epoch 43 ->\tTraining loss (MSE)=0.0005465947443879509\tTraining RMSE=0.022174944928674785\tValidation loss (MSE)=0.0006113693198130932\tValidation RMSE=0.020338407045023307\t\n",
      "Epoch 44 ->\tTraining loss (MSE)=0.0005457474251693103\tTraining RMSE=0.022097216528138997\tValidation loss (MSE)=0.0006107891957206583\tValidation RMSE=0.020314738482098887\t\n",
      "Epoch 45 ->\tTraining loss (MSE)=0.0005450457480307065\tTraining RMSE=0.022332999533341256\tValidation loss (MSE)=0.0006092878990144796\tValidation RMSE=0.020294013864326256\t\n",
      "Epoch 46 ->\tTraining loss (MSE)=0.0005432960158627176\tTraining RMSE=0.022057921884374485\tValidation loss (MSE)=0.0006081711839619351\tValidation RMSE=0.02027315379085916\t\n",
      "Epoch 47 ->\tTraining loss (MSE)=0.0005422373865538832\tTraining RMSE=0.02191295658445193\tValidation loss (MSE)=0.0006064357827805603\tValidation RMSE=0.020239020870239648\t\n",
      "Epoch 48 ->\tTraining loss (MSE)=0.0005425848046371234\tTraining RMSE=0.02205269956489864\tValidation loss (MSE)=0.0006052194477416817\tValidation RMSE=0.020219126492048854\t\n",
      "Epoch 49 ->\tTraining loss (MSE)=0.0005399494482754776\tTraining RMSE=0.02202225538591544\tValidation loss (MSE)=0.0006039989112415006\tValidation RMSE=0.020198394854863484\t\n",
      "Epoch 50 ->\tTraining loss (MSE)=0.0005395332352943822\tTraining RMSE=0.022117335958705273\tValidation loss (MSE)=0.0006025418202895067\tValidation RMSE=0.020168168211562768\t\n",
      "Epoch 51 ->\tTraining loss (MSE)=0.0005372255438296746\tTraining RMSE=0.022081598800457555\tValidation loss (MSE)=0.0006015098568039119\tValidation RMSE=0.02015580026501859\t\n",
      "Epoch 52 ->\tTraining loss (MSE)=0.0005383487108320193\tTraining RMSE=0.022026479410950418\tValidation loss (MSE)=0.0006005136956109372\tValidation RMSE=0.020146176149999653\t\n",
      "Epoch 53 ->\tTraining loss (MSE)=0.0005379942841601507\tTraining RMSE=0.021903619532369904\tValidation loss (MSE)=0.0005993949599980153\tValidation RMSE=0.020113809103215183\t\n",
      "Epoch 54 ->\tTraining loss (MSE)=0.000535868171779985\tTraining RMSE=0.02189648021472457\tValidation loss (MSE)=0.000598708183571472\tValidation RMSE=0.020099628768447373\t\n",
      "Epoch 55 ->\tTraining loss (MSE)=0.0005361444920798718\tTraining RMSE=0.0218770593278294\tValidation loss (MSE)=0.000598417702820842\tValidation RMSE=0.02011273956547181\t\n",
      "Epoch 56 ->\tTraining loss (MSE)=0.0005391990669769807\tTraining RMSE=0.02207710690807873\tValidation loss (MSE)=0.0005964297140760917\tValidation RMSE=0.020062244117811875\t\n",
      "Epoch 57 ->\tTraining loss (MSE)=0.0005320298471444941\tTraining RMSE=0.021954176303597143\tValidation loss (MSE)=0.0005955931052017992\tValidation RMSE=0.020045035122031415\t\n",
      "Epoch 58 ->\tTraining loss (MSE)=0.0005368604135672981\tTraining RMSE=0.022038419035343843\tValidation loss (MSE)=0.0005949436813858079\tValidation RMSE=0.020037429665939674\t\n",
      "Epoch 59 ->\tTraining loss (MSE)=0.0005502087504417566\tTraining RMSE=0.02219072892043142\tValidation loss (MSE)=0.0005938349917145118\tValidation RMSE=0.020010905857715342\t\n",
      "Epoch 60 ->\tTraining loss (MSE)=0.0005307667307073486\tTraining RMSE=0.02199386690892739\tValidation loss (MSE)=0.0005929217959756325\tValidation RMSE=0.01999981058071609\t\n",
      "Epoch 61 ->\tTraining loss (MSE)=0.0005307394692212211\tTraining RMSE=0.021885593993773246\tValidation loss (MSE)=0.0005920743034814088\tValidation RMSE=0.019984595667294883\t\n",
      "Epoch 62 ->\tTraining loss (MSE)=0.0005324084883380428\tTraining RMSE=0.021804562660601035\tValidation loss (MSE)=0.0005918233945084028\tValidation RMSE=0.01998978503117407\t\n",
      "Epoch 63 ->\tTraining loss (MSE)=0.0005282831686296655\tTraining RMSE=0.021713853606747255\tValidation loss (MSE)=0.0005911244997049733\tValidation RMSE=0.019960825210782112\t\n",
      "Epoch 64 ->\tTraining loss (MSE)=0.0005296735450238087\tTraining RMSE=0.02202024066498802\tValidation loss (MSE)=0.0005907801367154914\tValidation RMSE=0.01994716781363995\t\n",
      "Epoch 65 ->\tTraining loss (MSE)=0.0005278875225799928\tTraining RMSE=0.0218950272652746\tValidation loss (MSE)=0.0005896428953780776\tValidation RMSE=0.019936456276034867\t\n",
      "Epoch 66 ->\tTraining loss (MSE)=0.0005268439979570556\tTraining RMSE=0.021710194193148687\tValidation loss (MSE)=0.0005891623383191742\tValidation RMSE=0.01992849931465807\t\n",
      "Epoch 67 ->\tTraining loss (MSE)=0.00052596994835228\tTraining RMSE=0.021643559678377194\tValidation loss (MSE)=0.0005881286988032689\tValidation RMSE=0.019903497218533798\t\n",
      "Epoch 68 ->\tTraining loss (MSE)=0.0005263370792952763\tTraining RMSE=0.021836480712173163\tValidation loss (MSE)=0.000587363731028745\tValidation RMSE=0.019889696363221715\t\n",
      "Epoch 69 ->\tTraining loss (MSE)=0.000524429511485469\tTraining RMSE=0.02172840221060647\tValidation loss (MSE)=0.0005868924090464134\tValidation RMSE=0.019880211791368545\t\n",
      "Epoch 70 ->\tTraining loss (MSE)=0.0005245091135158738\tTraining RMSE=0.021764059930487915\tValidation loss (MSE)=0.0005863390705834954\tValidation RMSE=0.01986852248578712\t\n",
      "Epoch 71 ->\tTraining loss (MSE)=0.0005232708266097082\tTraining RMSE=0.021668627314890426\tValidation loss (MSE)=0.0005859538904149775\tValidation RMSE=0.019860058488255297\t\n",
      "Epoch 72 ->\tTraining loss (MSE)=0.0005232602182971917\tTraining RMSE=0.021603098248396024\tValidation loss (MSE)=0.0005854069770268527\tValidation RMSE=0.019849456760480447\t\n",
      "Epoch 73 ->\tTraining loss (MSE)=0.0005244263484874628\tTraining RMSE=0.021721498603806084\tValidation loss (MSE)=0.0005847481668213193\tValidation RMSE=0.019836144263131752\t\n",
      "Epoch 74 ->\tTraining loss (MSE)=0.0005217397388656132\tTraining RMSE=0.021746743686014303\tValidation loss (MSE)=0.000584254224425725\tValidation RMSE=0.019831139715043484\t\n",
      "Epoch 75 ->\tTraining loss (MSE)=0.0005272578845854438\tTraining RMSE=0.021784149018334753\tValidation loss (MSE)=0.0005837712672439886\tValidation RMSE=0.0198201986581639\t\n",
      "Epoch 76 ->\tTraining loss (MSE)=0.000522149636876916\tTraining RMSE=0.021618374508380153\tValidation loss (MSE)=0.0005837733799664959\tValidation RMSE=0.019820467413713534\t\n",
      "Epoch 77 ->\tTraining loss (MSE)=0.0005254905514942229\tTraining RMSE=0.0216439736397638\tValidation loss (MSE)=0.0005827463970086279\tValidation RMSE=0.01979923179304158\t\n",
      "Epoch 78 ->\tTraining loss (MSE)=0.000519944006341968\tTraining RMSE=0.021557234082497472\tValidation loss (MSE)=0.0005823918494301055\tValidation RMSE=0.019798028127600748\t\n",
      "Epoch 79 ->\tTraining loss (MSE)=0.0005196573564503916\tTraining RMSE=0.021633288650600997\tValidation loss (MSE)=0.0005813282080419378\tValidation RMSE=0.019780893223705114\t\n",
      "Epoch 80 ->\tTraining loss (MSE)=0.0005193513551839041\tTraining RMSE=0.021704399716798908\tValidation loss (MSE)=0.0005803171269589155\tValidation RMSE=0.019758698082080594\t\n",
      "Epoch 81 ->\tTraining loss (MSE)=0.0005189619221283342\tTraining RMSE=0.021592448683984487\tValidation loss (MSE)=0.0005797295342307296\tValidation RMSE=0.019747742037806246\t\n",
      "Epoch 82 ->\tTraining loss (MSE)=0.0005179417019485019\tTraining RMSE=0.02152285728327654\tValidation loss (MSE)=0.0005792642999343419\tValidation RMSE=0.019740254152566195\t\n",
      "Epoch 83 ->\tTraining loss (MSE)=0.0005181718640224174\tTraining RMSE=0.021592131516907687\tValidation loss (MSE)=0.0005785260386969808\tValidation RMSE=0.019727928046551015\t\n",
      "Epoch 84 ->\tTraining loss (MSE)=0.0005166674581019317\tTraining RMSE=0.021595817536437585\tValidation loss (MSE)=0.0005778453558849395\tValidation RMSE=0.019713601425152133\t\n",
      "Epoch 85 ->\tTraining loss (MSE)=0.0005185567711179582\tTraining RMSE=0.021723373419386738\tValidation loss (MSE)=0.000576990211104405\tValidation RMSE=0.019700044093446598\t\n",
      "Epoch 86 ->\tTraining loss (MSE)=0.0005160217705575454\tTraining RMSE=0.02155446362060805\tValidation loss (MSE)=0.0005764513966655444\tValidation RMSE=0.019685537450843386\t\n",
      "Epoch 87 ->\tTraining loss (MSE)=0.0005158683522528952\tTraining RMSE=0.021710081727324443\tValidation loss (MSE)=0.0005760305818209114\tValidation RMSE=0.01968494381893564\t\n",
      "Epoch 88 ->\tTraining loss (MSE)=0.0005140301258237472\tTraining RMSE=0.021446102178069176\tValidation loss (MSE)=0.0005752480062289612\tValidation RMSE=0.019666983901212614\t\n",
      "Epoch 89 ->\tTraining loss (MSE)=0.0005139131582252981\tTraining RMSE=0.021338103160859993\tValidation loss (MSE)=0.0005746521538067436\tValidation RMSE=0.019654078598789596\t\n",
      "Epoch 90 ->\tTraining loss (MSE)=0.0005151004020992759\tTraining RMSE=0.021383392860262113\tValidation loss (MSE)=0.0005741439746526777\tValidation RMSE=0.019643157237657794\t\n",
      "Epoch 91 ->\tTraining loss (MSE)=0.0005124556336352131\tTraining RMSE=0.021598056901568248\tValidation loss (MSE)=0.0005737269554326\tValidation RMSE=0.019638969735414895\t\n",
      "Epoch 92 ->\tTraining loss (MSE)=0.0005178951912428063\tTraining RMSE=0.021608199504560527\tValidation loss (MSE)=0.000573386483867782\tValidation RMSE=0.01962967382536994\t\n",
      "Epoch 93 ->\tTraining loss (MSE)=0.0005141841538716108\tTraining RMSE=0.021533221299587575\tValidation loss (MSE)=0.0005731300924607138\tValidation RMSE=0.01962186934219466\t\n",
      "Epoch 94 ->\tTraining loss (MSE)=0.000511702775434565\tTraining RMSE=0.021611481864741187\tValidation loss (MSE)=0.0005725868939879109\tValidation RMSE=0.019612473452946654\t\n",
      "Epoch 95 ->\tTraining loss (MSE)=0.000511638689743079\tTraining RMSE=0.021418653146858202\tValidation loss (MSE)=0.0005721723798530918\tValidation RMSE=0.019611785809199016\t\n",
      "Epoch 96 ->\tTraining loss (MSE)=0.0005111572015861676\tTraining RMSE=0.021471928322204837\tValidation loss (MSE)=0.0005717035921945892\tValidation RMSE=0.0196017078468921\t\n",
      "Epoch 97 ->\tTraining loss (MSE)=0.0005103872041936595\tTraining RMSE=0.021435943978298226\tValidation loss (MSE)=0.0005710669240754753\tValidation RMSE=0.019587186061673693\t\n",
      "Epoch 98 ->\tTraining loss (MSE)=0.0005100385751126099\tTraining RMSE=0.021465892253880514\tValidation loss (MSE)=0.0005706474574388401\tValidation RMSE=0.019577712907145422\t\n",
      "Epoch 99 ->\tTraining loss (MSE)=0.000509923319475126\tTraining RMSE=0.02140229463542777\tValidation loss (MSE)=0.0005703146665904636\tValidation RMSE=0.019574566433827083\t\n",
      "Epoch 100 ->\tTraining loss (MSE)=0.0005115352865282653\tTraining RMSE=0.021506682863275027\tValidation loss (MSE)=0.000569220165844955\tValidation RMSE=0.019552222780745337\t\n",
      "Epoch 101 ->\tTraining loss (MSE)=0.0005092522884678831\tTraining RMSE=0.021526633457131225\tValidation loss (MSE)=0.0005687104549670712\tValidation RMSE=0.0195433528645447\t\n",
      "Epoch 102 ->\tTraining loss (MSE)=0.0005100322612431935\tTraining RMSE=0.021428426020537263\tValidation loss (MSE)=0.0005680523720878193\tValidation RMSE=0.01952987478149158\t\n",
      "Epoch 103 ->\tTraining loss (MSE)=0.000507458073340707\tTraining RMSE=0.021426332873050815\tValidation loss (MSE)=0.0005678001704196773\tValidation RMSE=0.019524066484774703\t\n",
      "Epoch 104 ->\tTraining loss (MSE)=0.0005070487248663141\tTraining RMSE=0.021387687056428857\tValidation loss (MSE)=0.0005674470948751723\tValidation RMSE=0.019519176886037545\t\n",
      "Epoch 105 ->\tTraining loss (MSE)=0.0005071676913993778\tTraining RMSE=0.021362655839227414\tValidation loss (MSE)=0.0005670676437148359\tValidation RMSE=0.019513573997688514\t\n",
      "Epoch 106 ->\tTraining loss (MSE)=0.0005065596278991843\tTraining RMSE=0.021223984865678683\tValidation loss (MSE)=0.0005666210878795633\tValidation RMSE=0.019503205324764603\t\n",
      "Epoch 107 ->\tTraining loss (MSE)=0.0005067291385336078\tTraining RMSE=0.02129386594220076\tValidation loss (MSE)=0.0005663261694684883\tValidation RMSE=0.019498400286667876\t\n",
      "Epoch 108 ->\tTraining loss (MSE)=0.0005060679524584334\tTraining RMSE=0.021401996534593678\tValidation loss (MSE)=0.0005658463949792484\tValidation RMSE=0.01949225274707984\t\n",
      "Epoch 109 ->\tTraining loss (MSE)=0.0005050175516962442\tTraining RMSE=0.021224101372982983\tValidation loss (MSE)=0.0005657022146031657\tValidation RMSE=0.019489406297604244\t\n",
      "Epoch 110 ->\tTraining loss (MSE)=0.0005112420642237541\tTraining RMSE=0.021261209323082442\tValidation loss (MSE)=0.0005650353011366895\tValidation RMSE=0.019480988176332578\t\n",
      "Epoch 111 ->\tTraining loss (MSE)=0.0005072997616177114\tTraining RMSE=0.021427130654316257\tValidation loss (MSE)=0.0005641261431500661\tValidation RMSE=0.019456974161719834\t\n",
      "Epoch 112 ->\tTraining loss (MSE)=0.0005065890187706603\tTraining RMSE=0.021447825958423407\tValidation loss (MSE)=0.0005640401137093755\tValidation RMSE=0.019459712370816205\t\n",
      "Epoch 113 ->\tTraining loss (MSE)=0.0005031588501721818\tTraining RMSE=0.021348950675872648\tValidation loss (MSE)=0.0005634734164535379\tValidation RMSE=0.019441558065375796\t\n",
      "Epoch 114 ->\tTraining loss (MSE)=0.0005055671871009564\tTraining RMSE=0.02133440644549275\tValidation loss (MSE)=0.000562714816354586\tValidation RMSE=0.019426828767690394\t\n",
      "Epoch 115 ->\tTraining loss (MSE)=0.0005025935392445212\tTraining RMSE=0.021192681176564945\tValidation loss (MSE)=0.0005624304060431852\tValidation RMSE=0.019419458229094744\t\n",
      "Epoch 116 ->\tTraining loss (MSE)=0.0005023923140133438\tTraining RMSE=0.021147798781317693\tValidation loss (MSE)=0.0005624509859545571\tValidation RMSE=0.01941635068161068\t\n",
      "Epoch 117 ->\tTraining loss (MSE)=0.0005016539249695205\tTraining RMSE=0.02123049304843593\tValidation loss (MSE)=0.0005621119757792998\tValidation RMSE=0.0194150992255244\t\n",
      "Epoch 118 ->\tTraining loss (MSE)=0.0005019155533649789\tTraining RMSE=0.0213019250271221\tValidation loss (MSE)=0.0005616523862786211\tValidation RMSE=0.019401939810012227\t\n",
      "Epoch 119 ->\tTraining loss (MSE)=0.0005012491962397801\tTraining RMSE=0.02113132611675947\tValidation loss (MSE)=0.0005613228824114669\tValidation RMSE=0.019398985706545687\t\n",
      "Epoch 120 ->\tTraining loss (MSE)=0.0005098015681366554\tTraining RMSE=0.02138667582982668\tValidation loss (MSE)=0.0005603671859178037\tValidation RMSE=0.01938404548154385\t\n",
      "Epoch 121 ->\tTraining loss (MSE)=0.0005007306090529137\tTraining RMSE=0.021138488576621\tValidation loss (MSE)=0.0005599182574466491\tValidation RMSE=0.019374113702387723\t\n",
      "Epoch 122 ->\tTraining loss (MSE)=0.0005006350465133258\tTraining RMSE=0.021276073076152875\tValidation loss (MSE)=0.0005598098444655383\tValidation RMSE=0.019368576799967775\t\n",
      "Epoch 123 ->\tTraining loss (MSE)=0.0005010627847460219\tTraining RMSE=0.02135409999431835\tValidation loss (MSE)=0.0005595017028099392\tValidation RMSE=0.019368620175454352\t\n",
      "Epoch 124 ->\tTraining loss (MSE)=0.0005039819727334148\tTraining RMSE=0.02130588349874741\tValidation loss (MSE)=0.000558989748646531\tValidation RMSE=0.019357375835103018\t\n",
      "Epoch 125 ->\tTraining loss (MSE)=0.0004996837157514276\tTraining RMSE=0.021166085415048365\tValidation loss (MSE)=0.000558755247496905\tValidation RMSE=0.019354688176126393\t\n",
      "Epoch 126 ->\tTraining loss (MSE)=0.000500534338322195\tTraining RMSE=0.021211303353746547\tValidation loss (MSE)=0.0005584524031426689\tValidation RMSE=0.019347171212925954\t\n",
      "Epoch 127 ->\tTraining loss (MSE)=0.0004982468345745328\tTraining RMSE=0.021087638664427272\tValidation loss (MSE)=0.0005581802796330993\tValidation RMSE=0.01934045118590196\t\n",
      "Epoch 128 ->\tTraining loss (MSE)=0.0005015417345369057\tTraining RMSE=0.021275618981661987\tValidation loss (MSE)=0.0005588864658892379\tValidation RMSE=0.019358326267037127\t\n",
      "Epoch 129 ->\tTraining loss (MSE)=0.0005065602971974582\tTraining RMSE=0.021443549169940346\tValidation loss (MSE)=0.000557611937741361\tValidation RMSE=0.019329368567991036\t\n",
      "Epoch 130 ->\tTraining loss (MSE)=0.000497172157585378\tTraining RMSE=0.021125222849487153\tValidation loss (MSE)=0.0005565690708290613\tValidation RMSE=0.019311169403846615\t\n",
      "Epoch 131 ->\tTraining loss (MSE)=0.0004973032573840931\tTraining RMSE=0.021034300051353597\tValidation loss (MSE)=0.000556079386081331\tValidation RMSE=0.019305583865692217\t\n",
      "Epoch 132 ->\tTraining loss (MSE)=0.0004983534106738969\tTraining RMSE=0.021130971748519828\tValidation loss (MSE)=0.0005558668111647152\tValidation RMSE=0.01930750665013437\t\n",
      "Epoch 133 ->\tTraining loss (MSE)=0.0004961454828451588\tTraining RMSE=0.02126637477726664\tValidation loss (MSE)=0.0005553408719370728\tValidation RMSE=0.019288567075919773\t\n",
      "Epoch 134 ->\tTraining loss (MSE)=0.000496736204953732\tTraining RMSE=0.021080488540278174\tValidation loss (MSE)=0.0005552058858787253\tValidation RMSE=0.019287913554796466\t\n",
      "Epoch 135 ->\tTraining loss (MSE)=0.0005015255049744874\tTraining RMSE=0.0211777683565922\tValidation loss (MSE)=0.0005548752991065461\tValidation RMSE=0.019278890953433735\t\n",
      "Epoch 136 ->\tTraining loss (MSE)=0.0004997996583381197\tTraining RMSE=0.021095136071290866\tValidation loss (MSE)=0.0005545454947692902\tValidation RMSE=0.019276202880535966\t\n",
      "Epoch 137 ->\tTraining loss (MSE)=0.0004967995682594016\tTraining RMSE=0.021100723748037845\tValidation loss (MSE)=0.0005543865397528323\tValidation RMSE=0.01927072992030945\t\n",
      "Epoch 138 ->\tTraining loss (MSE)=0.0004948495844125576\tTraining RMSE=0.021083181554359603\tValidation loss (MSE)=0.0005543741681995689\tValidation RMSE=0.01927362174589049\t\n",
      "Epoch 139 ->\tTraining loss (MSE)=0.0004947736195188352\tTraining RMSE=0.021061556396523008\tValidation loss (MSE)=0.0005539586029583545\tValidation RMSE=0.01926169020158273\t\n",
      "Epoch 140 ->\tTraining loss (MSE)=0.0004976133287404261\tTraining RMSE=0.021236373393301977\tValidation loss (MSE)=0.0005539263354337352\tValidation RMSE=0.01926330372656661\t\n",
      "Epoch 141 ->\tTraining loss (MSE)=0.0004939957333938506\tTraining RMSE=0.02109570773088454\tValidation loss (MSE)=0.0005518097750609741\tValidation RMSE=0.01921532790314544\t\n",
      "Epoch 142 ->\tTraining loss (MSE)=0.0004922574443983728\tTraining RMSE=0.02102946666253294\tValidation loss (MSE)=0.0005512717837485036\tValidation RMSE=0.0192087990473266\t\n",
      "Epoch 143 ->\tTraining loss (MSE)=0.0004945828055766739\tTraining RMSE=0.021065416377911598\tValidation loss (MSE)=0.0005508155354618793\tValidation RMSE=0.01919897324923012\t\n",
      "Epoch 144 ->\tTraining loss (MSE)=0.000491575846140615\tTraining RMSE=0.020942113020192877\tValidation loss (MSE)=0.0005505463941517519\tValidation RMSE=0.01919514603084988\t\n",
      "Epoch 145 ->\tTraining loss (MSE)=0.0004921676591712403\tTraining RMSE=0.020856713419090265\tValidation loss (MSE)=0.0005502711064090599\tValidation RMSE=0.019188966740060737\t\n",
      "Epoch 146 ->\tTraining loss (MSE)=0.0004917559635043605\tTraining RMSE=0.02097529042972091\tValidation loss (MSE)=0.0005500427031856143\tValidation RMSE=0.019183391722402087\t\n",
      "Epoch 147 ->\tTraining loss (MSE)=0.00049215958876439\tTraining RMSE=0.021249565295875072\tValidation loss (MSE)=0.0005501733886140519\tValidation RMSE=0.01918691865823887\t\n",
      "Epoch 148 ->\tTraining loss (MSE)=0.0004938935848325922\tTraining RMSE=0.021257349174975612\tValidation loss (MSE)=0.0005496809820518441\tValidation RMSE=0.019177603388757066\t\n",
      "Epoch 149 ->\tTraining loss (MSE)=0.0004944174372527122\tTraining RMSE=0.021046365749228885\tValidation loss (MSE)=0.0005493757703165842\tValidation RMSE=0.019171551904744573\t\n",
      "Epoch 150 ->\tTraining loss (MSE)=0.0005038502692547843\tTraining RMSE=0.02136883239928679\tValidation loss (MSE)=0.0005494567426298831\tValidation RMSE=0.019177338151537156\t\n",
      "Epoch 151 ->\tTraining loss (MSE)=0.000500102300717162\tTraining RMSE=0.02120388703374767\tValidation loss (MSE)=0.0005489427812453532\tValidation RMSE=0.019165491279973475\t\n",
      "Epoch 152 ->\tTraining loss (MSE)=0.0004907348246225961\tTraining RMSE=0.020798082359962992\tValidation loss (MSE)=0.0005487677290484835\tValidation RMSE=0.019160011515918153\t\n",
      "Epoch 153 ->\tTraining loss (MSE)=0.000489875297200432\tTraining RMSE=0.02112953876214171\tValidation loss (MSE)=0.0005486671654108688\tValidation RMSE=0.019158849553032604\t\n",
      "Epoch 154 ->\tTraining loss (MSE)=0.0004897422503644419\tTraining RMSE=0.02071857804512996\tValidation loss (MSE)=0.0005484821156440173\tValidation RMSE=0.019153268705984508\t\n",
      "Epoch 155 ->\tTraining loss (MSE)=0.000489547751486599\tTraining RMSE=0.021111133149077678\tValidation loss (MSE)=0.0005482363779905157\tValidation RMSE=0.019148096252508736\t\n",
      "Epoch 156 ->\tTraining loss (MSE)=0.0004918803676030811\tTraining RMSE=0.021125630337606977\tValidation loss (MSE)=0.0005481827356561553\tValidation RMSE=0.01915056297245125\t\n",
      "Epoch 157 ->\tTraining loss (MSE)=0.0004894970844799429\tTraining RMSE=0.020995988251848355\tValidation loss (MSE)=0.0005480134816564343\tValidation RMSE=0.019146823548470384\t\n",
      "Epoch 158 ->\tTraining loss (MSE)=0.000489152609431731\tTraining RMSE=0.021067264302990135\tValidation loss (MSE)=0.0005478226506022986\tValidation RMSE=0.019141862186154834\t\n",
      "Epoch 159 ->\tTraining loss (MSE)=0.0004901869318321891\tTraining RMSE=0.02107973249318699\tValidation loss (MSE)=0.000547678223180507\tValidation RMSE=0.019139292287743755\t\n",
      "Epoch 160 ->\tTraining loss (MSE)=0.0004902900213050156\tTraining RMSE=0.021095661104392305\tValidation loss (MSE)=0.0005474997400327724\tValidation RMSE=0.019133978238743212\t\n",
      "Epoch 161 ->\tTraining loss (MSE)=0.0004898584928822755\tTraining RMSE=0.020889524784353044\tValidation loss (MSE)=0.0005472943273532481\tValidation RMSE=0.01912952966436192\t\n",
      "Epoch 162 ->\tTraining loss (MSE)=0.0004889023937856244\tTraining RMSE=0.021016082864392688\tValidation loss (MSE)=0.0005471651233232546\tValidation RMSE=0.0191263594078245\t\n",
      "Epoch 163 ->\tTraining loss (MSE)=0.000492322909849463\tTraining RMSE=0.021045090728381902\tValidation loss (MSE)=0.0005469177859696276\tValidation RMSE=0.01912304090715393\t\n",
      "Epoch 164 ->\tTraining loss (MSE)=0.0004892795459903779\tTraining RMSE=0.02092104772232289\tValidation loss (MSE)=0.0005467657945161962\tValidation RMSE=0.019122058680901926\t\n",
      "Epoch 165 ->\tTraining loss (MSE)=0.0004930739514411134\tTraining RMSE=0.02109748875308368\tValidation loss (MSE)=0.0005465966214278187\tValidation RMSE=0.019119888406108926\t\n",
      "Epoch 166 ->\tTraining loss (MSE)=0.0004913475972842453\tTraining RMSE=0.021110875670004775\tValidation loss (MSE)=0.0005464646922822188\tValidation RMSE=0.01911566618623005\t\n",
      "Epoch 167 ->\tTraining loss (MSE)=0.0004891500944430857\tTraining RMSE=0.020927042855570715\tValidation loss (MSE)=0.0005463310433831928\tValidation RMSE=0.019115101502932334\t\n",
      "Epoch 168 ->\tTraining loss (MSE)=0.0004877198723225314\tTraining RMSE=0.020832361575836938\tValidation loss (MSE)=0.0005462855187269506\tValidation RMSE=0.019115179449457814\t\n",
      "Epoch 169 ->\tTraining loss (MSE)=0.0004909496675495182\tTraining RMSE=0.021014915422800883\tValidation loss (MSE)=0.0005460075122014301\tValidation RMSE=0.01910885582091632\t\n",
      "Epoch 170 ->\tTraining loss (MSE)=0.0004875892516815558\tTraining RMSE=0.020975721965509432\tValidation loss (MSE)=0.0005455944542013134\tValidation RMSE=0.019099997572117933\t\n",
      "Epoch 171 ->\tTraining loss (MSE)=0.0004872739318221805\tTraining RMSE=0.021087799244272856\tValidation loss (MSE)=0.0005454189045445269\tValidation RMSE=0.019099655285201692\t\n",
      "Epoch 172 ->\tTraining loss (MSE)=0.0004887954274175924\tTraining RMSE=0.02103935526166525\tValidation loss (MSE)=0.0005453902055402674\tValidation RMSE=0.019097866214535856\t\n",
      "Epoch 173 ->\tTraining loss (MSE)=0.0004887109738505118\tTraining RMSE=0.021008936262884994\tValidation loss (MSE)=0.0005450811304062545\tValidation RMSE=0.019091168892811292\t\n",
      "Epoch 174 ->\tTraining loss (MSE)=0.00048763446587448317\tTraining RMSE=0.020970321961759048\tValidation loss (MSE)=0.0005451386122230259\tValidation RMSE=0.01909537123585189\t\n",
      "Epoch 175 ->\tTraining loss (MSE)=0.00048689504874795445\tTraining RMSE=0.02100555187114227\tValidation loss (MSE)=0.0005446666169451351\tValidation RMSE=0.019084420520812273\t\n",
      "Epoch 176 ->\tTraining loss (MSE)=0.0004867177987852088\tTraining RMSE=0.020963271372710115\tValidation loss (MSE)=0.0005444745115204973\tValidation RMSE=0.019078225561383146\t\n",
      "Epoch 177 ->\tTraining loss (MSE)=0.0004873802826531809\tTraining RMSE=0.020999542503606205\tValidation loss (MSE)=0.0005445575772211197\tValidation RMSE=0.019076395098602882\t\n",
      "Epoch 178 ->\tTraining loss (MSE)=0.00048623071766255805\tTraining RMSE=0.020894453564750743\tValidation loss (MSE)=0.0005442516732593369\tValidation RMSE=0.019069124763417575\t\n",
      "Epoch 179 ->\tTraining loss (MSE)=0.0004880082967168692\tTraining RMSE=0.02107561704484217\tValidation loss (MSE)=0.0005439224410279335\tValidation RMSE=0.01906890121150624\t\n",
      "Epoch 180 ->\tTraining loss (MSE)=0.0004874362030808877\tTraining RMSE=0.021069539710879326\tValidation loss (MSE)=0.0005436242606972681\tValidation RMSE=0.019059404566953028\t\n",
      "Epoch 181 ->\tTraining loss (MSE)=0.0004860872904154584\tTraining RMSE=0.020962782796288347\tValidation loss (MSE)=0.0005437277687397673\tValidation RMSE=0.01907166701534556\t\n",
      "Epoch 182 ->\tTraining loss (MSE)=0.000487926106601393\tTraining RMSE=0.020851558154839793\tValidation loss (MSE)=0.0005434667257725744\tValidation RMSE=0.019059723225870618\t\n",
      "Epoch 183 ->\tTraining loss (MSE)=0.00048590427908448523\tTraining RMSE=0.020872929627880638\tValidation loss (MSE)=0.0005431960386583038\tValidation RMSE=0.019053152124224988\t\n",
      "Epoch 184 ->\tTraining loss (MSE)=0.0004967023302733763\tTraining RMSE=0.021013051616372885\tValidation loss (MSE)=0.0005431032571963173\tValidation RMSE=0.019050626679220133\t\n",
      "Epoch 185 ->\tTraining loss (MSE)=0.0004885339599746254\tTraining RMSE=0.020851615436927037\tValidation loss (MSE)=0.0005430744918999995\tValidation RMSE=0.0190483709383342\t\n",
      "Epoch 186 ->\tTraining loss (MSE)=0.0004853354196565766\tTraining RMSE=0.020788468840175572\tValidation loss (MSE)=0.0005428876630941415\tValidation RMSE=0.01904848523231016\t\n",
      "Epoch 187 ->\tTraining loss (MSE)=0.00048497797471987094\tTraining RMSE=0.020784202986109404\tValidation loss (MSE)=0.0005427898946369101\tValidation RMSE=0.019045647716632596\t\n",
      "Epoch 188 ->\tTraining loss (MSE)=0.0004854460526131996\tTraining RMSE=0.02083621681150463\tValidation loss (MSE)=0.0005427734987380165\tValidation RMSE=0.01904910164712756\t\n",
      "Epoch 189 ->\tTraining loss (MSE)=0.00048529054478377125\tTraining RMSE=0.020936353287349146\tValidation loss (MSE)=0.0005424651391405388\tValidation RMSE=0.01903901826072898\t\n",
      "Epoch 190 ->\tTraining loss (MSE)=0.0004864724735482553\tTraining RMSE=0.02092288837791133\tValidation loss (MSE)=0.0005424736877844056\tValidation RMSE=0.019040457792235194\t\n",
      "Epoch 191 ->\tTraining loss (MSE)=0.0004973653931189386\tTraining RMSE=0.021069574744705066\tValidation loss (MSE)=0.0005422810126118646\tValidation RMSE=0.019034870736369933\t\n",
      "Epoch 192 ->\tTraining loss (MSE)=0.000488832050051977\tTraining RMSE=0.021001551754450724\tValidation loss (MSE)=0.0005422796235102902\tValidation RMSE=0.019035787149159995\t\n",
      "Epoch 193 ->\tTraining loss (MSE)=0.0004852001186884957\tTraining RMSE=0.020944101692832732\tValidation loss (MSE)=0.0005423680296964314\tValidation RMSE=0.01903666888743087\t\n",
      "Epoch 194 ->\tTraining loss (MSE)=0.0004885177763809887\tTraining RMSE=0.02086607057509231\tValidation loss (MSE)=0.00054215193415951\tValidation RMSE=0.019037747419335775\t\n",
      "Epoch 195 ->\tTraining loss (MSE)=0.00048457369536472584\tTraining RMSE=0.020697908889916208\tValidation loss (MSE)=0.0005419841395956206\tValidation RMSE=0.01902944860221059\t\n",
      "Epoch 196 ->\tTraining loss (MSE)=0.000487454129199003\tTraining RMSE=0.020971858491661187\tValidation loss (MSE)=0.0005419027661614733\tValidation RMSE=0.019031506669880065\t\n",
      "Epoch 197 ->\tTraining loss (MSE)=0.0004970932256616868\tTraining RMSE=0.021196009275032048\tValidation loss (MSE)=0.0005417102193961102\tValidation RMSE=0.01902710899917616\t\n",
      "Epoch 198 ->\tTraining loss (MSE)=0.0004848450871689653\tTraining RMSE=0.020854447062851653\tValidation loss (MSE)=0.000541607577125197\tValidation RMSE=0.019025811192545074\t\n",
      "Epoch 199 ->\tTraining loss (MSE)=0.000487186402568585\tTraining RMSE=0.021000742435133384\tValidation loss (MSE)=0.000541518343568357\tValidation RMSE=0.01902515177304546\t\n",
      "Epoch 200 ->\tTraining loss (MSE)=0.00048348377601912406\tTraining RMSE=0.020827621895601445\tValidation loss (MSE)=0.0005414203361498141\tValidation RMSE=0.01902051636500767\t\n",
      "Epoch 201 ->\tTraining loss (MSE)=0.00048632611534781006\tTraining RMSE=0.02085673115733597\tValidation loss (MSE)=0.0005413351032917422\tValidation RMSE=0.019018872899727687\t\n",
      "Epoch 202 ->\tTraining loss (MSE)=0.00048332240943255096\tTraining RMSE=0.020828222594350392\tValidation loss (MSE)=0.0005413856538830765\tValidation RMSE=0.019021790586756886\t\n",
      "Epoch 203 ->\tTraining loss (MSE)=0.0004839325357565547\tTraining RMSE=0.02086311633365206\tValidation loss (MSE)=0.0005411587482268698\tValidation RMSE=0.01901680082772617\t\n",
      "Epoch 204 ->\tTraining loss (MSE)=0.0004836463723548172\tTraining RMSE=0.0207192310632241\tValidation loss (MSE)=0.0005412657070270076\tValidation RMSE=0.01902436998810757\t\n",
      "Epoch 205 ->\tTraining loss (MSE)=0.00048424103608991616\tTraining RMSE=0.020897628739476204\tValidation loss (MSE)=0.0005411000410175701\tValidation RMSE=0.01901620974833215\t\n",
      "Epoch 206 ->\tTraining loss (MSE)=0.00048591318202706593\tTraining RMSE=0.02072870591080483\tValidation loss (MSE)=0.0005412353570540694\tValidation RMSE=0.019015358838563163\t\n",
      "Epoch 207 ->\tTraining loss (MSE)=0.00048533891880831587\tTraining RMSE=0.020896396668696846\tValidation loss (MSE)=0.0005408336054204954\tValidation RMSE=0.01901018806664204\t\n",
      "Epoch 208 ->\tTraining loss (MSE)=0.0004847851612143086\tTraining RMSE=0.020932121984200712\tValidation loss (MSE)=0.0005407008732247589\tValidation RMSE=0.019009485194045637\t\n",
      "Epoch 209 ->\tTraining loss (MSE)=0.0004872020039957859\tTraining RMSE=0.020872335544683866\tValidation loss (MSE)=0.0005406844877682117\tValidation RMSE=0.019008518093162112\t\n",
      "Epoch 210 ->\tTraining loss (MSE)=0.0004860785427606768\tTraining RMSE=0.020980146345624954\tValidation loss (MSE)=0.0005405470977469425\tValidation RMSE=0.019006188536338783\t\n",
      "Epoch 211 ->\tTraining loss (MSE)=0.00048469372245843847\tTraining RMSE=0.020973636320343724\tValidation loss (MSE)=0.0005405845874553995\tValidation RMSE=0.019008697045070154\t\n",
      "Epoch 212 ->\tTraining loss (MSE)=0.0004835421432177594\tTraining RMSE=0.020917450342281364\tValidation loss (MSE)=0.0005404795025511972\tValidation RMSE=0.019005991164939822\t\n",
      "Epoch 213 ->\tTraining loss (MSE)=0.00048311124167266375\tTraining RMSE=0.020804829602302226\tValidation loss (MSE)=0.0005402427082622631\tValidation RMSE=0.01900064290708138\t\n",
      "Epoch 214 ->\tTraining loss (MSE)=0.00048358475661514625\tTraining RMSE=0.02082007389837577\tValidation loss (MSE)=0.0005402365592696393\tValidation RMSE=0.019000802603032853\t\n",
      "Epoch 215 ->\tTraining loss (MSE)=0.0004854088603881084\tTraining RMSE=0.020858303172352873\tValidation loss (MSE)=0.0005401029698583039\tValidation RMSE=0.01899951019462336\t\n",
      "Epoch 216 ->\tTraining loss (MSE)=0.00048473191728354086\tTraining RMSE=0.020820858899825885\tValidation loss (MSE)=0.0005400267055611712\tValidation RMSE=0.018998166701445978\t\n",
      "Epoch 217 ->\tTraining loss (MSE)=0.0004917850048154777\tTraining RMSE=0.021054588976879545\tValidation loss (MSE)=0.0005398662585018125\tValidation RMSE=0.018993331930013718\t\n",
      "Epoch 218 ->\tTraining loss (MSE)=0.0004845571698909079\tTraining RMSE=0.02090545491039477\tValidation loss (MSE)=0.000539689730026617\tValidation RMSE=0.01899193493752844\t\n",
      "Epoch 219 ->\tTraining loss (MSE)=0.00048483070304989527\tTraining RMSE=0.020856651138330316\tValidation loss (MSE)=0.0005396391062714867\tValidation RMSE=0.018993155254671972\t\n",
      "Epoch 220 ->\tTraining loss (MSE)=0.0004829063851323071\tTraining RMSE=0.020886330060476875\tValidation loss (MSE)=0.0005394615808553579\tValidation RMSE=0.018988132606156997\t\n",
      "Epoch 221 ->\tTraining loss (MSE)=0.0004847317619283718\tTraining RMSE=0.020844247249633442\tValidation loss (MSE)=0.0005380423532457186\tValidation RMSE=0.018965280166378728\t\n",
      "Epoch 222 ->\tTraining loss (MSE)=0.0004825590620856491\tTraining RMSE=0.02080445027231802\tValidation loss (MSE)=0.000537814605464788\tValidation RMSE=0.018964469510441024\t\n",
      "Epoch 223 ->\tTraining loss (MSE)=0.00048417439792962044\tTraining RMSE=0.020790452183580693\tValidation loss (MSE)=0.0005377085164110651\tValidation RMSE=0.018961516243440134\t\n",
      "Epoch 224 ->\tTraining loss (MSE)=0.0004810605648715325\tTraining RMSE=0.020739678494678237\tValidation loss (MSE)=0.0005374856789583444\tValidation RMSE=0.01895789946946833\t\n",
      "Epoch 225 ->\tTraining loss (MSE)=0.0004809297234815959\tTraining RMSE=0.02089755859134006\tValidation loss (MSE)=0.000537400292294613\tValidation RMSE=0.018956491956487298\t\n",
      "Epoch 226 ->\tTraining loss (MSE)=0.00048029260775634883\tTraining RMSE=0.02089710847509128\tValidation loss (MSE)=0.0005374154813950925\tValidation RMSE=0.01896043090770642\t\n",
      "Epoch 227 ->\tTraining loss (MSE)=0.00047999339055681005\tTraining RMSE=0.020693904481667243\tValidation loss (MSE)=0.0005372866704015501\tValidation RMSE=0.018954706749085476\t\n",
      "Epoch 228 ->\tTraining loss (MSE)=0.0004799224074147024\tTraining RMSE=0.0205884158588302\tValidation loss (MSE)=0.0005371454775471378\tValidation RMSE=0.018952900345471722\t\n",
      "Epoch 229 ->\tTraining loss (MSE)=0.00048697674272845667\tTraining RMSE=0.02084466228319079\tValidation loss (MSE)=0.0005374542815843597\tValidation RMSE=0.01896636087136964\t\n",
      "Epoch 230 ->\tTraining loss (MSE)=0.0004811841030863838\tTraining RMSE=0.020836178334084927\tValidation loss (MSE)=0.0005371530405008579\tValidation RMSE=0.018956702444012517\t\n",
      "Epoch 231 ->\tTraining loss (MSE)=0.00048099398491889767\tTraining RMSE=0.02084545148421585\tValidation loss (MSE)=0.0005369795452915477\tValidation RMSE=0.018950401960561674\t\n",
      "Epoch 232 ->\tTraining loss (MSE)=0.0004829740097687327\tTraining RMSE=0.0209068671087332\tValidation loss (MSE)=0.0005368534716749478\tValidation RMSE=0.018947636717240566\t\n",
      "Epoch 233 ->\tTraining loss (MSE)=0.000480059994493157\tTraining RMSE=0.020887944502411066\tValidation loss (MSE)=0.0005367993191477446\tValidation RMSE=0.018946512403932435\t\n",
      "Epoch 234 ->\tTraining loss (MSE)=0.00048516221350938495\tTraining RMSE=0.02081357214576852\tValidation loss (MSE)=0.000536917190739014\tValidation RMSE=0.018950571055972466\t\n",
      "Epoch 235 ->\tTraining loss (MSE)=0.0004800403593334421\tTraining RMSE=0.0208857589987693\tValidation loss (MSE)=0.0005365508193925187\tValidation RMSE=0.018942629625261935\t\n",
      "Epoch 236 ->\tTraining loss (MSE)=0.00048053596859702976\tTraining RMSE=0.020893814634347772\tValidation loss (MSE)=0.0005364392064721971\tValidation RMSE=0.01894085830146516\t\n",
      "Epoch 237 ->\tTraining loss (MSE)=0.0004948170754676224\tTraining RMSE=0.020996217851607518\tValidation loss (MSE)=0.0005362724353256419\tValidation RMSE=0.01893612096534559\t\n",
      "Epoch 238 ->\tTraining loss (MSE)=0.00048007662409850407\tTraining RMSE=0.0208317707810137\tValidation loss (MSE)=0.0005361468914391784\tValidation RMSE=0.018934480156059617\t\n",
      "Epoch 239 ->\tTraining loss (MSE)=0.0004794520129822266\tTraining RMSE=0.020904071859003587\tValidation loss (MSE)=0.0005357231612261212\tValidation RMSE=0.018929067214399024\t\n",
      "Epoch 240 ->\tTraining loss (MSE)=0.0004805836382010854\tTraining RMSE=0.020819152607639998\tValidation loss (MSE)=0.0005355979621392045\tValidation RMSE=0.018927455802137654\t\n",
      "Epoch 241 ->\tTraining loss (MSE)=0.00047870857685475244\tTraining RMSE=0.020728743640866912\tValidation loss (MSE)=0.0005354718243190156\tValidation RMSE=0.018925876028973748\t\n",
      "Epoch 242 ->\tTraining loss (MSE)=0.00048001074461398625\tTraining RMSE=0.020724430596915845\tValidation loss (MSE)=0.0005354198978979925\tValidation RMSE=0.018925287045055517\t\n",
      "Epoch 243 ->\tTraining loss (MSE)=0.00047970264908436534\tTraining RMSE=0.020905670287363506\tValidation loss (MSE)=0.0005353653365023934\tValidation RMSE=0.018924145433292898\t\n",
      "Epoch 244 ->\tTraining loss (MSE)=0.00048146554860225335\tTraining RMSE=0.020850443499691694\tValidation loss (MSE)=0.000535277187197978\tValidation RMSE=0.018922364822140447\t\n",
      "Epoch 245 ->\tTraining loss (MSE)=0.00047833354363791404\tTraining RMSE=0.020791576545754517\tValidation loss (MSE)=0.0005352493965431306\tValidation RMSE=0.018922694096410717\t\n",
      "Epoch 246 ->\tTraining loss (MSE)=0.00047961411159413715\tTraining RMSE=0.02091971592241783\tValidation loss (MSE)=0.000535307415635476\tValidation RMSE=0.018923234858515638\t\n",
      "Epoch 247 ->\tTraining loss (MSE)=0.0004844893769642775\tTraining RMSE=0.020891005052598537\tValidation loss (MSE)=0.0005351454371891494\tValidation RMSE=0.01892162597289792\t\n",
      "Epoch 248 ->\tTraining loss (MSE)=0.00047857362670460313\tTraining RMSE=0.020633810814929966\tValidation loss (MSE)=0.0005350541452773743\tValidation RMSE=0.01891917979379219\t\n",
      "Epoch 249 ->\tTraining loss (MSE)=0.00047920286342110715\tTraining RMSE=0.0206211494644069\tValidation loss (MSE)=0.0005349936489909413\tValidation RMSE=0.018917797598987818\t\n",
      "Epoch 250 ->\tTraining loss (MSE)=0.0004796739979127902\tTraining RMSE=0.02073670518124637\tValidation loss (MSE)=0.0005349486112175293\tValidation RMSE=0.01891725136967445\t\n",
      "Epoch 251 ->\tTraining loss (MSE)=0.0004780848286019237\tTraining RMSE=0.02075698753002525\tValidation loss (MSE)=0.0005349466179419929\tValidation RMSE=0.01891903524907927\t\n",
      "Epoch 252 ->\tTraining loss (MSE)=0.0004781479183068847\tTraining RMSE=0.020829518176155325\tValidation loss (MSE)=0.0005347902816639463\tValidation RMSE=0.018912879517301917\t\n",
      "Epoch 253 ->\tTraining loss (MSE)=0.0004781827063444875\tTraining RMSE=0.02073930509181486\tValidation loss (MSE)=0.0005348462711009672\tValidation RMSE=0.018917018996068725\t\n",
      "Epoch 254 ->\tTraining loss (MSE)=0.0004781234973186791\tTraining RMSE=0.020772697993289724\tValidation loss (MSE)=0.000534626265073678\tValidation RMSE=0.018910543837894995\t\n",
      "Epoch 255 ->\tTraining loss (MSE)=0.00047897693395588034\tTraining RMSE=0.020808463554001518\tValidation loss (MSE)=0.0005346486757641995\tValidation RMSE=0.018910797261115577\t\n",
      "Epoch 256 ->\tTraining loss (MSE)=0.00047863741565379314\tTraining RMSE=0.020806287576294977\tValidation loss (MSE)=0.0005346249594435061\tValidation RMSE=0.018911060704677192\t\n",
      "Epoch 257 ->\tTraining loss (MSE)=0.00047817368267490644\tTraining RMSE=0.020741528520981472\tValidation loss (MSE)=0.0005344723188803376\tValidation RMSE=0.01890818950616651\t\n",
      "Epoch 258 ->\tTraining loss (MSE)=0.00048092288017397987\tTraining RMSE=0.020730308262792266\tValidation loss (MSE)=0.0005344297830257952\tValidation RMSE=0.01890699013515755\t\n",
      "Epoch 259 ->\tTraining loss (MSE)=0.00048071738633608885\tTraining RMSE=0.020689234700927765\tValidation loss (MSE)=0.0005344088830396584\tValidation RMSE=0.01890696592077061\t\n",
      "Epoch 260 ->\tTraining loss (MSE)=0.0004789671402699084\tTraining RMSE=0.020754473870275198\tValidation loss (MSE)=0.0005343425091268728\tValidation RMSE=0.018905617520902997\t\n",
      "Epoch 261 ->\tTraining loss (MSE)=0.0004787800222447352\tTraining RMSE=0.02074488105516835\tValidation loss (MSE)=0.0005343456612333992\tValidation RMSE=0.01890525826321984\t\n",
      "Epoch 262 ->\tTraining loss (MSE)=0.00047843358441713025\tTraining RMSE=0.02063168660611098\tValidation loss (MSE)=0.0005342977693308731\tValidation RMSE=0.018905377248302102\t\n",
      "Epoch 263 ->\tTraining loss (MSE)=0.0004893138077337134\tTraining RMSE=0.02088221738597861\tValidation loss (MSE)=0.0005343157729469196\tValidation RMSE=0.018905407981947064\t\n",
      "Epoch 264 ->\tTraining loss (MSE)=0.0004816252003365662\tTraining RMSE=0.020750820240074838\tValidation loss (MSE)=0.0005343021944605021\tValidation RMSE=0.018906524473870243\t\n",
      "Epoch 265 ->\tTraining loss (MSE)=0.00048028081401292936\tTraining RMSE=0.020773205124301676\tValidation loss (MSE)=0.0005342229206117163\tValidation RMSE=0.018905164992988662\t\n",
      "Epoch 266 ->\tTraining loss (MSE)=0.00047800452672096006\tTraining RMSE=0.020781484438267387\tValidation loss (MSE)=0.0005342577228643846\tValidation RMSE=0.01890547654625994\t\n",
      "Epoch 267 ->\tTraining loss (MSE)=0.0004781916519098076\tTraining RMSE=0.020727436598620298\tValidation loss (MSE)=0.0005340940620549696\tValidation RMSE=0.018901670765545633\t\n",
      "Epoch 268 ->\tTraining loss (MSE)=0.00047790669644054564\tTraining RMSE=0.020735115157785238\tValidation loss (MSE)=0.0005341111245124248\tValidation RMSE=0.018903585211201397\t\n",
      "Epoch 269 ->\tTraining loss (MSE)=0.00047799917076280293\tTraining RMSE=0.02062909892633741\tValidation loss (MSE)=0.0005340493976459753\tValidation RMSE=0.018901845629982376\t\n",
      "Epoch 270 ->\tTraining loss (MSE)=0.0004779420286254601\tTraining RMSE=0.02088742465769619\tValidation loss (MSE)=0.0005340133423117179\tValidation RMSE=0.01890231807785178\t\n",
      "Epoch 271 ->\tTraining loss (MSE)=0.0004775072568323302\tTraining RMSE=0.02067832401551214\tValidation loss (MSE)=0.0005339050505965672\tValidation RMSE=0.01889796787010575\t\n",
      "Epoch 272 ->\tTraining loss (MSE)=0.0004780314926774511\tTraining RMSE=0.02072917772916916\tValidation loss (MSE)=0.0005338744229919501\tValidation RMSE=0.018898420053085795\t\n",
      "Epoch 273 ->\tTraining loss (MSE)=0.0004774039756020899\tTraining RMSE=0.020542729553985006\tValidation loss (MSE)=0.0005339424375658078\tValidation RMSE=0.018898648908361793\t\n",
      "Epoch 274 ->\tTraining loss (MSE)=0.000478197280267242\tTraining RMSE=0.020833375883852073\tValidation loss (MSE)=0.000533828297126025\tValidation RMSE=0.01889689233050578\t\n",
      "Epoch 275 ->\tTraining loss (MSE)=0.0004774665294574749\tTraining RMSE=0.020689975731879657\tValidation loss (MSE)=0.0005337836063753693\tValidation RMSE=0.01889719480338196\t\n",
      "Epoch 276 ->\tTraining loss (MSE)=0.00048005381720514257\tTraining RMSE=0.02080594648802538\tValidation loss (MSE)=0.0005335989347562039\tValidation RMSE=0.01889028056110773\t\n",
      "Epoch 277 ->\tTraining loss (MSE)=0.00047716852875762724\tTraining RMSE=0.020671641436853894\tValidation loss (MSE)=0.0005333965437549287\tValidation RMSE=0.01888496016531631\t\n",
      "Epoch 278 ->\tTraining loss (MSE)=0.00047984509607582976\tTraining RMSE=0.020676114872373915\tValidation loss (MSE)=0.0005334117525948117\tValidation RMSE=0.018885541560680227\t\n",
      "Epoch 279 ->\tTraining loss (MSE)=0.000477315038787546\tTraining RMSE=0.02079745714644683\tValidation loss (MSE)=0.0005334288417981481\tValidation RMSE=0.01888630678014899\t\n",
      "Epoch 280 ->\tTraining loss (MSE)=0.0004768359410297341\tTraining RMSE=0.020891300710148098\tValidation loss (MSE)=0.0005336417254713726\tValidation RMSE=0.01889688355192818\t\n",
      "Epoch 281 ->\tTraining loss (MSE)=0.0004785713361028976\tTraining RMSE=0.020651366147730086\tValidation loss (MSE)=0.000533327946565502\tValidation RMSE=0.01888332139114263\t\n",
      "Epoch 282 ->\tTraining loss (MSE)=0.000477213456201472\tTraining RMSE=0.020718161950899677\tValidation loss (MSE)=0.000533348422255518\tValidation RMSE=0.01888312602898589\t\n",
      "Epoch 283 ->\tTraining loss (MSE)=0.000483637959876904\tTraining RMSE=0.020696906582739802\tValidation loss (MSE)=0.0005331966731318971\tValidation RMSE=0.018881643251343457\t\n",
      "Epoch 284 ->\tTraining loss (MSE)=0.0004877778008471697\tTraining RMSE=0.02089005090687194\tValidation loss (MSE)=0.0005331994072750806\tValidation RMSE=0.018881725207730023\t\n",
      "Epoch 285 ->\tTraining loss (MSE)=0.00047818763082039276\tTraining RMSE=0.020665747878130202\tValidation loss (MSE)=0.0005331662679829471\tValidation RMSE=0.018882324548300217\t\n",
      "Epoch 286 ->\tTraining loss (MSE)=0.00047727041691146435\tTraining RMSE=0.02061417645395354\tValidation loss (MSE)=0.0005331493926792906\tValidation RMSE=0.0188823075171698\t\n",
      "Epoch 287 ->\tTraining loss (MSE)=0.0004776471907439653\tTraining RMSE=0.020834629349180577\tValidation loss (MSE)=0.0005330806437053036\tValidation RMSE=0.01887932769022882\t\n",
      "Epoch 288 ->\tTraining loss (MSE)=0.00048313263979241723\tTraining RMSE=0.02090361685043684\tValidation loss (MSE)=0.0005330347350401873\tValidation RMSE=0.01887841486475534\t\n",
      "Epoch 289 ->\tTraining loss (MSE)=0.0004773135177306414\tTraining RMSE=0.020668830811678442\tValidation loss (MSE)=0.0005330533772562545\tValidation RMSE=0.018879280770542444\t\n",
      "Epoch 290 ->\tTraining loss (MSE)=0.0004770671886361037\tTraining RMSE=0.020606297959378104\tValidation loss (MSE)=0.0005329539609293635\tValidation RMSE=0.018878330036790833\t\n",
      "Epoch 291 ->\tTraining loss (MSE)=0.00047622885365876475\tTraining RMSE=0.020697240012031003\tValidation loss (MSE)=0.0005327245305158647\tValidation RMSE=0.018876248867147498\t\n",
      "Epoch 292 ->\tTraining loss (MSE)=0.00047674580864510556\tTraining RMSE=0.020665772190248524\tValidation loss (MSE)=0.0005324927379088304\tValidation RMSE=0.018871780985069496\t\n",
      "Epoch 293 ->\tTraining loss (MSE)=0.00047621644010593434\tTraining RMSE=0.02071349317046963\tValidation loss (MSE)=0.0005323565728758695\tValidation RMSE=0.01886838809931996\t\n",
      "Epoch 294 ->\tTraining loss (MSE)=0.0004763762702134921\tTraining RMSE=0.020714049302271487\tValidation loss (MSE)=0.000532360276809885\tValidation RMSE=0.018870058012436384\t\n",
      "Epoch 295 ->\tTraining loss (MSE)=0.0004760852303818719\tTraining RMSE=0.02051508231203497\tValidation loss (MSE)=0.0005321507505195095\tValidation RMSE=0.018862823886727845\t\n",
      "Epoch 296 ->\tTraining loss (MSE)=0.00047605364500073554\tTraining RMSE=0.020715147584538768\tValidation loss (MSE)=0.0005321491550636927\tValidation RMSE=0.01886325863327969\t\n",
      "Epoch 297 ->\tTraining loss (MSE)=0.00047582994215819195\tTraining RMSE=0.020614953881221604\tValidation loss (MSE)=0.0005322181625362848\tValidation RMSE=0.018863003847568675\t\n",
      "Epoch 298 ->\tTraining loss (MSE)=0.0004764362415306006\tTraining RMSE=0.020780385710459984\tValidation loss (MSE)=0.0005320201166964749\tValidation RMSE=0.01886019639291421\t\n",
      "Epoch 299 ->\tTraining loss (MSE)=0.0004763386612601742\tTraining RMSE=0.020672967519473145\tValidation loss (MSE)=0.0005320218576040739\tValidation RMSE=0.018859265734338097\t\n",
      "Epoch 300 ->\tTraining loss (MSE)=0.0004760736122046149\tTraining RMSE=0.020682327483815176\tValidation loss (MSE)=0.0005319546476972324\tValidation RMSE=0.018859232801737055\t\n",
      "Epoch 301 ->\tTraining loss (MSE)=0.00047570967608071105\tTraining RMSE=0.020667461879597403\tValidation loss (MSE)=0.0005319455866373135\tValidation RMSE=0.018858679785841593\t\n",
      "Epoch 302 ->\tTraining loss (MSE)=0.00047635661122726167\tTraining RMSE=0.020705067719344373\tValidation loss (MSE)=0.0005319225056155119\tValidation RMSE=0.01885842288740807\t\n",
      "Epoch 303 ->\tTraining loss (MSE)=0.00047646909917131307\tTraining RMSE=0.02080323932301483\tValidation loss (MSE)=0.0005318023172380721\tValidation RMSE=0.018856250474022493\t\n",
      "Epoch 304 ->\tTraining loss (MSE)=0.0004791835254977348\tTraining RMSE=0.020735722377229437\tValidation loss (MSE)=0.0005319908346724265\tValidation RMSE=0.018864461281164376\t\n",
      "Epoch 305 ->\tTraining loss (MSE)=0.0004763129162947061\tTraining RMSE=0.020666521243797042\tValidation loss (MSE)=0.0005318351608433726\tValidation RMSE=0.018859015734590315\t\n",
      "Epoch 306 ->\tTraining loss (MSE)=0.00047803652730327026\tTraining RMSE=0.020815745421489815\tValidation loss (MSE)=0.000531772574201265\tValidation RMSE=0.018855847625268832\t\n",
      "Epoch 307 ->\tTraining loss (MSE)=0.0004760543661289791\tTraining RMSE=0.020697310739369305\tValidation loss (MSE)=0.0005317237937561003\tValidation RMSE=0.018855694767639593\t\n",
      "Epoch 308 ->\tTraining loss (MSE)=0.00047574669381755943\tTraining RMSE=0.02070728297356838\tValidation loss (MSE)=0.0005316916245242788\tValidation RMSE=0.018854869124307123\t\n",
      "Epoch 309 ->\tTraining loss (MSE)=0.0004783309374976886\tTraining RMSE=0.020991742731658398\tValidation loss (MSE)=0.0005316770267300955\tValidation RMSE=0.018855005744154805\t\n",
      "Epoch 310 ->\tTraining loss (MSE)=0.0004780267035702882\tTraining RMSE=0.020772555107135833\tValidation loss (MSE)=0.0005316489816188416\tValidation RMSE=0.018855457815031212\t\n",
      "Epoch 311 ->\tTraining loss (MSE)=0.0004763690865078343\tTraining RMSE=0.020605764742710708\tValidation loss (MSE)=0.0005316090270450028\tValidation RMSE=0.01885401763677321\t\n",
      "Epoch 312 ->\tTraining loss (MSE)=0.0004761813724829232\tTraining RMSE=0.02058115761359165\tValidation loss (MSE)=0.0005315713166998683\tValidation RMSE=0.018852967743037477\t\n",
      "Epoch 313 ->\tTraining loss (MSE)=0.0004762286003644903\tTraining RMSE=0.020774447543109642\tValidation loss (MSE)=0.000531610334696274\tValidation RMSE=0.018853534711524844\t\n",
      "Epoch 314 ->\tTraining loss (MSE)=0.0004759104926905158\tTraining RMSE=0.02071473159953768\tValidation loss (MSE)=0.0005315169432665074\tValidation RMSE=0.018852049269265047\t\n",
      "Epoch 315 ->\tTraining loss (MSE)=0.00047824402200704854\tTraining RMSE=0.020830795117136505\tValidation loss (MSE)=0.0005315412285940342\tValidation RMSE=0.018852170737874176\t\n",
      "Epoch 316 ->\tTraining loss (MSE)=0.0004756390384567634\tTraining RMSE=0.020739894840337426\tValidation loss (MSE)=0.0005315853020335824\tValidation RMSE=0.018855908790741255\t\n",
      "Epoch 317 ->\tTraining loss (MSE)=0.0004915626658107668\tTraining RMSE=0.020885429451426053\tValidation loss (MSE)=0.0005315307619942663\tValidation RMSE=0.01885296144798674\t\n",
      "Epoch 318 ->\tTraining loss (MSE)=0.0004775606900278226\tTraining RMSE=0.020776698015124342\tValidation loss (MSE)=0.0005314694257389487\tValidation RMSE=0.018851718140972987\t\n",
      "Epoch 319 ->\tTraining loss (MSE)=0.0004767418542519689\tTraining RMSE=0.020683780105577573\tValidation loss (MSE)=0.0005314073407336966\tValidation RMSE=0.018850349838397017\t\n",
      "Epoch 320 ->\tTraining loss (MSE)=0.0004771383339377482\tTraining RMSE=0.020700126357468557\tValidation loss (MSE)=0.0005313909471927521\tValidation RMSE=0.018850041070470103\t\n",
      "Epoch 321 ->\tTraining loss (MSE)=0.0004778517282201066\tTraining RMSE=0.020489683676580037\tValidation loss (MSE)=0.000531388514126\tValidation RMSE=0.01885063185666998\t\n",
      "Epoch 322 ->\tTraining loss (MSE)=0.00047630388247471496\tTraining RMSE=0.02056121636170572\tValidation loss (MSE)=0.0005313687384100545\tValidation RMSE=0.018850055083425507\t\n",
      "Epoch 323 ->\tTraining loss (MSE)=0.0004778905125100591\tTraining RMSE=0.020554410129854525\tValidation loss (MSE)=0.000531560713608525\tValidation RMSE=0.018855957607566205\t\n",
      "Epoch 324 ->\tTraining loss (MSE)=0.00047572554269860164\tTraining RMSE=0.02070565701657791\tValidation loss (MSE)=0.0005313132472413903\tValidation RMSE=0.018849066613862913\t\n",
      "Epoch 325 ->\tTraining loss (MSE)=0.00047563527851583965\tTraining RMSE=0.020607958513277548\tValidation loss (MSE)=0.000531364464323916\tValidation RMSE=0.018850319958464416\t\n",
      "Epoch 326 ->\tTraining loss (MSE)=0.00047633484567164867\tTraining RMSE=0.02075014457129586\tValidation loss (MSE)=0.0005312686986413879\tValidation RMSE=0.018848798970726353\t\n",
      "Epoch 327 ->\tTraining loss (MSE)=0.00047587175827680365\tTraining RMSE=0.020696995578660272\tValidation loss (MSE)=0.0005312700722478675\tValidation RMSE=0.018849649216496834\t\n",
      "Epoch 328 ->\tTraining loss (MSE)=0.00048182051160651615\tTraining RMSE=0.020759760384896287\tValidation loss (MSE)=0.000531305285120551\tValidation RMSE=0.01885143475158623\t\n",
      "Epoch 329 ->\tTraining loss (MSE)=0.00047515956194493574\tTraining RMSE=0.020601948450124006\tValidation loss (MSE)=0.0005312075316868248\tValidation RMSE=0.018847909229980024\t\n",
      "Epoch 330 ->\tTraining loss (MSE)=0.00047540928165817827\tTraining RMSE=0.020819878602331435\tValidation loss (MSE)=0.0005311226343986129\tValidation RMSE=0.018846284210061032\t\n",
      "Epoch 331 ->\tTraining loss (MSE)=0.0004757558768508233\tTraining RMSE=0.02052099649782902\tValidation loss (MSE)=0.0005311367974543289\tValidation RMSE=0.018847722456687026\t\n",
      "Epoch 332 ->\tTraining loss (MSE)=0.0004804530195812091\tTraining RMSE=0.020928424915275825\tValidation loss (MSE)=0.000531087575196849\tValidation RMSE=0.018845858940578723\t\n",
      "Epoch 333 ->\tTraining loss (MSE)=0.00047524085747693194\tTraining RMSE=0.02065767522579358\tValidation loss (MSE)=0.0005310785302383549\tValidation RMSE=0.018845582415384275\t\n",
      "Epoch 334 ->\tTraining loss (MSE)=0.0004782303402424549\tTraining RMSE=0.020938127721303407\tValidation loss (MSE)=0.0005310381827628051\tValidation RMSE=0.018844533763411973\t\n",
      "Epoch 335 ->\tTraining loss (MSE)=0.00047674424907503027\tTraining RMSE=0.020681475179936782\tValidation loss (MSE)=0.0005310544822552603\tValidation RMSE=0.018845087934837298\t\n",
      "Epoch 336 ->\tTraining loss (MSE)=0.00047546139786042917\tTraining RMSE=0.020753986133193528\tValidation loss (MSE)=0.0005309681796293093\tValidation RMSE=0.018843500314418365\t\n",
      "Epoch 337 ->\tTraining loss (MSE)=0.0004767953781274515\tTraining RMSE=0.020707389592756458\tValidation loss (MSE)=0.0005308703738491102\tValidation RMSE=0.01884245161070592\t\n",
      "Epoch 338 ->\tTraining loss (MSE)=0.00047511949742329307\tTraining RMSE=0.020520636328944453\tValidation loss (MSE)=0.0005308943934020741\tValidation RMSE=0.018844554511209328\t\n",
      "Epoch 339 ->\tTraining loss (MSE)=0.0004750594897937665\tTraining RMSE=0.020411736728554522\tValidation loss (MSE)=0.000530891548502647\tValidation RMSE=0.018845387346421678\t\n",
      "Epoch 340 ->\tTraining loss (MSE)=0.0004873086135050449\tTraining RMSE=0.020942160457280683\tValidation loss (MSE)=0.0005308862282954915\tValidation RMSE=0.018845599153320545\t\n",
      "Epoch 341 ->\tTraining loss (MSE)=0.0004844774727790076\tTraining RMSE=0.020878432755484993\tValidation loss (MSE)=0.0005308463664901124\tValidation RMSE=0.0188448332871\t\n",
      "Epoch 342 ->\tTraining loss (MSE)=0.0004818096795470747\tTraining RMSE=0.020813205112691645\tValidation loss (MSE)=0.0005307646630104433\tValidation RMSE=0.01884094849056392\t\n",
      "Epoch 343 ->\tTraining loss (MSE)=0.00047563523427622084\tTraining RMSE=0.020817126877559922\tValidation loss (MSE)=0.0005308811114782993\tValidation RMSE=0.01884580456168839\t\n",
      "Epoch 344 ->\tTraining loss (MSE)=0.0004758096669612303\tTraining RMSE=0.020731201038960322\tValidation loss (MSE)=0.0005307294579526772\tValidation RMSE=0.018841503498454888\t\n",
      "Epoch 345 ->\tTraining loss (MSE)=0.0004749019080244815\tTraining RMSE=0.0207494573817117\tValidation loss (MSE)=0.0005307787617952003\tValidation RMSE=0.018840995177419648\t\n",
      "Epoch 346 ->\tTraining loss (MSE)=0.0004797845477573171\tTraining RMSE=0.02071443459961885\tValidation loss (MSE)=0.0005307004053234037\tValidation RMSE=0.018840023497533467\t\n",
      "Epoch 347 ->\tTraining loss (MSE)=0.0004752559719090192\tTraining RMSE=0.02058776867299996\tValidation loss (MSE)=0.0005307356025662523\tValidation RMSE=0.018840339345236618\t\n",
      "Epoch 348 ->\tTraining loss (MSE)=0.0004749938154059181\tTraining RMSE=0.02064690743515521\tValidation loss (MSE)=0.0005306777381553764\tValidation RMSE=0.018840958985189598\t\n",
      "Epoch 349 ->\tTraining loss (MSE)=0.0004776736623395585\tTraining RMSE=0.020814539738183403\tValidation loss (MSE)=0.0005306424725320001\tValidation RMSE=0.018839991513501714\t\n",
      "Epoch 350 ->\tTraining loss (MSE)=0.00047494771332159513\tTraining RMSE=0.02067058240810846\tValidation loss (MSE)=0.0005306079392876536\tValidation RMSE=0.018839375935149966\t\n",
      "Epoch 351 ->\tTraining loss (MSE)=0.00047475639992087425\tTraining RMSE=0.02061036048617996\tValidation loss (MSE)=0.0005305911275812563\tValidation RMSE=0.018838920828851836\t\n",
      "Epoch 352 ->\tTraining loss (MSE)=0.00047483039903723964\tTraining RMSE=0.020777936231482913\tValidation loss (MSE)=0.0005306178419333366\tValidation RMSE=0.01884021430655762\t\n",
      "Epoch 353 ->\tTraining loss (MSE)=0.0004807203351649352\tTraining RMSE=0.02071840817361702\tValidation loss (MSE)=0.0005305746491541396\tValidation RMSE=0.018838822332866215\t\n",
      "Epoch 354 ->\tTraining loss (MSE)=0.00047994286895702295\tTraining RMSE=0.020799931753886704\tValidation loss (MSE)=0.0005305987853245353\tValidation RMSE=0.0188395825335411\t\n",
      "Epoch 355 ->\tTraining loss (MSE)=0.0004774165118075792\tTraining RMSE=0.020850805657697313\tValidation loss (MSE)=0.0005305740829094753\tValidation RMSE=0.018839268393262668\t\n",
      "Epoch 356 ->\tTraining loss (MSE)=0.00047527154504082935\tTraining RMSE=0.020702677704163907\tValidation loss (MSE)=0.0005305779087831508\tValidation RMSE=0.01883902234301247\t\n",
      "Epoch 357 ->\tTraining loss (MSE)=0.0004768918260882025\tTraining RMSE=0.020750174273012414\tValidation loss (MSE)=0.0005305961240082979\tValidation RMSE=0.01884017307828698\t\n",
      "Epoch 358 ->\tTraining loss (MSE)=0.00047558521938849424\tTraining RMSE=0.02087429781547851\tValidation loss (MSE)=0.0005305438349346837\tValidation RMSE=0.018839253069556975\t\n",
      "Epoch 359 ->\tTraining loss (MSE)=0.0004754070417186961\tTraining RMSE=0.020756566402628834\tValidation loss (MSE)=0.0005304405089932347\tValidation RMSE=0.018835932093982894\t\n",
      "Epoch 360 ->\tTraining loss (MSE)=0.00047780671386899034\tTraining RMSE=0.02072859475573088\tValidation loss (MSE)=0.0005304403330228524\tValidation RMSE=0.01883581213446127\t\n",
      "Epoch 361 ->\tTraining loss (MSE)=0.0004745552580937295\tTraining RMSE=0.020581098008946874\tValidation loss (MSE)=0.0005304642746287518\tValidation RMSE=0.01883694151830342\t\n",
      "Epoch 362 ->\tTraining loss (MSE)=0.00048065418575137257\tTraining RMSE=0.02069903590808404\tValidation loss (MSE)=0.000530433475769651\tValidation RMSE=0.01883573223043371\t\n",
      "Epoch 363 ->\tTraining loss (MSE)=0.0004750919088110095\tTraining RMSE=0.020695052285000314\tValidation loss (MSE)=0.0005304454794828349\tValidation RMSE=0.018836793964038843\t\n",
      "Epoch 364 ->\tTraining loss (MSE)=0.0004756223661163937\tTraining RMSE=0.020644890736604547\tValidation loss (MSE)=0.0005304067382422983\tValidation RMSE=0.018836154205793584\t\n",
      "Epoch 365 ->\tTraining loss (MSE)=0.00047509711900299106\tTraining RMSE=0.020517787298210608\tValidation loss (MSE)=0.0005303864827173702\tValidation RMSE=0.01883523787923709\t\n",
      "Epoch 366 ->\tTraining loss (MSE)=0.000474607054243672\tTraining RMSE=0.02058766101900902\tValidation loss (MSE)=0.000530394338999974\tValidation RMSE=0.018834336755659292\t\n",
      "Epoch 367 ->\tTraining loss (MSE)=0.00047435584091556253\tTraining RMSE=0.020610753742885995\tValidation loss (MSE)=0.0005304533776695651\tValidation RMSE=0.01883823116723862\t\n",
      "Epoch 368 ->\tTraining loss (MSE)=0.00047464667120409475\tTraining RMSE=0.020696391529737065\tValidation loss (MSE)=0.000530469705726891\tValidation RMSE=0.018839104066568392\t\n",
      "Epoch 369 ->\tTraining loss (MSE)=0.00047608394958876523\tTraining RMSE=0.020589043851941824\tValidation loss (MSE)=0.0005303562583220708\tValidation RMSE=0.01883458531530643\t\n",
      "Epoch 370 ->\tTraining loss (MSE)=0.0004744076392213443\tTraining RMSE=0.020696411636531537\tValidation loss (MSE)=0.0005302416630675671\tValidation RMSE=0.01883322430809063\t\n",
      "Epoch 371 ->\tTraining loss (MSE)=0.0004742183693990767\tTraining RMSE=0.020573195012539257\tValidation loss (MSE)=0.0005302577704882155\tValidation RMSE=0.01883470043712468\t\n",
      "Epoch 372 ->\tTraining loss (MSE)=0.00047445614424119684\tTraining RMSE=0.02047851847277747\tValidation loss (MSE)=0.0005301810293463792\tValidation RMSE=0.018832716823521035\t\n",
      "Epoch 373 ->\tTraining loss (MSE)=0.00047505042702714154\tTraining RMSE=0.020700970037989777\tValidation loss (MSE)=0.0005300755547068547\tValidation RMSE=0.01883104089992466\t\n",
      "Epoch 374 ->\tTraining loss (MSE)=0.00047830835279051424\tTraining RMSE=0.02069759781092957\tValidation loss (MSE)=0.0005300679222273175\tValidation RMSE=0.018831007277455042\t\n",
      "Epoch 375 ->\tTraining loss (MSE)=0.000479230729709983\tTraining RMSE=0.020742124854874464\tValidation loss (MSE)=0.0005300660347226464\tValidation RMSE=0.0188309647815509\t\n",
      "Epoch 376 ->\tTraining loss (MSE)=0.00047423080652578514\tTraining RMSE=0.020665425519792387\tValidation loss (MSE)=0.0005300415175750257\tValidation RMSE=0.01883175576761089\t\n",
      "Epoch 377 ->\tTraining loss (MSE)=0.00047543473931314727\tTraining RMSE=0.02075243895057083\tValidation loss (MSE)=0.0005299466877962308\tValidation RMSE=0.01882858758930255\t\n",
      "Epoch 378 ->\tTraining loss (MSE)=0.00047503154865684516\tTraining RMSE=0.020784288018160027\tValidation loss (MSE)=0.0005298399786018612\tValidation RMSE=0.018827653032968992\t\n",
      "Epoch 379 ->\tTraining loss (MSE)=0.00047441054697696083\tTraining RMSE=0.02070592411931742\tValidation loss (MSE)=0.000529834684668997\tValidation RMSE=0.01882927641445012\t\n",
      "Epoch 380 ->\tTraining loss (MSE)=0.00047380739081824156\tTraining RMSE=0.0205695196500416\tValidation loss (MSE)=0.0005296649041055287\tValidation RMSE=0.018826374939332407\t\n",
      "Epoch 381 ->\tTraining loss (MSE)=0.0004738495892390212\tTraining RMSE=0.02057438622010343\tValidation loss (MSE)=0.0005290495473336368\tValidation RMSE=0.018816420955031558\t\n",
      "Epoch 382 ->\tTraining loss (MSE)=0.0004731086969457712\tTraining RMSE=0.02062202079716012\tValidation loss (MSE)=0.0005288507247853515\tValidation RMSE=0.018813687997559708\t\n",
      "Epoch 383 ->\tTraining loss (MSE)=0.00047477892044691636\tTraining RMSE=0.020619509718668314\tValidation loss (MSE)=0.0005288331873695521\tValidation RMSE=0.01881506277627691\t\n",
      "Epoch 384 ->\tTraining loss (MSE)=0.0004732848727952741\tTraining RMSE=0.020707450582887286\tValidation loss (MSE)=0.0005288036794539679\tValidation RMSE=0.01881300699379709\t\n",
      "Epoch 385 ->\tTraining loss (MSE)=0.00048073864035145955\tTraining RMSE=0.02076795120022179\tValidation loss (MSE)=0.0005287760691485515\tValidation RMSE=0.018813156358966673\t\n",
      "Epoch 386 ->\tTraining loss (MSE)=0.0004752236574275252\tTraining RMSE=0.020704807661887672\tValidation loss (MSE)=0.0005287721375040106\tValidation RMSE=0.018812560605713062\t\n",
      "Epoch 387 ->\tTraining loss (MSE)=0.00047550933649526323\tTraining RMSE=0.020565305304725046\tValidation loss (MSE)=0.0005286353258540017\tValidation RMSE=0.018807639548968937\t\n",
      "Epoch 388 ->\tTraining loss (MSE)=0.0004731336245225257\tTraining RMSE=0.02050813298419486\tValidation loss (MSE)=0.0005283954261459359\tValidation RMSE=0.01880464876174099\t\n",
      "Epoch 389 ->\tTraining loss (MSE)=0.00047371784572016715\tTraining RMSE=0.020697225189917248\tValidation loss (MSE)=0.0005278653586207872\tValidation RMSE=0.01879711085240598\t\n",
      "Epoch 390 ->\tTraining loss (MSE)=0.0004762899365075246\tTraining RMSE=0.020791970633177295\tValidation loss (MSE)=0.0005276858013934947\tValidation RMSE=0.018794823412059083\t\n",
      "Epoch 391 ->\tTraining loss (MSE)=0.0004723144418586557\tTraining RMSE=0.020586440684618772\tValidation loss (MSE)=0.0005275799314943099\tValidation RMSE=0.018793847084183385\t\n",
      "Epoch 392 ->\tTraining loss (MSE)=0.0004722324985867786\tTraining RMSE=0.020639117226510503\tValidation loss (MSE)=0.0005276665335124428\tValidation RMSE=0.01879438530239794\t\n",
      "Epoch 393 ->\tTraining loss (MSE)=0.00047233381741938903\tTraining RMSE=0.02058897406598668\tValidation loss (MSE)=0.0005275201896168895\tValidation RMSE=0.018793162570714398\t\n",
      "Epoch 394 ->\tTraining loss (MSE)=0.00047298177544431426\tTraining RMSE=0.020585595998039215\tValidation loss (MSE)=0.0005274446343214044\tValidation RMSE=0.018792773148527852\t\n",
      "Epoch 395 ->\tTraining loss (MSE)=0.0004722274354859155\tTraining RMSE=0.020574645998738247\tValidation loss (MSE)=0.0005273429119875396\tValidation RMSE=0.018791336281639006\t\n",
      "Epoch 396 ->\tTraining loss (MSE)=0.00047229313985340407\tTraining RMSE=0.020817523733080354\tValidation loss (MSE)=0.0005271905952718549\tValidation RMSE=0.018787305103614926\t\n",
      "Epoch 397 ->\tTraining loss (MSE)=0.00047207674746190334\tTraining RMSE=0.02058173671801701\tValidation loss (MSE)=0.0005271203203678683\tValidation RMSE=0.018786808346501656\t\n",
      "Epoch 398 ->\tTraining loss (MSE)=0.0004742211304902513\tTraining RMSE=0.020623155743067646\tValidation loss (MSE)=0.0005271179465866966\tValidation RMSE=0.018786789504466234\t\n",
      "Epoch 399 ->\tTraining loss (MSE)=0.0004797476249672242\tTraining RMSE=0.020798287432019908\tValidation loss (MSE)=0.0005270854467030235\tValidation RMSE=0.018786151686476335\t\n",
      "Epoch 400 ->\tTraining loss (MSE)=0.0004721833713883376\tTraining RMSE=0.020579841482326573\tValidation loss (MSE)=0.0005260093153925414\tValidation RMSE=0.018766040137658518\t\n",
      "Epoch 401 ->\tTraining loss (MSE)=0.00047388313540090796\tTraining RMSE=0.02053499379321749\tValidation loss (MSE)=0.0005259119867938958\tValidation RMSE=0.018764823761389212\t\n",
      "Epoch 402 ->\tTraining loss (MSE)=0.0004706302528193416\tTraining RMSE=0.02055253007127271\tValidation loss (MSE)=0.0005259607328803719\tValidation RMSE=0.018767708800388156\t\n",
      "Epoch 403 ->\tTraining loss (MSE)=0.0004708792087816012\tTraining RMSE=0.020602283361195413\tValidation loss (MSE)=0.0005259356143209583\tValidation RMSE=0.018767883449240966\t\n",
      "Epoch 404 ->\tTraining loss (MSE)=0.0004735780996809438\tTraining RMSE=0.020810633984014944\tValidation loss (MSE)=0.0005258389842142437\tValidation RMSE=0.018763909314724582\t\n",
      "Epoch 405 ->\tTraining loss (MSE)=0.000473075125733481\tTraining RMSE=0.02051434741516448\tValidation loss (MSE)=0.0005258667417230619\tValidation RMSE=0.018765121129237942\t\n",
      "Epoch 406 ->\tTraining loss (MSE)=0.00047792991698557155\tTraining RMSE=0.020758300680483198\tValidation loss (MSE)=0.0005258264837148427\tValidation RMSE=0.018763071098537358\t\n",
      "Epoch 407 ->\tTraining loss (MSE)=0.0004705456740563853\tTraining RMSE=0.020449893428357662\tValidation loss (MSE)=0.0005257932620965961\tValidation RMSE=0.018763862705479067\t\n",
      "Epoch 408 ->\tTraining loss (MSE)=0.0004714136341562276\tTraining RMSE=0.02068226918992069\tValidation loss (MSE)=0.0005257425904414116\tValidation RMSE=0.01876206232096862\t\n",
      "Epoch 409 ->\tTraining loss (MSE)=0.00047079875526204564\tTraining RMSE=0.020452834818121275\tValidation loss (MSE)=0.0005257395026058441\tValidation RMSE=0.018762682090271956\t\n",
      "Epoch 410 ->\tTraining loss (MSE)=0.00047171037766491754\tTraining RMSE=0.02067082576499677\tValidation loss (MSE)=0.0005257020840400839\tValidation RMSE=0.01876127974268187\t\n",
      "Epoch 411 ->\tTraining loss (MSE)=0.0004705549041700354\tTraining RMSE=0.020586618095820332\tValidation loss (MSE)=0.0005256838002322991\tValidation RMSE=0.01876100369177207\t\n",
      "Epoch 412 ->\tTraining loss (MSE)=0.0004736034902129581\tTraining RMSE=0.020766873734738723\tValidation loss (MSE)=0.0005257320731794203\tValidation RMSE=0.018762519134691468\t\n",
      "Epoch 413 ->\tTraining loss (MSE)=0.0004713844366815016\tTraining RMSE=0.020698945055267325\tValidation loss (MSE)=0.0005256888500837333\tValidation RMSE=0.018760529113933444\t\n",
      "Epoch 414 ->\tTraining loss (MSE)=0.0004709617824568183\tTraining RMSE=0.020539397816461177\tValidation loss (MSE)=0.0005257153672438882\tValidation RMSE=0.018762548264391995\t\n",
      "Epoch 415 ->\tTraining loss (MSE)=0.00047070808858857893\tTraining RMSE=0.02056361196194718\tValidation loss (MSE)=0.0005255675523248674\tValidation RMSE=0.01875951062646453\t\n",
      "Epoch 416 ->\tTraining loss (MSE)=0.00047232594022963536\tTraining RMSE=0.020803363473488042\tValidation loss (MSE)=0.0005255464777831178\tValidation RMSE=0.018759005884122517\t\n",
      "Epoch 417 ->\tTraining loss (MSE)=0.00047818872517831393\tTraining RMSE=0.02076061452842421\tValidation loss (MSE)=0.0005255642387998739\tValidation RMSE=0.018759205825281917\t\n",
      "Epoch 418 ->\tTraining loss (MSE)=0.0004709001446983929\tTraining RMSE=0.02062111208502801\tValidation loss (MSE)=0.0005255422322618499\tValidation RMSE=0.01875887370530378\t\n",
      "Epoch 419 ->\tTraining loss (MSE)=0.00047573507977232777\tTraining RMSE=0.02066142618495189\tValidation loss (MSE)=0.0005255005008722139\tValidation RMSE=0.01875836816098955\t\n",
      "Epoch 420 ->\tTraining loss (MSE)=0.0004785612355242196\tTraining RMSE=0.020668309328525706\tValidation loss (MSE)=0.0005254819015708226\tValidation RMSE=0.018757919160028298\t\n",
      "Epoch 421 ->\tTraining loss (MSE)=0.00047021000540957577\tTraining RMSE=0.020469146800790854\tValidation loss (MSE)=0.0005254013167022667\tValidation RMSE=0.018756419308138667\t\n",
      "Epoch 422 ->\tTraining loss (MSE)=0.00047089158487110974\tTraining RMSE=0.02049860583020397\tValidation loss (MSE)=0.0005254206461594785\tValidation RMSE=0.01875664536082358\t\n",
      "Epoch 423 ->\tTraining loss (MSE)=0.0004705403970333857\tTraining RMSE=0.020547629276543487\tValidation loss (MSE)=0.0005253508403526481\tValidation RMSE=0.018755082118635375\t\n",
      "Epoch 424 ->\tTraining loss (MSE)=0.0004742115917098191\tTraining RMSE=0.020723011430904822\tValidation loss (MSE)=0.0005253452363831457\tValidation RMSE=0.01875487762434339\t\n",
      "Epoch 425 ->\tTraining loss (MSE)=0.00047029830369146907\tTraining RMSE=0.02049135704766269\tValidation loss (MSE)=0.0005253428773559992\tValidation RMSE=0.018755180804334855\t\n",
      "Epoch 426 ->\tTraining loss (MSE)=0.0004783638285426818\tTraining RMSE=0.02081543393145649\tValidation loss (MSE)=0.0005253259526027789\tValidation RMSE=0.018754815372327965\t\n",
      "Epoch 427 ->\tTraining loss (MSE)=0.00048358505474975516\tTraining RMSE=0.020814967264110844\tValidation loss (MSE)=0.0005253360606595244\tValidation RMSE=0.01875553022276748\t\n",
      "Epoch 428 ->\tTraining loss (MSE)=0.0004714729676143853\tTraining RMSE=0.02060749323716686\tValidation loss (MSE)=0.0005253116791950392\tValidation RMSE=0.018754490737423853\t\n",
      "Epoch 429 ->\tTraining loss (MSE)=0.00047013266433808623\tTraining RMSE=0.020526108817760774\tValidation loss (MSE)=0.000525308999486798\tValidation RMSE=0.018754902459612047\t\n",
      "Epoch 430 ->\tTraining loss (MSE)=0.00047015760477127816\tTraining RMSE=0.020554073985643042\tValidation loss (MSE)=0.000525256526762624\tValidation RMSE=0.018752804284708366\t\n",
      "Epoch 431 ->\tTraining loss (MSE)=0.00047150055710247854\tTraining RMSE=0.020614251293195985\tValidation loss (MSE)=0.0005252302600166769\tValidation RMSE=0.018752647546568402\t\n",
      "Epoch 432 ->\tTraining loss (MSE)=0.0004700404858823153\tTraining RMSE=0.020515128725817726\tValidation loss (MSE)=0.0005252878073170669\tValidation RMSE=0.01875483865539233\t\n",
      "Epoch 433 ->\tTraining loss (MSE)=0.0004739648540837354\tTraining RMSE=0.02047067395710375\tValidation loss (MSE)=0.0005252487577915114\tValidation RMSE=0.01875380316266307\t\n",
      "Epoch 434 ->\tTraining loss (MSE)=0.0004704484542602541\tTraining RMSE=0.020543828480129624\tValidation loss (MSE)=0.0005252279272638218\tValidation RMSE=0.018752383439008286\t\n",
      "Epoch 435 ->\tTraining loss (MSE)=0.0004759092087534768\tTraining RMSE=0.02065906274120933\tValidation loss (MSE)=0.0005251947003907171\tValidation RMSE=0.01875192484887386\t\n",
      "Epoch 436 ->\tTraining loss (MSE)=0.0004718011671313973\tTraining RMSE=0.020541114775739885\tValidation loss (MSE)=0.0005253232334157305\tValidation RMSE=0.018759822697137243\t\n",
      "Epoch 437 ->\tTraining loss (MSE)=0.0004705167432769452\tTraining RMSE=0.02061835971063026\tValidation loss (MSE)=0.0005250458397548991\tValidation RMSE=0.01874976863015305\t\n",
      "Epoch 438 ->\tTraining loss (MSE)=0.0004715843233610446\tTraining RMSE=0.0204721841742687\tValidation loss (MSE)=0.0005250457432810907\tValidation RMSE=0.018749963017870432\t\n",
      "Epoch 439 ->\tTraining loss (MSE)=0.0004698986744143092\tTraining RMSE=0.020661290013518782\tValidation loss (MSE)=0.0005250306066618241\tValidation RMSE=0.01874947632421498\t\n",
      "Epoch 440 ->\tTraining loss (MSE)=0.00047535674226018286\tTraining RMSE=0.020711680724756952\tValidation loss (MSE)=0.0005252264501096862\tValidation RMSE=0.018757871446993063\t\n",
      "Epoch 441 ->\tTraining loss (MSE)=0.0004703566320569263\tTraining RMSE=0.0205415015735514\tValidation loss (MSE)=0.0005245638739897577\tValidation RMSE=0.018739816025589353\t\n",
      "Epoch 442 ->\tTraining loss (MSE)=0.00047133986412024377\tTraining RMSE=0.020539946917352486\tValidation loss (MSE)=0.0005243406971270882\tValidation RMSE=0.018734279149039475\t\n",
      "Epoch 443 ->\tTraining loss (MSE)=0.00047007040778641374\tTraining RMSE=0.020623933656118167\tValidation loss (MSE)=0.0005243306439768747\tValidation RMSE=0.018733582097209163\t\n",
      "Epoch 444 ->\tTraining loss (MSE)=0.0004733896022554728\tTraining RMSE=0.020792942223955453\tValidation loss (MSE)=0.0005242471793112431\tValidation RMSE=0.018732713888985692\t\n",
      "Epoch 445 ->\tTraining loss (MSE)=0.0004694777986223028\tTraining RMSE=0.02046598362977858\tValidation loss (MSE)=0.0005242256224004517\tValidation RMSE=0.018733415036910662\t\n",
      "Epoch 446 ->\tTraining loss (MSE)=0.00047018844968898735\tTraining RMSE=0.020544478641018087\tValidation loss (MSE)=0.0005241473686005014\tValidation RMSE=0.018731633054644422\t\n",
      "Epoch 447 ->\tTraining loss (MSE)=0.00047024681717394506\tTraining RMSE=0.020599503323067852\tValidation loss (MSE)=0.0005241832009401319\tValidation RMSE=0.018732022054286465\t\n",
      "Epoch 448 ->\tTraining loss (MSE)=0.0004696572804176766\tTraining RMSE=0.020408438553136808\tValidation loss (MSE)=0.0005242219829395051\tValidation RMSE=0.0187316358572355\t\n",
      "Epoch 449 ->\tTraining loss (MSE)=0.00046965821232412434\tTraining RMSE=0.020541293132636283\tValidation loss (MSE)=0.0005242170601478494\tValidation RMSE=0.01873424281021235\t\n",
      "Epoch 450 ->\tTraining loss (MSE)=0.0004704651612063358\tTraining RMSE=0.020708586106559745\tValidation loss (MSE)=0.0005241803096227899\tValidation RMSE=0.01873196214782419\t\n",
      "Epoch 451 ->\tTraining loss (MSE)=0.00047015987863154425\tTraining RMSE=0.02058804512058419\tValidation loss (MSE)=0.0005240739442174176\tValidation RMSE=0.018730414996820467\t\n",
      "Epoch 452 ->\tTraining loss (MSE)=0.0004695916985063512\tTraining RMSE=0.020537365696473436\tValidation loss (MSE)=0.0005240543655586576\tValidation RMSE=0.018730649759096128\t\n",
      "Epoch 453 ->\tTraining loss (MSE)=0.00047098090277516205\tTraining RMSE=0.020396025037898877\tValidation loss (MSE)=0.0005240337275069949\tValidation RMSE=0.018729201112701384\t\n",
      "Epoch 454 ->\tTraining loss (MSE)=0.0004693175068058298\tTraining RMSE=0.020628280000600182\tValidation loss (MSE)=0.0005240276921001539\tValidation RMSE=0.01872947642200247\t\n",
      "Epoch 455 ->\tTraining loss (MSE)=0.00048138634174168225\tTraining RMSE=0.02081571688392648\tValidation loss (MSE)=0.0005240006023625567\tValidation RMSE=0.01872933943135043\t\n",
      "Epoch 456 ->\tTraining loss (MSE)=0.0004693215048771688\tTraining RMSE=0.02053010953521287\tValidation loss (MSE)=0.0005240232599640472\tValidation RMSE=0.01872969502410679\t\n",
      "Epoch 457 ->\tTraining loss (MSE)=0.00046940956639850303\tTraining RMSE=0.020636674488124288\tValidation loss (MSE)=0.0005239695552450872\tValidation RMSE=0.018727929426219175\t\n",
      "Epoch 458 ->\tTraining loss (MSE)=0.0004710899725686443\tTraining RMSE=0.020594158980213564\tValidation loss (MSE)=0.0005239599139949187\tValidation RMSE=0.01872805124838595\t\n",
      "Epoch 459 ->\tTraining loss (MSE)=0.00047541795724954016\tTraining RMSE=0.02077092609172802\tValidation loss (MSE)=0.0005239495669768614\tValidation RMSE=0.018728134679366595\t\n",
      "Epoch 460 ->\tTraining loss (MSE)=0.00048349130861705494\tTraining RMSE=0.02070582196990281\tValidation loss (MSE)=0.0005239395423241485\tValidation RMSE=0.018729095847380382\t\n",
      "Epoch 461 ->\tTraining loss (MSE)=0.00046984144817677724\tTraining RMSE=0.020707278495171556\tValidation loss (MSE)=0.000523930135723613\tValidation RMSE=0.01872724393831083\t\n",
      "Epoch 462 ->\tTraining loss (MSE)=0.0004796480892382758\tTraining RMSE=0.020643022957683346\tValidation loss (MSE)=0.0005238835540997874\tValidation RMSE=0.018727208755013568\t\n",
      "Epoch 463 ->\tTraining loss (MSE)=0.0004687273981430118\tTraining RMSE=0.020444287179773788\tValidation loss (MSE)=0.0005239460657591095\tValidation RMSE=0.01873023289738706\t\n",
      "Epoch 464 ->\tTraining loss (MSE)=0.00046870933211148777\tTraining RMSE=0.020532722057441225\tValidation loss (MSE)=0.0005238318625978772\tValidation RMSE=0.018726454789025918\t\n",
      "Epoch 465 ->\tTraining loss (MSE)=0.00046915141870102406\tTraining RMSE=0.02054608057908438\tValidation loss (MSE)=0.0005238574477617578\tValidation RMSE=0.01872626188452597\t\n",
      "Epoch 466 ->\tTraining loss (MSE)=0.00047033595789495547\tTraining RMSE=0.0207482720322815\tValidation loss (MSE)=0.0005238029102825008\tValidation RMSE=0.018726448580208752\t\n",
      "Epoch 467 ->\tTraining loss (MSE)=0.00046872049365503455\tTraining RMSE=0.02048286189394141\tValidation loss (MSE)=0.0005237767723479515\tValidation RMSE=0.018725943725763097\t\n",
      "Epoch 468 ->\tTraining loss (MSE)=0.0004695561452805392\tTraining RMSE=0.02063112529454592\tValidation loss (MSE)=0.0005237680859998498\tValidation RMSE=0.018726291781705286\t\n",
      "Epoch 469 ->\tTraining loss (MSE)=0.00047001765410697527\tTraining RMSE=0.020556071629448804\tValidation loss (MSE)=0.000523778344359016\tValidation RMSE=0.018727381437740945\t\n",
      "Epoch 470 ->\tTraining loss (MSE)=0.0004727210827111798\tTraining RMSE=0.02058210267317424\tValidation loss (MSE)=0.0005238189406992644\tValidation RMSE=0.01872831336395056\t\n",
      "Epoch 471 ->\tTraining loss (MSE)=0.0004714701192678609\tTraining RMSE=0.020522522636585765\tValidation loss (MSE)=0.0005236811612795227\tValidation RMSE=0.018723986654852826\t\n",
      "Epoch 472 ->\tTraining loss (MSE)=0.0004689872844603177\tTraining RMSE=0.02038718888214157\tValidation loss (MSE)=0.00052370510982453\tValidation RMSE=0.01872445406668164\t\n",
      "Epoch 473 ->\tTraining loss (MSE)=0.00046973431608982795\tTraining RMSE=0.02060067046571661\tValidation loss (MSE)=0.0005235216862342053\tValidation RMSE=0.01871845421070854\t\n",
      "Epoch 474 ->\tTraining loss (MSE)=0.0004854403743570682\tTraining RMSE=0.020540277490875235\tValidation loss (MSE)=0.0005235152012001318\tValidation RMSE=0.01871812718713449\t\n",
      "Epoch 475 ->\tTraining loss (MSE)=0.00047924535622623357\tTraining RMSE=0.02054167192797234\tValidation loss (MSE)=0.0005234928271588784\tValidation RMSE=0.018716816755908507\t\n",
      "Epoch 476 ->\tTraining loss (MSE)=0.000469150886321002\tTraining RMSE=0.02052659820765257\tValidation loss (MSE)=0.0005235290427663131\tValidation RMSE=0.018717990644897025\t\n",
      "Epoch 477 ->\tTraining loss (MSE)=0.00046860043432478746\tTraining RMSE=0.020497621425117057\tValidation loss (MSE)=0.0005234777313658183\tValidation RMSE=0.018716666476663063\t\n",
      "Epoch 478 ->\tTraining loss (MSE)=0.0004690349787217852\tTraining RMSE=0.020480778864527375\tValidation loss (MSE)=0.0005234908270116042\tValidation RMSE=0.018717579457356973\t\n",
      "Epoch 479 ->\tTraining loss (MSE)=0.0004732002345895406\tTraining RMSE=0.020798270354898257\tValidation loss (MSE)=0.0005235176610801352\tValidation RMSE=0.01871765348025494\t\n",
      "Epoch 480 ->\tTraining loss (MSE)=0.00046878185336633277\tTraining RMSE=0.02059170643419579\tValidation loss (MSE)=0.0005234646605121843\tValidation RMSE=0.018716486610679164\t\n",
      "Epoch 481 ->\tTraining loss (MSE)=0.00047290089073444335\tTraining RMSE=0.020664031137309878\tValidation loss (MSE)=0.0005234584710302039\tValidation RMSE=0.01871639707436164\t\n",
      "Epoch 482 ->\tTraining loss (MSE)=0.000474249147598051\tTraining RMSE=0.020684339703969015\tValidation loss (MSE)=0.0005235839137943307\tValidation RMSE=0.018718241360383452\t\n",
      "Epoch 483 ->\tTraining loss (MSE)=0.0004694361915976459\tTraining RMSE=0.020675743682838886\tValidation loss (MSE)=0.0005234640855094229\tValidation RMSE=0.018716522716675645\t\n",
      "Epoch 484 ->\tTraining loss (MSE)=0.0004685733074438758\tTraining RMSE=0.020458454306432863\tValidation loss (MSE)=0.0005234649524936685\tValidation RMSE=0.018716808882783407\t\n",
      "Epoch 485 ->\tTraining loss (MSE)=0.00047513232453234703\tTraining RMSE=0.020745691371920668\tValidation loss (MSE)=0.0005235313904079332\tValidation RMSE=0.018719501413956837\t\n",
      "Epoch 486 ->\tTraining loss (MSE)=0.0004773731118881569\tTraining RMSE=0.02060866905203848\tValidation loss (MSE)=0.0005235432470524554\tValidation RMSE=0.018717042075608065\t\n",
      "Epoch 487 ->\tTraining loss (MSE)=0.000470722265208117\tTraining RMSE=0.02065058747007523\tValidation loss (MSE)=0.0005234486158129854\tValidation RMSE=0.018716135131264175\t\n",
      "Epoch 488 ->\tTraining loss (MSE)=0.0004687837232200696\tTraining RMSE=0.020433911674276547\tValidation loss (MSE)=0.0005234225632361982\tValidation RMSE=0.018715689122607862\t\n",
      "Epoch 489 ->\tTraining loss (MSE)=0.0004687572390038339\tTraining RMSE=0.020419789665022198\tValidation loss (MSE)=0.0005234077562582334\tValidation RMSE=0.018715522545217363\t\n",
      "Epoch 490 ->\tTraining loss (MSE)=0.0004777299005963644\tTraining RMSE=0.020493255948279925\tValidation loss (MSE)=0.0005234786321024229\tValidation RMSE=0.018718065754337994\t\n",
      "Epoch 491 ->\tTraining loss (MSE)=0.00046862114174507077\tTraining RMSE=0.020518970192858466\tValidation loss (MSE)=0.0005234170415928087\tValidation RMSE=0.01871600284896515\t\n",
      "Epoch 492 ->\tTraining loss (MSE)=0.0004758985724936478\tTraining RMSE=0.02054635205961488\tValidation loss (MSE)=0.0005234305307466355\tValidation RMSE=0.0187162756488693\t\n",
      "Epoch 493 ->\tTraining loss (MSE)=0.00047224414627208594\tTraining RMSE=0.02071898526592571\tValidation loss (MSE)=0.0005234396960952802\tValidation RMSE=0.01871630954728634\t\n",
      "Epoch 494 ->\tTraining loss (MSE)=0.00047156052972207327\tTraining RMSE=0.02058346638524974\tValidation loss (MSE)=0.0005234091101926795\tValidation RMSE=0.018716222934286902\t\n",
      "Epoch 495 ->\tTraining loss (MSE)=0.00046924474181763305\tTraining RMSE=0.02057543892217915\tValidation loss (MSE)=0.000523381758992198\tValidation RMSE=0.018715210828102298\t\n",
      "Epoch 496 ->\tTraining loss (MSE)=0.0004684610093933355\tTraining RMSE=0.020531307514986875\tValidation loss (MSE)=0.0005233900318233753\tValidation RMSE=0.018715116566185047\t\n",
      "Epoch 497 ->\tTraining loss (MSE)=0.0004715229814241922\tTraining RMSE=0.020628425735336395\tValidation loss (MSE)=0.0005233666187349525\tValidation RMSE=0.0187147727098178\t\n",
      "Epoch 498 ->\tTraining loss (MSE)=0.00046878360956691687\tTraining RMSE=0.0204506853716103\tValidation loss (MSE)=0.0005234084596008028\tValidation RMSE=0.018714624845112365\t\n",
      "Epoch 499 ->\tTraining loss (MSE)=0.00046966642220928564\tTraining RMSE=0.0205822565799786\tValidation loss (MSE)=0.0005232600904289737\tValidation RMSE=0.018714004049629526\t"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics exported in the following csv file: logged_metrics/MultivariateNNtraining_metrics_22-05-2023_10-05-37.csv\n"
     ]
    }
   ],
   "source": [
    "input_space = train_dataloader.dataset.x.shape[1]\n",
    "output_space = train_dataloader.dataset.y.shape[1]\n",
    "print(f\"{input_space=}\\t{output_space=}\")\n",
    "\n",
    "class MultivariateNN(nn.Module):\n",
    "    def __init__(self, input_space, output_space) -> None:\n",
    "        super(MultivariateNN, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_space, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, output_space),\n",
    "            nn.ReLU() #TODO?\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "model = MultivariateNN(input_space, output_space).to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    (p for p in model.parameters() if p.requires_grad), lr=LR\n",
    ")\n",
    "\n",
    "loss_function = torch.nn.MSELoss()\n",
    "\n",
    "metrics_logger = MetricsLogger(name=\"MultivariateNN training\")\n",
    "best_v_loss = 1_000_000.\n",
    "for epoch in range(EPOCHS):\n",
    "    '''TRAINING'''\n",
    "    model.train(True)\n",
    "    epoch_t_loss = 0\n",
    "    epoch_t_rmse = 0\n",
    "    for batch in iter(train_dataloader):\n",
    "        input_features, wake_field = batch[0].to(DEVICE), batch[1].to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        prediction = model.forward(input_features)\n",
    "        tloss = loss_function(prediction, wake_field)\n",
    "        tloss.backward()\n",
    "        #torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5) TODO\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_t_loss += tloss.item()\n",
    "\n",
    "        # other metrics\n",
    "        rmse = metrics.field_based_rmse(prediction, wake_field).item()\n",
    "        epoch_t_rmse += rmse\n",
    "\n",
    "    avg_t_loss = epoch_t_loss / len(train_dataloader)\n",
    "    avg_t_rmse = epoch_t_rmse / len(train_dataloader)\n",
    "    metrics_logger.log_metric(epoch, 'Training loss (MSE)', avg_t_loss)\n",
    "    metrics_logger.log_metric(epoch, 'Training RMSE', avg_t_rmse)\n",
    "\n",
    "    '''VALIDATION'''\n",
    "    if valid_dataloader:\n",
    "        model.train(False)\n",
    "        epoch_v_loss = 0\n",
    "        epoch_v_rmse = 0\n",
    "        for batch in iter(valid_dataloader):\n",
    "            input_features, wake_field = batch[0].to(DEVICE), batch[1].to(DEVICE)\n",
    "            prediction = model(input_features)\n",
    "            vloss = loss_function(prediction, wake_field)\n",
    "\n",
    "            epoch_v_loss += vloss.item()\n",
    "            rmse = metrics.field_based_rmse(prediction, wake_field).item()\n",
    "            epoch_v_rmse += rmse\n",
    "\n",
    "        avg_v_loss = epoch_v_loss / len(valid_dataloader)\n",
    "        avg_v_rmse = epoch_v_rmse / len(valid_dataloader)\n",
    "        metrics_logger.log_metric(epoch, 'Validation loss (MSE)', avg_v_loss)\n",
    "        metrics_logger.log_metric(epoch, 'Validation RMSE', avg_v_rmse)\n",
    "        # Track best performance, and save the model's state\n",
    "        if avg_v_loss < best_v_loss:\n",
    "            best_vloss = avg_v_loss\n",
    "            torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
    "\n",
    "metrics_logger.plot_metrics_by_epoch()\n",
    "metrics_logger.save_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing results: avg loss=5.510379314391348e-05\n"
     ]
    }
   ],
   "source": [
    "# loading best model\n",
    "model = MultivariateNN(input_space, output_space).to(DEVICE)\n",
    "model.load_state_dict(torch.load(BEST_MODEL_PATH))\n",
    "model.train(False)\n",
    "\n",
    "total_loss = 0\n",
    "with torch.no_grad():\n",
    "    for batch in iter(test_dataloader):\n",
    "        # to device\n",
    "        x, y = batch[0].to(DEVICE), batch[1].to(DEVICE)\n",
    "\n",
    "        prediction = model(x)\n",
    "        tloss = loss_function(prediction, y)\n",
    "\n",
    "        total_loss += tloss.item()\n",
    "\n",
    "avg_loss = total_loss / len(test_dataloader) / BATCH_SIZE #TODO\n",
    "print(f\"Testing results: avg loss={avg_loss}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
