{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpytorch\n",
    "import torch\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "\n",
    "from utils.metrics import MetricsLogger\n",
    "from utils.data_loaders import get_wake_dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE=device(type='cpu')\n"
     ]
    }
   ],
   "source": [
    "FACTORS_FOLDER = \"discr_factors_x2_30_y-2_2_step0.125_TIstep0.01_CTstep0.01\"\n",
    "DATA_FOLDER = f\"data/{FACTORS_FOLDER}/\"\n",
    "MODEL_NAME = f\"multivariate_GP_ExactGP\" #TODO\n",
    "BEST_MODEL_PATH = f\"saved_models/{FACTORS_FOLDER}/{MODEL_NAME}.pt\"\n",
    "CONSIDER_WS = False #TODO\n",
    "COORDS_AS_INPUT = False # multivariate setting\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"{DEVICE=}\")\n",
    "\n",
    "# hyperparameters\n",
    "EPOCHS = 50\n",
    "LR = 0.1\n",
    "BATCH_SIZE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5160\n",
      "Train shapes:  torch.Size([5160, 2]) torch.Size([5160, 7168])\n",
      "Test shapes:  torch.Size([1720, 2]) torch.Size([1720, 7168])\n",
      "Valid shapes:  torch.Size([1720, 2]) torch.Size([1720, 7168])\n",
      "grid_size=7168\n"
     ]
    }
   ],
   "source": [
    "valid_dataloader = None\n",
    "train_dataloader, valid_dataloader, test_dataloader = get_wake_dataloaders(DATA_FOLDER,\n",
    "                                                          consider_ws=CONSIDER_WS,\n",
    "                                                          coords_as_input=COORDS_AS_INPUT,\n",
    "                                                          train_perc=0.6,\n",
    "                                                          test_perc=0.2,\n",
    "                                                          validation_perc=0.2,\n",
    "                                                          batch_size=BATCH_SIZE)\n",
    "\n",
    "print(len(train_dataloader))\n",
    "\n",
    "train_x, train_y = train_dataloader.dataset.inputs, train_dataloader.dataset.outputs\n",
    "\n",
    "test_x, test_y = test_dataloader.dataset.inputs, test_dataloader.dataset.outputs\n",
    "print(\"Train shapes: \", train_x.shape, train_y.shape)\n",
    "print(\"Test shapes: \", test_x.shape, test_y.shape)\n",
    "\n",
    "if valid_dataloader is not None:\n",
    "    valid_x, valid_y = valid_dataloader.dataset.inputs, valid_dataloader.dataset.outputs\n",
    "    print(\"Valid shapes: \", valid_x.shape, valid_y.shape)\n",
    "\n",
    "grid_size = train_dataloader.dataset.num_cells\n",
    "print(f\"{grid_size=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([71680, 4]) torch.Size([71680])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "REDUCE THE AMOUNT OF DATA\n",
    "num_instances = 10 #TODO\n",
    "train_x= torch.split(train_x, num_instances)[0]#.reshape(num_instances, grid_size, num_features)\n",
    "train_y = torch.split(train_y,num_instances)[0]#.reshape(num_instances, grid_size)\n",
    "print(train_x.shape, train_y.shape)\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nToo much data for simple Regressor\\nkernel = RBF(length_scale=1.0)\\ngp = GaussianProcessRegressor(kernel=kernel)\\ngp.fit(train_x, train_y)\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel = RBF(length_scale=1.0)\n",
    "gp = GaussianProcessRegressor(kernel=kernel)\n",
    "gp.fit(train_x, train_y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gpytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultitaskGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(MultitaskGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.MultitaskMean(\n",
    "            gpytorch.means.ConstantMean(), num_tasks=grid_size\n",
    "        )\n",
    "        self.covar_module = gpytorch.kernels.MultitaskKernel(\n",
    "            gpytorch.kernels.RBFKernel(), num_tasks=grid_size, rank=1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultitaskMultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=grid_size)\n",
    "model = MultitaskGPModel(train_x, train_y, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging multivariate_GP_ExactGP\n",
      "Epoch 0 ->\tTraining loss=1.0853651762008667\t\n",
      "Epoch 1 ->\tTraining loss=1.0487637519836426\t\n",
      "Epoch 2 ->\tTraining loss=1.0113800764083862\t\n",
      "Epoch 3 ->\tTraining loss=0.9732176661491394\t\n",
      "Epoch 4 ->\tTraining loss=0.9342836141586304\t\n",
      "Epoch 5 ->\tTraining loss=0.8945872187614441\t\n",
      "Epoch 6 ->\tTraining loss=0.85414057970047\t\n",
      "Epoch 7 ->\tTraining loss=0.8129580616950989\t\n",
      "Epoch 8 ->\tTraining loss=0.7710564136505127\t\n",
      "Epoch 9 ->\tTraining loss=0.7284545302391052\t\n",
      "Epoch 10 ->\tTraining loss=0.6851732134819031\t\n",
      "Epoch 11 ->\tTraining loss=0.6412352919578552\t\n",
      "Epoch 12 ->\tTraining loss=0.5966648459434509\t\n",
      "Epoch 13 ->\tTraining loss=0.5514872074127197\t\n",
      "Epoch 14 ->\tTraining loss=nan\t\n",
      "Epoch 15 ->\tTraining loss=nan\t\n",
      "Epoch 16 ->\tTraining loss=nan\t"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     15\u001b[0m output \u001b[39m=\u001b[39m model(train_x)\n\u001b[0;32m---> 16\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mmll(output, train_y)\n\u001b[1;32m     17\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     18\u001b[0m loss_value \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/gpytorch/module.py:31\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[Tensor, Distribution, LinearOperator]:\n\u001b[0;32m---> 31\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     32\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(outputs, \u001b[39mlist\u001b[39m):\n\u001b[1;32m     33\u001b[0m         \u001b[39mreturn\u001b[39;00m [_validate_module_outputs(output) \u001b[39mfor\u001b[39;00m output \u001b[39min\u001b[39;00m outputs]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/gpytorch/mlls/exact_marginal_log_likelihood.py:64\u001b[0m, in \u001b[0;36mExactMarginalLogLikelihood.forward\u001b[0;34m(self, function_dist, target, *params)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39m# Get the log prob of the marginal distribution\u001b[39;00m\n\u001b[1;32m     63\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlikelihood(function_dist, \u001b[39m*\u001b[39mparams)\n\u001b[0;32m---> 64\u001b[0m res \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39;49mlog_prob(target)\n\u001b[1;32m     65\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_add_other_terms(res, params)\n\u001b[1;32m     67\u001b[0m \u001b[39m# Scale by the amount of data we have\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/gpytorch/distributions/multitask_multivariate_normal.py:212\u001b[0m, in \u001b[0;36mMultitaskMultivariateNormal.log_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    210\u001b[0m     new_shape \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39mshape[:\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m] \u001b[39m+\u001b[39m value\u001b[39m.\u001b[39mshape[:\u001b[39m-\u001b[39m\u001b[39m3\u001b[39m:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    211\u001b[0m     value \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39mview(new_shape)\u001b[39m.\u001b[39mtranspose(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mcontiguous()\n\u001b[0;32m--> 212\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlog_prob(value\u001b[39m.\u001b[39;49mreshape(\u001b[39m*\u001b[39;49mvalue\u001b[39m.\u001b[39;49mshape[:\u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m], \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/gpytorch/distributions/multivariate_normal.py:193\u001b[0m, in \u001b[0;36mMultivariateNormal.log_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[39m# Get log determininant and first part of quadratic form\u001b[39;00m\n\u001b[1;32m    192\u001b[0m covar \u001b[39m=\u001b[39m covar\u001b[39m.\u001b[39mevaluate_kernel()\n\u001b[0;32m--> 193\u001b[0m inv_quad, logdet \u001b[39m=\u001b[39m covar\u001b[39m.\u001b[39;49minv_quad_logdet(inv_quad_rhs\u001b[39m=\u001b[39;49mdiff\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m), logdet\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    195\u001b[0m res \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m0.5\u001b[39m \u001b[39m*\u001b[39m \u001b[39msum\u001b[39m([inv_quad, logdet, diff\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m math\u001b[39m.\u001b[39mlog(\u001b[39m2\u001b[39m \u001b[39m*\u001b[39m math\u001b[39m.\u001b[39mpi)])\n\u001b[1;32m    196\u001b[0m \u001b[39mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/linear_operator/operators/kronecker_product_added_diag_linear_operator.py:64\u001b[0m, in \u001b[0;36mKroneckerProductAddedDiagLinearOperator.inv_quad_logdet\u001b[0;34m(self, inv_quad_rhs, logdet, reduce_inv_quad)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minv_quad_logdet\u001b[39m(\u001b[39mself\u001b[39m, inv_quad_rhs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, logdet\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, reduce_inv_quad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m     63\u001b[0m     \u001b[39mif\u001b[39;00m inv_quad_rhs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m         inv_quad_term, _ \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49minv_quad_logdet(\n\u001b[1;32m     65\u001b[0m             inv_quad_rhs\u001b[39m=\u001b[39;49minv_quad_rhs, logdet\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, reduce_inv_quad\u001b[39m=\u001b[39;49mreduce_inv_quad\n\u001b[1;32m     66\u001b[0m         )\n\u001b[1;32m     67\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     68\u001b[0m         inv_quad_term \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/linear_operator/operators/_linear_operator.py:1626\u001b[0m, in \u001b[0;36mLinearOperator.inv_quad_logdet\u001b[0;34m(self, inv_quad_rhs, logdet, reduce_inv_quad)\u001b[0m\n\u001b[1;32m   1624\u001b[0m     \u001b[39mif\u001b[39;00m inv_quad_rhs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1625\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mEither `inv_quad_rhs` or `logdet` must be specifed.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1626\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minv_quad(inv_quad_rhs, reduce_inv_quad\u001b[39m=\u001b[39;49mreduce_inv_quad), torch\u001b[39m.\u001b[39mzeros(\n\u001b[1;32m   1627\u001b[0m         [], dtype\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\n\u001b[1;32m   1628\u001b[0m     )\n\u001b[1;32m   1630\u001b[0m \u001b[39m# Default: use modified batch conjugate gradients to compute these terms\u001b[39;00m\n\u001b[1;32m   1631\u001b[0m \u001b[39m# See NeurIPS 2018 paper: https://arxiv.org/abs/1809.11165\u001b[39;00m\n\u001b[1;32m   1632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_square:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/linear_operator/operators/_linear_operator.py:1574\u001b[0m, in \u001b[0;36mLinearOperator.inv_quad\u001b[0;34m(self, inv_quad_rhs, reduce_inv_quad)\u001b[0m\n\u001b[1;32m   1572\u001b[0m args \u001b[39m=\u001b[39m (inv_quad_rhs\u001b[39m.\u001b[39mexpand(\u001b[39m*\u001b[39mresult_shape[:\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m], \u001b[39m*\u001b[39minv_quad_rhs\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m:]),) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrepresentation()\n\u001b[1;32m   1573\u001b[0m func \u001b[39m=\u001b[39m InvQuad\u001b[39m.\u001b[39mapply\n\u001b[0;32m-> 1574\u001b[0m inv_quad_term \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrepresentation_tree(), \u001b[39m*\u001b[39;49margs)\n\u001b[1;32m   1576\u001b[0m \u001b[39mif\u001b[39;00m reduce_inv_quad:\n\u001b[1;32m   1577\u001b[0m     inv_quad_term \u001b[39m=\u001b[39m inv_quad_term\u001b[39m.\u001b[39msum(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/autograd/function.py:506\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    504\u001b[0m     \u001b[39m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    505\u001b[0m     args \u001b[39m=\u001b[39m _functorch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 506\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mapply(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39msetup_context \u001b[39m==\u001b[39m _SingleLevelFunction\u001b[39m.\u001b[39msetup_context:\n\u001b[1;32m    509\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    510\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    511\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    512\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mstaticmethod. For more details, please see \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    513\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/linear_operator/functions/_inv_quad.py:51\u001b[0m, in \u001b[0;36mInvQuad.forward\u001b[0;34m(ctx, representation_tree, *args)\u001b[0m\n\u001b[1;32m     48\u001b[0m     ctx\u001b[39m.\u001b[39mis_vector \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[39m# Perform solves (for inv_quad) and tridiagonalization (for estimating logdet)\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m inv_quad_solves \u001b[39m=\u001b[39m _solve(linear_op, inv_quad_rhs)\n\u001b[1;32m     52\u001b[0m inv_quad_term \u001b[39m=\u001b[39m (inv_quad_solves \u001b[39m*\u001b[39m inv_quad_rhs)\u001b[39m.\u001b[39msum(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m     54\u001b[0m to_save \u001b[39m=\u001b[39m matrix_args \u001b[39m+\u001b[39m [inv_quad_solves]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/linear_operator/functions/_inv_quad.py:19\u001b[0m, in \u001b[0;36m_solve\u001b[0;34m(linear_op, rhs)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     18\u001b[0m     preconditioner \u001b[39m=\u001b[39m linear_op\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39m_solve_preconditioner()\n\u001b[0;32m---> 19\u001b[0m \u001b[39mreturn\u001b[39;00m linear_op\u001b[39m.\u001b[39;49m_solve(rhs, preconditioner)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)  # Includes GaussianLikelihood parameters\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "metrics_logger = MetricsLogger(name=MODEL_NAME, automatic_save_after=5)\n",
    "best_tloss = 1_000_000.\n",
    "for epoch in range(EPOCHS):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(train_x)\n",
    "    loss = -mll(output, train_y)\n",
    "    loss.backward()\n",
    "    loss_value = loss.item()\n",
    "    metrics_logger.log_metric(epoch, 'Training loss', loss_value)\n",
    "    if loss_value < best_tloss:\n",
    "        best_tloss = loss_value\n",
    "        torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
    "    optimizer.step()\n",
    "\n",
    "metrics_logger.plot_metrics_by_epoch()\n",
    "metrics_logger.save_metrics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
