{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pyperclip\n",
    "import math\n",
    "import re\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import sys\n",
    "from typing import Optional\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "from src.logging_metrics import MetricsLogger\n",
    "\n",
    "RESULT_FOLDER = \"../../metrics/final_results/\"\n",
    "METRICS_LOGGER_FOLDER = \"../../metrics/logged_metrics/\"\n",
    "\n",
    "SET_TYPES = [\"test\", \"train\"]\n",
    "EXPERIMENTS = [\"interpolation\", \"interpolation_coords\", \"extrapolation\"]\n",
    "VALID_EXPERIMENTS = EXPERIMENTS + [\"OLD\", \"DTdepth\"]\n",
    "\n",
    "# TODO fix the following lists\n",
    "USELESS_METRICS = [\n",
    "    \"explained_variance_score\",\n",
    "    \"mean_absolute_percentage_error\",\n",
    "    \"median_absolute_error\",\n",
    "    \"timestamp\",\n",
    "]\n",
    "USEFUL_METRICS = [\"r2\", \"MSE\", \"MAE\", \"Prediction Time\"]  # \"PSNR\",\n",
    "assert set(USELESS_METRICS).isdisjoint(USEFUL_METRICS)\n",
    "USELESS_MODELS = [\"autoencoder\"]\n",
    "\n",
    "METRIC_TO_ABBREVIATION = {\n",
    "    \"r2_score\": \"r2\",\n",
    "    \"explained_variance_score\": \"EVS\",\n",
    "    \"mean_squared_error\": \"MSE\",\n",
    "    \"mean_absolute_error\": \"MAE\",\n",
    "    \"median_absolute_error\": \"MedAE\",\n",
    "    \"mean_absolute_percentage_error\": \"MAPE\",\n",
    "    \"peak_signal_noise_ratio\": \"PSNR\",\n",
    "}\n",
    "\n",
    "COLUMNS_TO_BE_MAXIMIZED = [\n",
    "    \"r2_score\",\n",
    "    \"explained_variance_score\",\n",
    "    \"peak_signal_noise_ratio\",\n",
    "]\n",
    "COLUMNS_TO_BE_MAXIMIZED += [\n",
    "    abb for m, abb in METRIC_TO_ABBREVIATION.items() if m in COLUMNS_TO_BE_MAXIMIZED\n",
    "]\n",
    "COLUMNS_TO_BE_MINIMIZED = [\n",
    "    \"prediction_time\",\n",
    "    \"mean_squared_error\",\n",
    "    \"mean_absolute_error\",\n",
    "    \"median_absolute_error\",\n",
    "    \"mean_absolute_percentage_error\",\n",
    "    \"Prediction Time\",\n",
    "]\n",
    "COLUMNS_TO_BE_MINIMIZED += [\n",
    "    abb for m, abb in METRIC_TO_ABBREVIATION.items() if m in COLUMNS_TO_BE_MINIMIZED\n",
    "]\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __find_most_recent_metrics_log(model_name: str) -> Optional[str]:\n",
    "    # code from MetricsLogger conventions\n",
    "    model_name = (\n",
    "        re.sub(r\"[^a-zA-Z0-9 -_]\", \"\", model_name)\n",
    "        .replace(\" \", \"-\")\n",
    "        .split(\"_discr_factors\")[0]\n",
    "    )\n",
    "    matching_files = [\n",
    "        file\n",
    "        for file in os.listdir(METRICS_LOGGER_FOLDER)\n",
    "        if file.startswith(model_name)\n",
    "    ]\n",
    "\n",
    "    if not matching_files:\n",
    "        # warnings.warn(f\"No files found in {METRICS_LOGGER_FOLDER} with the prefix {model_name}\")\n",
    "        return None\n",
    "\n",
    "    # Sort the files based on their timestamps (assuming the timestamp is at the end of the filename)\n",
    "    sorted_files = sorted(\n",
    "        matching_files,\n",
    "        key=lambda file: datetime.strptime(file[-23:-4], \"%d-%m-%Y_%H-%M-%S\"),\n",
    "        reverse=True,\n",
    "    )\n",
    "\n",
    "    return os.path.join(METRICS_LOGGER_FOLDER, sorted_files[0])\n",
    "\n",
    "\n",
    "def compute_training_time(model_name: str) -> float:\n",
    "    most_recent_log_file = __find_most_recent_metrics_log(model_name)\n",
    "    if most_recent_log_file is None:\n",
    "        return np.nan\n",
    "    metrics_logger = MetricsLogger.from_csv(most_recent_log_file)\n",
    "    training_time = metrics_logger.get_training_time(\"Validation loss\")\n",
    "    return np.nan if int(training_time) == 0 else training_time\n",
    "\n",
    "\n",
    "compute_training_time(\n",
    "    \"conv_autoencoders_channels2-4-8-8_kernels3-4-5-5_strides2-2-2-2_paddings1-1-1-1_lin-layers4096-2048-1024-512-256-128-64-32-16-8_consider_ws\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_metrics(\n",
    "    set_type: str,\n",
    "    experiment: str,\n",
    "    only_model_name: bool = True,\n",
    "    columns_to_remove: list = USELESS_METRICS,\n",
    "    infer_training_time_attempt: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    __check_arguments(set_type, experiment)\n",
    "    filepath = __get_filepath(set_type, experiment)\n",
    "    metrics = pd.read_csv(filepath)\n",
    "    # remove uninteresting records\n",
    "    for model_name in USELESS_MODELS:\n",
    "        metrics = metrics[~metrics[\"model_description\"].str.contains(model_name)]\n",
    "    # metrics = metrics[~metrics['model_description'].str.contains('consider_ws')] #TODO ?\n",
    "\n",
    "    metrics = __remove_duplicates(metrics)\n",
    "\n",
    "    if infer_training_time_attempt:  # TODO feature in progress\n",
    "        metrics[\"Training Time (s)\"] = metrics[\"model_description\"].apply(\n",
    "            compute_training_time\n",
    "        )\n",
    "\n",
    "    if only_model_name:\n",
    "        metrics = extract_model_name(metrics)\n",
    "        metrics.drop(\"model_description\", axis=1, inplace=True)\n",
    "        # metrics.rename(columns={\"model_description\": \"model_name\"}, inplace=True)\n",
    "        metrics.sort_values(by=\"model_name\")\n",
    "    else:\n",
    "        metrics.sort_values(by=\"model_description\")\n",
    "\n",
    "    if columns_to_remove:\n",
    "        metrics.drop(columns_to_remove, axis=1, inplace=True)\n",
    "\n",
    "    return metrics.rename(columns=__rename_column)\n",
    "\n",
    "\n",
    "def extract_model_name(\n",
    "    metrics_df: pd.DataFrame, remove_name_from_description: bool = False\n",
    ") -> pd.DataFrame:\n",
    "    if \"model_description\" in metrics_df.columns:\n",
    "        model_desc = \"model_description\"\n",
    "        model_name = \"model_name\"\n",
    "    else:\n",
    "        model_desc = \"Model Description\"\n",
    "        model_name = \"Model Name\"\n",
    "    metrics_df.insert(\n",
    "        0,\n",
    "        model_name,\n",
    "        metrics_df[model_desc]\n",
    "        .str.split(\"_\")\n",
    "        .str[:2]\n",
    "        .str.join(\" \")\n",
    "        .apply(__first_capital),\n",
    "    )\n",
    "    if remove_name_from_description:\n",
    "        metrics_df[model_desc] = (\n",
    "            metrics_df[model_desc].str.split(\"_\").str[2:].str.join(\"_\")\n",
    "        )\n",
    "    return metrics_df\n",
    "\n",
    "\n",
    "def __check_arguments(\n",
    "    set_type: Optional[str] = None,\n",
    "    experiment: Optional[str] = None,\n",
    "    metric: Optional[str] = None,\n",
    ") -> None:\n",
    "    if set_type and set_type not in SET_TYPES:\n",
    "        raise ValueError(\"set_type should be 'train' or 'test'\")\n",
    "    if experiment and experiment not in VALID_EXPERIMENTS:\n",
    "        raise ValueError(\"experiment argument not valid\")\n",
    "    if metric and metric not in METRIC_TO_ABBREVIATION.values():\n",
    "        raise ValueError(\n",
    "            f\"Not a valid (abbreviated) metric, choose among: {METRIC_TO_ABBREVIATION.values()}\"\n",
    "        )\n",
    "\n",
    "\n",
    "def __get_filepath(set_type: str, experiment: str) -> str:\n",
    "    __check_arguments(set_type, experiment)\n",
    "    filename = \"_\".join([f\"{set_type}set\", \"results\", experiment]) + \".csv\"\n",
    "    return os.path.join(RESULT_FOLDER, filename)\n",
    "\n",
    "\n",
    "def __remove_duplicates(\n",
    "    df: pd.DataFrame,\n",
    ") -> pd.DataFrame:  # choosing the best according to MSE\n",
    "    groups = df.groupby(\"model_description\")\n",
    "\n",
    "    # Find the row with minimum MSE for each group\n",
    "    best_records = groups.apply(\n",
    "        lambda group: group.loc[group[\"mean_squared_error\"].idxmin()]\n",
    "    )\n",
    "\n",
    "    # Keep only the best records in the DataFrame\n",
    "    df = best_records.reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def __rename_column(column_name: str) -> str:\n",
    "    if column_name in METRIC_TO_ABBREVIATION.keys():\n",
    "        return METRIC_TO_ABBREVIATION[column_name]\n",
    "    return column_name.replace(\"_\", \" \").title()\n",
    "\n",
    "\n",
    "def __first_capital(value: str) -> str:\n",
    "    return value[0].upper() + value[1:]\n",
    "\n",
    "\n",
    "def compare_metrics(\n",
    "    set_type1: str, experiment1: str, set_type2: str, experiment2: str, by_name: bool\n",
    ") -> pd.DataFrame:\n",
    "    if set_type1 == set_type2 and experiment1 == experiment2:\n",
    "        raise ValueError(\"types and experiments are the same -> impossible to merge\")\n",
    "\n",
    "    metrics1 = load_metrics(set_type1, experiment1, only_model_name=by_name)\n",
    "    metrics2 = load_metrics(set_type2, experiment2, only_model_name=by_name)\n",
    "\n",
    "    if set_type1 != set_type2 and experiment1 == experiment2:\n",
    "        suffix1 = f\" ({set_type1})\"\n",
    "        suffix2 = f\" ({set_type2})\"\n",
    "    elif set_type1 == set_type2 and experiment1 != experiment2:\n",
    "        suffix1 = f\" ({experiment1})\"\n",
    "        suffix2 = f\" ({experiment2})\"\n",
    "    elif set_type1 != set_type2 and experiment1 != experiment2:\n",
    "        suffix1 = f\" ({set_type1}, {experiment1})\"\n",
    "        suffix2 = f\" ({set_type2}, {experiment2})\"\n",
    "\n",
    "    return merge_metrics(metrics1, metrics2, suffix1, suffix2, by_name)\n",
    "\n",
    "\n",
    "def merge_metrics(\n",
    "    metrics1: pd.DataFrame,\n",
    "    metrics2: pd.DataFrame,\n",
    "    suffix1: str,\n",
    "    suffix2: str,\n",
    "    by_name: bool,\n",
    ") -> pd.DataFrame:\n",
    "    merge_on = \"Model Name\" if by_name else \"Model Description\"\n",
    "    return (\n",
    "        pd.merge(metrics1, metrics2, on=merge_on, suffixes=(suffix1, suffix2))\n",
    "        .set_index(merge_on)\n",
    "        .sort_index(axis=1)\n",
    "        .reset_index()\n",
    "        .sort_values(by=merge_on)\n",
    "    )\n",
    "\n",
    "\n",
    "def load_all_metrics(\n",
    "    set_type: str,\n",
    "    experiments: list[str] = EXPERIMENTS,\n",
    "    columns_to_remove: list = USELESS_METRICS,\n",
    "    infer_training_time_attempt: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    # TODO update method including with more set types?\n",
    "    # the problem is that the information about the type is not contained\n",
    "    # anywhere but in the name of the file\n",
    "    all_metrics = []\n",
    "    for experiment in experiments:\n",
    "        metrics = load_metrics(\n",
    "            set_type,\n",
    "            experiment,\n",
    "            only_model_name=False,\n",
    "            columns_to_remove=columns_to_remove,\n",
    "        )\n",
    "        all_metrics.append(metrics)\n",
    "\n",
    "    all_metrics = pd.concat(all_metrics).reset_index(drop=True)\n",
    "\n",
    "    if infer_training_time_attempt:  # TODO feature in progress\n",
    "        all_metrics[\"Training Time (s)\"] = all_metrics[\"Model Description\"].apply(\n",
    "            compute_training_time\n",
    "        )\n",
    "    return all_metrics.sort_values(by=\"Model Description\")\n",
    "\n",
    "\n",
    "def load_metrics_by_model(\n",
    "    model_name: str,\n",
    "    set_type: str,\n",
    "    experiments: list[str] = EXPERIMENTS,\n",
    "    columns_to_remove: list = USELESS_METRICS,\n",
    "    infer_training_time_attempt: bool = False,\n",
    "):\n",
    "    model_name = model_name.lower().replace(\" \", \"_\")\n",
    "\n",
    "    all_metrics = load_all_metrics(\n",
    "        set_type,\n",
    "        experiments=experiments,\n",
    "        columns_to_remove=columns_to_remove,\n",
    "        infer_training_time_attempt=infer_training_time_attempt,\n",
    "    )\n",
    "    all_metrics = all_metrics[\n",
    "        all_metrics[\"Model Description\"].str.lower().str.contains(model_name)\n",
    "    ]\n",
    "    return all_metrics.sort_values(by=\"Model Description\")\n",
    "\n",
    "\n",
    "def best_model_per_algo(by_metric: str, set_type: str):\n",
    "    __check_arguments(set_type=set_type, metric=by_metric)\n",
    "    all_metrics = load_all_metrics(set_type)\n",
    "    return best_model_per_algo_in_df(all_metrics, by_metric=by_metric)\n",
    "\n",
    "\n",
    "def best_model_per_algo_in_df(\n",
    "    df: pd.DataFrame, by_metric: str, metric_name: str = None\n",
    "):\n",
    "    __check_arguments(metric=by_metric)\n",
    "    if metric_name is None:\n",
    "        metric_name = by_metric\n",
    "    # add a colum to group the models on\n",
    "    df[\"Model Name\"] = (\n",
    "        df[\"Model Description\"]\n",
    "        .str.split(\"_\")\n",
    "        .str[:2]\n",
    "        .str.join(\" \")\n",
    "        .apply(__first_capital)\n",
    "    )\n",
    "    # group\n",
    "    if by_metric in COLUMNS_TO_BE_MAXIMIZED:\n",
    "        group = df.loc[df.groupby(\"Model Name\")[metric_name].idxmax()]\n",
    "    elif by_metric in COLUMNS_TO_BE_MINIMIZED:\n",
    "        group = df.loc[df.groupby(\"Model Name\")[metric_name].idxmin()]\n",
    "    else:\n",
    "        raise ValueError(\"metric not found\")\n",
    "\n",
    "    return group.set_index(\"Model Name\").reset_index().sort_values(by=\"Model Name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_parameter(\n",
    "    metrics_df: pd.DataFrame,\n",
    "    parameter_names: list[str],\n",
    "    parameter_type: type,\n",
    "    regex: str,\n",
    ") -> pd.DataFrame:\n",
    "    # example of regex: r\"approxGP_(\\d+)fourier\"\n",
    "    metrics_df[parameter_names] = (\n",
    "        metrics_df[\"Model Description\"].str.extract(regex).astype(parameter_type)\n",
    "    )\n",
    "    metrics_df.sort_values(by=parameter_names, inplace=True)\n",
    "    return metrics_df\n",
    "\n",
    "\n",
    "def extract_reduction_factors(metrics_df: pd.DataFrame, also_coords: bool = False):\n",
    "    regex = r\"training_factors=\"\n",
    "    variables = [\"ti\", \"ct\"]\n",
    "    if also_coords:\n",
    "        variables += [\"xD\", \"yD\"]\n",
    "    for var in variables:\n",
    "        parameter_name = f\"reduction factor {var}\"\n",
    "        regex = regex.replace(\"(\", \"\").replace(\")\", \"\") + var + r\"(\\d+)\"\n",
    "        metrics_df = extract_parameter(\n",
    "            metrics_df,\n",
    "            parameter_names=[parameter_name],\n",
    "            parameter_type=int,\n",
    "            regex=regex,\n",
    "        )\n",
    "        regex += \"-\"\n",
    "    return metrics_df\n",
    "\n",
    "\n",
    "def extract_reduction_ranges(metrics_df: pd.DataFrame, also_coords: bool = False):\n",
    "    # training_ranges=ti0.15-0.4-ct0.3-0.7\n",
    "    # r\"ti([\\d.]+)-([\\d.]+)-ct([\\d.]+)-([\\d.]+)\"\n",
    "    regex = r\"training_ranges=\"\n",
    "    variables = [\"ti\", \"ct\"]\n",
    "    if also_coords:\n",
    "        variables += [\"xD\", \"yD\"]\n",
    "    for var in variables:\n",
    "        parameter_names = [f\"a_{var}\", f\"b_{var}\"]\n",
    "        regex = regex.replace(\"(\", \"\").replace(\")\", \"\") + var + r\"([\\d.]+)-([\\d.]+)\"\n",
    "        metrics_df = extract_parameter(\n",
    "            metrics_df,\n",
    "            parameter_names=parameter_names,\n",
    "            parameter_type=float,\n",
    "            regex=regex,\n",
    "        )\n",
    "        regex += \"-\"\n",
    "    return metrics_df\n",
    "\n",
    "\n",
    "# TODO move this method in \"plotting.py\"\n",
    "def plot_metrics_per_parameter(\n",
    "    metrics_df: pd.DataFrame,\n",
    "    parameter_name: str,\n",
    "    metrics_to_plot: Optional[list[str]] = None,\n",
    "):  # TODO change\n",
    "    if metrics_to_plot:\n",
    "        for metric in metrics_to_plot:\n",
    "            if metric not in USEFUL_METRICS:\n",
    "                raise ValueError(\"Metric to plot not valid\")\n",
    "    else:\n",
    "        metrics_to_plot = USEFUL_METRICS\n",
    "    ax = plt.gca()\n",
    "    colors = plt.cm.tab10.colors\n",
    "\n",
    "    for i, column in enumerate(metrics_to_plot):\n",
    "        ax.plot(\n",
    "            metrics_df[parameter_name],\n",
    "            metrics_df[column],\n",
    "            label=column,\n",
    "            color=colors[i % len(colors)],\n",
    "        )\n",
    "\n",
    "    model_name = \"\".join(metrics_df[\"Model Description\"].iloc[1].split(\"_\")[:2])\n",
    "\n",
    "    plt.xlabel(parameter_name)\n",
    "    plt.ylabel(\"Metric value\")\n",
    "    plt.title(f\"Results on {model_name} as {parameter_name} increases\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To Latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def print_latex_table_code__(df: pd.DataFrame) -> None:\n",
    "    latex_table = df.style.hide(axis=\"index\").to_latex(escape=False)\n",
    "    #.set_table_styles(\n",
    "    #    [{'selector': 'thead', 'props': [('background-color', '#606060'), ('color', 'white')]}]\n",
    "    #)\n",
    "    \n",
    "    print(latex_table)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def print_copy_latex_table_code(\n",
    "    metrics: pd.DataFrame,\n",
    "    digits_after_dec: int = 4,\n",
    "    bolden_best_value: bool = False,\n",
    "    column_format: Optional[str] = None,\n",
    ") -> None:\n",
    "    for column in metrics.columns:\n",
    "        if bolden_best_value:\n",
    "            if (\n",
    "                column in COLUMNS_TO_BE_MAXIMIZED\n",
    "                or column.split(\" \")[0] in COLUMNS_TO_BE_MAXIMIZED\n",
    "            ):\n",
    "                metrics[column] = __format_highest_value_bold(\n",
    "                    metrics[column], digits_after_dec=digits_after_dec\n",
    "                )\n",
    "                continue\n",
    "            elif (\n",
    "                column in COLUMNS_TO_BE_MINIMIZED\n",
    "                or column.split(\" \")[0] in COLUMNS_TO_BE_MINIMIZED\n",
    "            ):\n",
    "                metrics[column] = __format_lowest_value_bold(\n",
    "                    metrics[column], digits_after_dec=digits_after_dec\n",
    "                )\n",
    "                continue\n",
    "        if metrics[column].dtype.kind == \"f\":\n",
    "            metrics[column] = [\n",
    "                __format_float(val, digits_after_dec=digits_after_dec)\n",
    "                for val in metrics[column]\n",
    "            ]\n",
    "\n",
    "    formatted_columns = [rf\"\\textbf{{{col}}}\" for col in metrics.columns]\n",
    "    latex_table = metrics.to_latex(\n",
    "        index=False, escape=False, column_format=column_format, header=formatted_columns\n",
    "    )  # , float_format=f\"%.{digits_after_dec}f\")\n",
    "\n",
    "    # copy to clipboard\n",
    "    pyperclip.copy(latex_table)\n",
    "\n",
    "    print(latex_table)\n",
    "\n",
    "\n",
    "def __format_highest_value_bold(values, digits_after_dec: int):\n",
    "    max_value = values.max()\n",
    "    return [\n",
    "        __format_float(val, bold_value=max_value, digits_after_dec=digits_after_dec)\n",
    "        for val in values\n",
    "    ]\n",
    "\n",
    "\n",
    "def __format_lowest_value_bold(values, digits_after_dec: int):\n",
    "    min_value = values.min()\n",
    "    return [\n",
    "        __format_float(val, bold_value=min_value, digits_after_dec=digits_after_dec)\n",
    "        for val in values\n",
    "    ]\n",
    "\n",
    "\n",
    "def __format_float(\n",
    "    value: float, bold_value: Optional[float] = None, digits_after_dec: int = 4\n",
    ") -> str:\n",
    "    if not isinstance(value, float):\n",
    "\n",
    "        return value\n",
    "    if value == 0.0:\n",
    "        return \"0\"\n",
    "    if pd.isna(value):\n",
    "        return \"N/A\"\n",
    "\n",
    "    exponent = abs(int(math.log10(abs(value))))\n",
    "\n",
    "    if exponent >= digits_after_dec:\n",
    "        digits_after_dec = max(digits_after_dec - 3, 0)\n",
    "        formatted_str = \"{:.{digits}e}\".format(value, digits=digits_after_dec)\n",
    "        # Remove unnecessary 0 in the exponent\n",
    "        formatted_str = formatted_str.replace(\"e+0\", \"e+\").replace(\"e-0\", \"e-\")\n",
    "    else:\n",
    "        formatted_str = \"{:.{digits}f}\".format(value, digits=digits_after_dec)\n",
    "\n",
    "    if bold_value and value == bold_value:\n",
    "        formatted_str = rf\"\\textbf{{{formatted_str}}}\"\n",
    "\n",
    "    return formatted_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics of different models on the same experiment (including different reduction factors)\n",
    "metrics = load_metrics(\n",
    "    set_type=\"test\",\n",
    "    experiment=\"extrapolation\",\n",
    "    only_model_name=False,\n",
    "    infer_training_time_attempt=False,\n",
    ")\n",
    "# print_copy_latex_table_code(metrics, digits_after_dec=4, bolden_best_value=True)\n",
    "metrics[\"consider_ws\"] = metrics[\"Model Description\"].str.contains(\"consider_ws\")\n",
    "metrics.sort_values(by=[\"consider_ws\", \"r2\"], inplace=True)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same model with different parameters in different experiments\n",
    "df = load_metrics_by_model(\n",
    "    model_name=\"univariate DT\",\n",
    "    set_type=\"test\",\n",
    "    experiments=EXPERIMENTS,\n",
    "    columns_to_remove=USELESS_METRICS,\n",
    "    infer_training_time_attempt=False,\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison between two experiments merging the models\n",
    "# (in case of many attempts with the same model for eadh experiment, all the combinations are considered)\n",
    "comparison = compare_metrics(\n",
    "    \"test\", \"interpolation\", \"test\", \"extrapolation\", by_name=True\n",
    ")\n",
    "comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each algorithm, the best model\n",
    "best_model_per_algo(\"r2\", \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_all_metrics(\n",
    "    set_type=\"test\",\n",
    "    experiments=EXPERIMENTS,\n",
    "    columns_to_remove=USELESS_METRICS,\n",
    "    infer_training_time_attempt=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INTERPOLATION - RESULTS\n",
    "two = list()\n",
    "for set_type in [\"train\", \"test\"]:\n",
    "    metrics = load_metrics(\n",
    "        set_type=set_type,\n",
    "        experiment=\"interpolation\",\n",
    "        only_model_name=False,\n",
    "        infer_training_time_attempt=False,\n",
    "    )\n",
    "    # metrics = compare_metrics(\"train\", \"interpolation\", \"test\", \"interpolation\", by_name=False)\n",
    "    metrics = extract_reduction_factors(metrics, also_coords=False)\n",
    "    metrics = metrics[metrics[\"reduction factor ti\"] == 12]\n",
    "    toremove_columns = [\n",
    "        col\n",
    "        for col in metrics.columns\n",
    "        if col.startswith(\"reduction factor\")\n",
    "        or col.startswith(\"Prediction Time\")\n",
    "        or col.startswith(\"MSE\")\n",
    "        or col.startswith(\"PSNR\")\n",
    "    ]\n",
    "    metrics.drop(toremove_columns, axis=1, inplace=True)\n",
    "\n",
    "    two.append(metrics)\n",
    "\n",
    "metrics = merge_metrics(\n",
    "    two[0], two[1], suffix1=\" (train)\", suffix2=\" (test)\", by_name=False\n",
    ")\n",
    "# metrics = best_model_per_algo_in_df(metrics, \"MAE\", \"MAE (test)\")\n",
    "# metrics = metrics[['Model Name', 'r2 (train)', 'MAE (train)', 'r2 (test)', 'MAE (test)', 'Model Description']]\n",
    "# print_copy_latex_table_code(metrics, bolden_best_value=True, column_format=\"lccccl\")\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolation - study of DT depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INTERPOLATION - DT DEPTH - RESULTS\n",
    "two = list()\n",
    "for set_type in [\"train\", \"test\"]:\n",
    "    metrics = load_metrics(\n",
    "        set_type=set_type,\n",
    "        experiment=\"DTdepth\",\n",
    "        only_model_name=False,\n",
    "        infer_training_time_attempt=False,\n",
    "    )\n",
    "    toremove_columns = [\n",
    "        col\n",
    "        for col in metrics.columns\n",
    "        if col.startswith(\"reduction factor\")\n",
    "        or col.startswith(\"Prediction Time\")\n",
    "        or col.startswith(\"MSE\")\n",
    "        or col.startswith(\"PSNR\")\n",
    "    ]\n",
    "    metrics.drop(toremove_columns, axis=1, inplace=True)\n",
    "\n",
    "    two.append(metrics)\n",
    "\n",
    "metrics = merge_metrics(\n",
    "    two[0], two[1], suffix1=\" (train)\", suffix2=\" (test)\", by_name=False\n",
    ")\n",
    "metrics.sort_values(by=\"Model Description\")\n",
    "# metrics = best_model_per_algo_in_df(metrics, \"MAE\", \"MAE (test)\")\n",
    "metrics = metrics[\n",
    "    [\"Model Description\", \"r2 (train)\", \"MAE (train)\", \"r2 (test)\", \"MAE (test)\"]\n",
    "]\n",
    "print_copy_latex_table_code(metrics, bolden_best_value=False, column_format=\"lcccc\")\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolation on coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INTERPOLATION ON COORDS - RESULTS\n",
    "two = []\n",
    "for set_type in [\"train\", \"test\"]:\n",
    "    metrics = load_metrics(\n",
    "        set_type=set_type,\n",
    "        experiment=\"interpolation_coords\",\n",
    "        only_model_name=False,\n",
    "        infer_training_time_attempt=False,\n",
    "    )\n",
    "    metrics = extract_reduction_factors(metrics, also_coords=False)\n",
    "    metrics = metrics[metrics[\"reduction factor ti\"] == 16]\n",
    "    toremove_columns = [\n",
    "        col\n",
    "        for col in metrics.columns\n",
    "        if col.startswith(\"Prediction Time\")\n",
    "        or col.startswith(\"MSE\")\n",
    "        or col.startswith(\"PSNR\")\n",
    "    ]\n",
    "    metrics.drop(toremove_columns, axis=1, inplace=True)\n",
    "\n",
    "    two.append(metrics)\n",
    "\n",
    "metrics = merge_metrics(\n",
    "    two[0], two[1], suffix1=\" (train)\", suffix2=\" (test)\", by_name=False\n",
    ")\n",
    "metrics = best_model_per_algo_in_df(metrics, \"MAE\", \"MAE (test)\")\n",
    "metrics = metrics[\n",
    "    [\n",
    "        \"Model Name\",\n",
    "        \"r2 (train)\",\n",
    "        \"MAE (train)\",\n",
    "        \"r2 (test)\",\n",
    "        \"MAE (test)\",\n",
    "        \"Model Description\",\n",
    "    ]\n",
    "]\n",
    "print_copy_latex_table_code(metrics, bolden_best_value=True, column_format=\"lccccl\")\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extrapolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRAPOLATIONS - RESULTS\n",
    "two = list()\n",
    "for set_type in [\"train\", \"test\"]:\n",
    "    metrics = load_metrics(\n",
    "        set_type=set_type,\n",
    "        experiment=\"extrapolation\",\n",
    "        only_model_name=False,\n",
    "        infer_training_time_attempt=False,\n",
    "    )\n",
    "    metrics = extract_reduction_ranges(metrics, also_coords=False)\n",
    "    # metrics = metrics[metrics[\"reduction factor ti\"]==4]\n",
    "    toremove_columns = [\n",
    "        col\n",
    "        for col in metrics.columns\n",
    "        if col.startswith(\"a_\")\n",
    "        or col.startswith(\"b_\")\n",
    "        or col.startswith(\"Prediction Time\")\n",
    "        or col.startswith(\"MSE\")\n",
    "        or col.startswith(\"PSNR\")\n",
    "    ]\n",
    "    metrics.drop(toremove_columns, axis=1, inplace=True)\n",
    "\n",
    "    two.append(metrics)\n",
    "\n",
    "metrics = merge_metrics(\n",
    "    two[0], two[1], suffix1=\" (train)\", suffix2=\" (test)\", by_name=False\n",
    ")\n",
    "# metrics = best_model_per_algo_in_df(metrics, \"MAE\", \"MAE (test)\")\n",
    "# metrics = metrics[['Model Name', 'r2 (train)', 'MAE (train)', 'r2 (test)', 'MAE (test)', 'Model Description']]\n",
    "# print_copy_latex_table_code(metrics, bolden_best_value=True, column_format=\"lccccl\")\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = load_all_metrics(\n",
    "    set_type=\"test\",\n",
    "    experiments=EXPERIMENTS,\n",
    "    columns_to_remove=USELESS_METRICS,\n",
    "    infer_training_time_attempt=True,\n",
    ")\n",
    "metrics = extract_model_name(metrics, False)\n",
    "metrics = metrics[metrics[\"r2\"] > 0]\n",
    "metrics.drop([\"r2\", \"MSE\", \"MAE\", \"PSNR\"], axis=1, inplace=True)\n",
    "metrics = metrics[\n",
    "    metrics[\"Model Description\"].str.contains(\"NeRF_256fourier-features\")\n",
    "    | ~metrics[\"Model Description\"].str.contains(\"NeRF\")\n",
    "]\n",
    "metrics.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# fixing missing values for non-logged models (through experience and console logs)\n",
    "training_time_label = \"Training Time new\"\n",
    "nerf_range = metrics[metrics[\"Model Name\"] == \"Univariate NeRF\"][\n",
    "    \"Training Time (s)\"\n",
    "].agg(lambda x: f\"{int(x.min())}-{int(x.max())}\")\n",
    "uninn_range = metrics[metrics[\"Model Name\"] == \"Univariate NN\"][\n",
    "    \"Training Time (s)\"\n",
    "].agg(lambda x: f\"{int(x.min())}-{int(x.max())}\")\n",
    "\n",
    "for i in range(len(metrics)):\n",
    "    if metrics.loc[i, \"Model Name\"] == \"Univariate DT\":\n",
    "        metrics.loc[i, training_time_label] = \"<20\"\n",
    "    elif metrics.loc[i, \"Model Name\"] == \"Multivariate NN\":\n",
    "        metrics.loc[i, training_time_label] = (\n",
    "            \"around 5/6 mins\"  # TODO convert in seconds (?)\n",
    "        )\n",
    "    elif (\n",
    "        metrics.loc[i, \"Model Name\"] == \"Multivariate LR\"\n",
    "        or metrics.loc[i, \"Model Name\"] == \"Univariate LR\"\n",
    "    ):\n",
    "        metrics.loc[i, training_time_label] = \"<1\"\n",
    "    elif metrics.loc[i, \"Model Name\"] == \"Univariate NeRF\":\n",
    "        metrics.loc[i, training_time_label] = nerf_range\n",
    "    elif metrics.loc[i, \"Model Name\"] == \"Univariate NN\":\n",
    "        metrics.loc[i, training_time_label] = uninn_range\n",
    "\n",
    "metrics = extract_reduction_factors(metrics, also_coords=True)\n",
    "metrics.drop([\"reduction factor ct\", \"reduction factor yD\"], axis=1, inplace=True)\n",
    "metrics.sort_values(\n",
    "    by=[\"Model Name\", \"reduction factor ti\", \"reduction factor xD\"], inplace=True\n",
    ")\n",
    "\n",
    "metrics[\"Prediction Time (s)\"] = metrics[\"Prediction Time\"].apply(\n",
    "    lambda x: \"$10^{\" + f\"{math.floor(math.log10(x))}\" + \"}$\"\n",
    ")\n",
    "metrics.rename(\n",
    "    columns={\n",
    "        \"Prediction Time\": \"Prediction Time (precise)\",\n",
    "        \"Training Time (s)\": \"Training Time (precise)\",\n",
    "        \"Training Time new\": \"Training Time (s)\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "# only classic interpolation\n",
    "metrics = metrics[\n",
    "    metrics[\"reduction factor ti\"].isin([4.0])\n",
    "    & metrics[\"reduction factor xD\"].isin([np.nan])\n",
    "]\n",
    "\n",
    "metrics = metrics[\n",
    "    [\n",
    "        \"Model Name\",\n",
    "        \"Training Time (s)\",\n",
    "        \"Prediction Time (s)\",\n",
    "        \"Model Description\",\n",
    "        \"Training Time (precise)\",\n",
    "        \"Prediction Time (precise)\",\n",
    "    ]\n",
    "]\n",
    "print_copy_latex_table_code(metrics, digits_after_dec=4, bolden_best_value=False)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other studies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of components for approximate GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Study on the number of components approximate GP (needs to consider the records with \"consider_ws\")\"\"\"\n",
    "\n",
    "metrics = load_metrics_by_model(\"approxGP\", set_type=\"test\", experiments=[\"OLD\"])\n",
    "metrics = extract_parameter(\n",
    "    metrics,\n",
    "    parameter_names=[\"components_number\"],\n",
    "    parameter_type=int,\n",
    "    regex=r\"approxGP_(\\d+)fourier\",\n",
    ")\n",
    "plot_metrics_per_parameter(metrics, \"components_number\")\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study of reduction factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree - interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"DECISION TREE REDUCTION FACTORS - Study on the training reduction factors for Decision Tree\"\"\"\n",
    "\n",
    "metrics_DT_rf = load_metrics_by_model(\n",
    "    \"DT\", set_type=\"test\", experiments=[\"interpolation\"]\n",
    ")\n",
    "metrics_DT_rf = extract_reduction_factors(metrics_DT_rf, also_coords=False)\n",
    "# leaving only deepest trees\n",
    "metrics_DT_rf = extract_parameter(metrics_DT_rf, [\"depth\"], float, regex=r\"depth(\\d+)_\")\n",
    "metrics_DT_rf = metrics_DT_rf[metrics_DT_rf[\"depth\"].isna()]\n",
    "metrics_DT_rf.drop(\"depth\", axis=1, inplace=True)\n",
    "# cleaning the table\n",
    "metrics_DT_rf[\"Model Description\"] = \"Univariate DT\"\n",
    "metrics_DT_rf.drop(\n",
    "    [\"Prediction Time\", \"MSE\", \"PSNR\", \"reduction factor ct\"], axis=1, inplace=True\n",
    ")\n",
    "metrics_DT_rf = metrics_DT_rf[metrics_DT_rf[\"reduction factor ti\"] < 64]\n",
    "metrics_DT_rf[\"Training Perc\"] = metrics_DT_rf[\"reduction factor ti\"].apply(\n",
    "    lambda x: math.ceil(100 / x) * math.ceil(86 / x) / 86\n",
    ")\n",
    "metrics_DT_rf[\"Simulations\"] = metrics_DT_rf[\"Training Perc\"].apply(\n",
    "    lambda x: int(x * 86)\n",
    ")\n",
    "\n",
    "plot_metrics_per_parameter(\n",
    "    metrics_DT_rf, \"reduction factor ti\", metrics_to_plot=[\"MAE\"]\n",
    ")\n",
    "metrics_DT_rf[\"Training Percentage\"] = metrics_DT_rf[\"Training Perc\"].apply(\n",
    "    lambda x: f\"& $ \\\\approx {x:.2f}\\\\%$ \\\\\\\\\"\n",
    ")\n",
    "# print_copy_latex_table_code(metrics_DT_rf, column_format=\"cccc\")\n",
    "metrics_DT_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NeRF - interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"NERF REDUCTION FACTORS - Study on the training reduction factors for NeRF\"\"\"\n",
    "\n",
    "metrics_NeRF_rf = load_metrics_by_model(\n",
    "    \"NeRF\", set_type=\"test\", experiments=[\"interpolation\"]\n",
    ")\n",
    "metrics_NeRF_rf = extract_reduction_factors(metrics_NeRF_rf, also_coords=False)\n",
    "# leaving only best structure\n",
    "metrics_NeRF_rf = extract_parameter(\n",
    "    metrics_NeRF_rf, [\"# ff\"], int, regex=r\"_(\\d+)fourier-features\"\n",
    ")\n",
    "metrics_NeRF_rf = metrics_NeRF_rf[metrics_NeRF_rf[\"# ff\"] == 256]\n",
    "metrics_NeRF_rf.drop(\"# ff\", axis=1, inplace=True)\n",
    "# cleaning the table\n",
    "metrics_NeRF_rf[\"Model Description\"] = \"NeRF\"\n",
    "metrics_NeRF_rf.drop(\n",
    "    [\"Prediction Time\", \"MSE\", \"PSNR\", \"reduction factor ct\"], axis=1, inplace=True\n",
    ")\n",
    "metrics_NeRF_rf[\"Training Perc\"] = metrics_NeRF_rf[\"reduction factor ti\"].apply(\n",
    "    lambda x: math.ceil(100 / x) * math.ceil(86 / x) / 86\n",
    ")\n",
    "metrics_NeRF_rf[\"Simulations\"] = metrics_NeRF_rf[\"Training Perc\"].apply(\n",
    "    lambda x: int(x * 86)\n",
    ")\n",
    "\n",
    "plot_metrics_per_parameter(\n",
    "    metrics_NeRF_rf, \"reduction factor ti\", metrics_to_plot=[\"MAE\"]\n",
    ")\n",
    "metrics_NeRF_rf[\"Training Percentage\"] = metrics_NeRF_rf[\"Training Perc\"].apply(\n",
    "    lambda x: f\"& $ \\\\approx {x:.2f}\\\\%$ \\\\\\\\\"\n",
    ")\n",
    "# print_copy_latex_table_code(metrics_NeRF_rf, column_format=\"cccc\")\n",
    "metrics_NeRF_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = metrics_DT_rf.drop(\n",
    "    [\n",
    "        \"reduction factor ti\",\n",
    "        \"Training Perc\",\n",
    "        \"Training Percentage\",\n",
    "        \"MAE\",\n",
    "        \"Model Description\",\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "b = metrics_NeRF_rf.drop(\n",
    "    [\n",
    "        \"reduction factor ti\",\n",
    "        \"Training Perc\",\n",
    "        \"Training Percentage\",\n",
    "        \"MAE\",\n",
    "        \"Model Description\",\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "pd.merge(a, b, on=\"Simulations\", suffixes=[\"_DT\", \"_NeRF\"]).reset_index(\n",
    "    drop=True\n",
    ").to_clipboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual Comparison - interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUAL COMPARISON\n",
    "metric_name = \"MAE\"\n",
    "ax = plt.gca()\n",
    "ax.plot(metrics_NeRF_rf[\"Simulations\"], metrics_NeRF_rf[metric_name], label=\"NeRF\")\n",
    "ax.plot(metrics_DT_rf[\"Simulations\"], metrics_DT_rf[metric_name], label=\"DT\")\n",
    "plt.xlabel(\"# of simulations in Training Set\")  # or \"Reduction factor\"\n",
    "plt.ylabel(f\"{metric_name} (test set)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "metric_name = \"r2\"\n",
    "ax = plt.gca()\n",
    "ax.plot(metrics_NeRF_rf[\"Simulations\"], metrics_NeRF_rf[metric_name], label=\"NeRF\")\n",
    "ax.plot(metrics_DT_rf[\"Simulations\"], metrics_DT_rf[metric_name], label=\"DT\")\n",
    "plt.xlabel(\"# of simulations in Training Set\")  # or \"Reduction factor\"\n",
    "plt.ylabel(f\"R2 (test set)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\"font.size\": 18})\n",
    "fig, ax1 = plt.subplots()\n",
    "fig.set_size_inches(8, 5)\n",
    "\n",
    "ax1.set_xlabel(\"# of simulations in Training Set\")  # , fontsize=18)\n",
    "ax1.set_ylabel(\"MAE (test set)\")  # , fontsize=18)\n",
    "ax1.plot(metrics_NeRF_rf[\"Simulations\"], metrics_NeRF_rf[\"MAE\"], label=\"FEMLP MAE\")\n",
    "ax1.plot(metrics_DT_rf[\"Simulations\"], metrics_DT_rf[\"MAE\"], label=\"RDT MAE\")\n",
    "ax1.tick_params(axis=\"y\")\n",
    "\n",
    "ax2 = ax1.twinx()  # second axes that shares the same x-axis\n",
    "\n",
    "color = \"tab:blue\"\n",
    "ax2.set_ylabel(\"R2 (test set)\", color=color)  # we already handled the x-label with ax1\n",
    "ax2.plot(\n",
    "    metrics_NeRF_rf[\"Simulations\"],\n",
    "    metrics_NeRF_rf[\"r2\"],\n",
    "    label=\"FEMLP R2\",\n",
    "    linestyle=\"dashed\",\n",
    ")\n",
    "ax2.plot(\n",
    "    metrics_DT_rf[\"Simulations\"],\n",
    "    metrics_DT_rf[metric_name],\n",
    "    label=\"RDT R2\",\n",
    "    linestyle=\"dashed\",\n",
    ")\n",
    "ax2.tick_params(axis=\"y\", labelcolor=color)\n",
    "\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "fig.legend(\n",
    "    ax1.get_legend_handles_labels()[1] + ax2.get_legend_handles_labels()[1],\n",
    "    loc=\"center right\",\n",
    "    bbox_to_anchor=(0.85, 0.6),\n",
    ")\n",
    "plt.xscale(\"log\", base=2)\n",
    "plt.savefig(\"results_size-dependence.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree - interpolation on coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"DECISION TREE REDUCTION FACTORS ON INT.COORDS - Study on the training reduction factors for Decision Tree\"\"\"\n",
    "\n",
    "metrics_DT_rf = load_metrics_by_model(\n",
    "    \"DT\", set_type=\"test\", experiments=[\"interpolation_coords\"]\n",
    ")\n",
    "metrics_DT_rf = extract_reduction_factors(metrics_DT_rf, also_coords=False)\n",
    "# leaving only deepest trees\n",
    "metrics_DT_rf = extract_parameter(metrics_DT_rf, [\"depth\"], float, regex=r\"depth(\\d+)_\")\n",
    "metrics_DT_rf = metrics_DT_rf[metrics_DT_rf[\"depth\"].isna()]\n",
    "metrics_DT_rf.drop(\"depth\", axis=1, inplace=True)\n",
    "\n",
    "# cleaning the table\n",
    "metrics_DT_rf[\"Model Description\"] = \"Univariate DT\"\n",
    "metrics_DT_rf.drop(\n",
    "    [\"Prediction Time\", \"MSE\", \"PSNR\", \"reduction factor ct\"], axis=1, inplace=True\n",
    ")\n",
    "metrics_DT_rf = metrics_DT_rf[metrics_DT_rf[\"reduction factor ti\"] < 16]\n",
    "\n",
    "plot_metrics_per_parameter(\n",
    "    metrics_DT_rf, \"reduction factor ti\", metrics_to_plot=[\"MAE\"]\n",
    ")\n",
    "metrics_DT_rf[\"Training Percentage\"] = metrics_DT_rf[\"reduction factor ti\"].apply(\n",
    "    lambda x: f\"& $ \\\\approx {math.ceil(100/x) * math.ceil(86/x) * math.ceil(224/x) * math.ceil(32/x)/(86*224*32):.2f}\\\\%$ \\\\\\\\\"\n",
    ")\n",
    "# print_copy_latex_table_code(metrics_DT_rf, column_format=\"cccc\")\n",
    "\n",
    "metrics_DT_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NeRF - interpolation on coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"NERF REDUCTION FACTORS - Study on the training reduction factors for NeRF\"\"\"\n",
    "\n",
    "metrics_NeRF_rf = load_metrics_by_model(\n",
    "    \"NeRF\", set_type=\"test\", experiments=[\"interpolation_coords\"]\n",
    ")\n",
    "metrics_NeRF_rf = extract_reduction_factors(metrics_NeRF_rf, also_coords=True)\n",
    "\n",
    "# leaving only best structure\n",
    "metrics_NeRF_rf = extract_parameter(\n",
    "    metrics_NeRF_rf, [\"# ff\"], int, regex=r\"_(\\d+)fourier-features\"\n",
    ")\n",
    "metrics_NeRF_rf = metrics_NeRF_rf[metrics_NeRF_rf[\"# ff\"] == 256]\n",
    "metrics_NeRF_rf.drop(\"# ff\", axis=1, inplace=True)\n",
    "\n",
    "# cleaning the table\n",
    "metrics_NeRF_rf[\"Model Description\"] = \"NeRF\"\n",
    "metrics_NeRF_rf.drop(\n",
    "    [\"Prediction Time\", \"MSE\", \"PSNR\", \"reduction factor ct\"], axis=1, inplace=True\n",
    ")\n",
    "metrics_NeRF_rf = metrics_NeRF_rf[metrics_NeRF_rf[\"reduction factor ti\"] < 16]\n",
    "\n",
    "plot_metrics_per_parameter(\n",
    "    metrics_NeRF_rf, \"reduction factor ti\", metrics_to_plot=[\"MAE\"]\n",
    ")\n",
    "metrics_NeRF_rf[\"Training Percentage\"] = metrics_NeRF_rf[\"reduction factor ti\"].apply(\n",
    "    lambda x: f\"& $ \\\\approx {math.ceil(100/x) * math.ceil(86/x) * math.ceil(224/x) * math.ceil(32/x)/(86*224*32):.2f}\\\\%$ \\\\\\\\\"\n",
    ")\n",
    "# print_copy_latex_table_code(metrics_NeRF_rf, column_format=\"cccc\")\n",
    "\n",
    "metrics_NeRF_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual Comparison - interpolation on coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percs = [0.4, 0.03, 0.01][::-1]\n",
    "print(percs)\n",
    "metric_name = \"MAE\"\n",
    "ax = plt.gca()\n",
    "ax.plot(\n",
    "    metrics_NeRF_rf[\"reduction factor ti\"], metrics_NeRF_rf[metric_name], label=\"NeRF\"\n",
    ")\n",
    "ax.plot(metrics_DT_rf[\"reduction factor ti\"], metrics_DT_rf[metric_name], label=\"DT\")\n",
    "plt.xlabel(\"Reduction factor\")\n",
    "plt.ylabel(f\"{metric_name} (test set)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "metric_name = \"r2\"\n",
    "ax = plt.gca()\n",
    "ax.plot(\n",
    "    metrics_NeRF_rf[\"reduction factor ti\"], metrics_NeRF_rf[metric_name], label=\"NeRF\"\n",
    ")\n",
    "ax.plot(metrics_DT_rf[\"reduction factor ti\"], metrics_DT_rf[metric_name], label=\"DT\")\n",
    "plt.xlabel(\"Reduction factor\")\n",
    "plt.ylabel(f\"R2 (test set)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualitative results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual comparison of centreline predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_loaders import get_wake_datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "import src.plotting as plotting\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "MODEL_FOLDER_OLD = \"saved_models/discr_factors_x2_30_y-2_2_step0.125_reducedTI-CT/\"\n",
    "MODEL_FOLDER = \"saved_models/discr_factors_x2_30_y-2_2_step0.125_TIstep0.01_CTstep0.01/\"\n",
    "\n",
    "DATA_FOLDER = \"data/discr_factors_x2_30_y-2_2_step0.125_TIstep0.01_CTstep0.01\"\n",
    "\n",
    "INPUT_VAR_TO_TRAIN_REDUCTION_FACTOR = {\n",
    "    \"ti\": 4,\n",
    "    \"ct\": 4,\n",
    "}  # , 'x/D': 4, 'y/D': 4} INTERPOLATION ON COORDS NOT POSSIBLE PLOTTING\n",
    "INPUT_VAR_TO_TRAIN_RANGES = {\"ti\": [(0.15, 0.4)], \"ct\": [(0.3, 0.7)]}\n",
    "\n",
    "MODEL_COLORS = [\n",
    "    \"crimson\",\n",
    "    \"deepskyblue\",\n",
    "    \"green\",\n",
    "    \"mediumorchid\",\n",
    "    \"magenta\",\n",
    "    \"navy\",\n",
    "    \"mediumblue\",\n",
    "]  # TODO add more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" FOR UNIVARIATE \"\"\"\n",
    "\n",
    "# DATA\n",
    "train_dataset_uni, valid_dataset, test_dataset_uni = get_wake_datasets(\n",
    "    DATA_FOLDER,\n",
    "    consider_ws=False,\n",
    "    coords_as_input=True,\n",
    "    input_var_to_train_reduction_factor=INPUT_VAR_TO_TRAIN_REDUCTION_FACTOR,\n",
    "    # input_var_to_train_ranges=INPUT_VAR_TO_TRAIN_RANGES,\n",
    ")\n",
    "del valid_dataset\n",
    "\n",
    "num_cells_uni = test_dataset_uni.num_cells\n",
    "num_fields_uni = len(test_dataset_uni) // num_cells_uni\n",
    "input_space_uni = test_dataset_uni.inputs.shape[1]\n",
    "output_space_uni = test_dataset_uni.outputs.shape[1]\n",
    "field_indices_uni = list(range(num_fields_uni))\n",
    "\n",
    "models_uni = list()\n",
    "model_descs_uni = list()\n",
    "\n",
    "# UNIVARIATE NN\n",
    "hidden_layers_units = [50, 250]\n",
    "activation_function = nn.ReLU()\n",
    "\n",
    "\n",
    "class UnivariateNN(nn.Module):\n",
    "    def __init__(self, input_space, output_space) -> None:\n",
    "        super(UnivariateNN, self).__init__()\n",
    "        layer_units = [input_space] + hidden_layers_units\n",
    "        layers = list()\n",
    "        for first, second in zip(layer_units, layer_units[1:]):\n",
    "            layers += [nn.Linear(first, second), activation_function]\n",
    "        layers.append(\n",
    "            nn.Linear(layer_units[-1], output_space)\n",
    "        )  # last layer not activated\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "model_path = os.path.join(\n",
    "    MODEL_FOLDER, \"univariate_NN_layers50-250_training_factors=ti4-ct4.pt\"\n",
    ")\n",
    "model_desc = \"Univariate MLP\"\n",
    "\n",
    "model = UnivariateNN(input_space_uni, output_space_uni)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "\n",
    "models_uni.append(model)\n",
    "model_descs_uni.append(model_desc)\n",
    "\n",
    "# NeRF\n",
    "n_mapping = 256\n",
    "hidden_layers_units = [256, 256, 256]\n",
    "\n",
    "\n",
    "class FourierLayer(nn.Module):\n",
    "    def __init__(self, n_features: int, n_mapping: int):\n",
    "        super(FourierLayer, self).__init__()\n",
    "        self.coefficients = nn.Parameter(\n",
    "            torch.randn(n_mapping, n_features)\n",
    "        )  # sampled from normal distribution with mean 0 and variance 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_proj = 2 * torch.pi * x @ self.coefficients.t()\n",
    "        out = torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class NeRF2D(nn.Module):\n",
    "    def __init__(self, n_coordinates: int, n_features: int, output_space: int):\n",
    "        super(NeRF2D, self).__init__()\n",
    "        self.fourier_layer = FourierLayer(n_coordinates, n_mapping)\n",
    "        layer_units = [n_mapping * 2 + n_features] + hidden_layers_units\n",
    "        mlp_layers = list()\n",
    "        for first, second in zip(layer_units, layer_units[1:]):\n",
    "            mlp_layers += [nn.Linear(first, second), activation_function]\n",
    "        mlp_layers.append(\n",
    "            nn.Linear(layer_units[-1], output_space)\n",
    "        )  # last layer not activated\n",
    "        self.mlp = nn.Sequential(*mlp_layers)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Pass coordinates through the Fourier Layer\n",
    "        coords = inputs[:, -2:]\n",
    "        input_features = inputs[:, :-2]\n",
    "        fourier_out = self.fourier_layer(coords)\n",
    "\n",
    "        # Concatenate Fourier layer output with other input features\n",
    "        input_features = torch.cat((fourier_out, input_features), dim=-1)\n",
    "\n",
    "        # Feed-forward through the rest of the neural network\n",
    "        x = self.mlp(input_features)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "model_path = os.path.join(\n",
    "    MODEL_FOLDER,\n",
    "    \"univariate_NeRF_256fourier-features_layers256-256-256_training_factors=ti4-ct4.pt\",\n",
    ")\n",
    "model_desc = \"Univariate NeRF\"\n",
    "\n",
    "model = NeRF2D(n_coordinates=2, n_features=2, output_space=1)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "\n",
    "models_uni.append(model)\n",
    "model_descs_uni.append(model_desc)\n",
    "\n",
    "# DECISION TREE\n",
    "model = DecisionTreeRegressor()\n",
    "model.fit(train_dataset_uni.inputs, train_dataset_uni.outputs)\n",
    "model_desc = \"Univariate DT\"\n",
    "models_uni.append(model)\n",
    "model_descs_uni.append(model_desc)\n",
    "\n",
    "# UNIVARIATE LINEAR REGRESSION\n",
    "model = LinearRegression()\n",
    "model.fit(train_dataset_uni.inputs, train_dataset_uni.outputs)\n",
    "model_desc = \"Univariate LR\"\n",
    "models_uni.append(model)\n",
    "model_descs_uni.append(model_desc)\n",
    "\n",
    "# UNIVARIATE APPROXIMATE GAUSSIAN PROCESS\n",
    "rbf_sampler = RBFSampler(n_components=32, gamma=\"scale\", random_state=0)\n",
    "train_x_fourier = rbf_sampler.fit_transform(train_dataset_uni.inputs)\n",
    "model = LinearRegression()\n",
    "model.fit(train_x_fourier, train_dataset_uni.outputs)\n",
    "model_desc = \"Univariate AGP\"\n",
    "models_uni.append(model)\n",
    "model_descs_uni.append(model_desc)\n",
    "\n",
    "print(model_descs_uni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" FOR MULTIVARIATE \"\"\"\n",
    "\n",
    "# DATA\n",
    "train_dataset_multi, valid_dataset, test_dataset_multi = get_wake_datasets(\n",
    "    DATA_FOLDER,\n",
    "    consider_ws=False,\n",
    "    coords_as_input=False,\n",
    "    input_var_to_train_reduction_factor=INPUT_VAR_TO_TRAIN_REDUCTION_FACTOR,\n",
    "    # input_var_to_train_ranges=INPUT_VAR_TO_TRAIN_RANGES\n",
    ")\n",
    "del valid_dataset\n",
    "\n",
    "input_space_multi = train_dataset_multi.inputs.shape[1]\n",
    "output_space_multi = train_dataset_multi.outputs.shape[1]\n",
    "indices_multi = list(range(len(test_dataset_multi)))\n",
    "\n",
    "models_multi = list()\n",
    "model_descs_multi = list()\n",
    "\n",
    "# MULTIVARIATE LINEAR REGRESSION\n",
    "model = LinearRegression()\n",
    "model.fit(train_dataset_multi.inputs, train_dataset_multi.outputs)\n",
    "model_desc = \"Multivariate LR\"\n",
    "models_multi.append(model)\n",
    "model_descs_multi.append(model_desc)\n",
    "\n",
    "# MULTIVARIATE MLP\n",
    "HIDDEN_LAYERS_UNITS = [50, 500]\n",
    "ACTIVATION_FUNCTION = nn.ReLU()\n",
    "\n",
    "\n",
    "class MultivariateNN(nn.Module):\n",
    "    def __init__(self, input_space, output_space) -> None:\n",
    "        super(MultivariateNN, self).__init__()\n",
    "        layer_units = [input_space] + HIDDEN_LAYERS_UNITS\n",
    "        layers = list()\n",
    "        for first, second in zip(layer_units, layer_units[1:]):\n",
    "            layers += [nn.Linear(first, second), ACTIVATION_FUNCTION]\n",
    "        layers.append(\n",
    "            nn.Linear(layer_units[-1], output_space)\n",
    "        )  # last layer not activated\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "model_path = os.path.join(\n",
    "    MODEL_FOLDER, \"multivariate_NN_layers50-500_training_factors=ti4-ct4.pt\"\n",
    ")\n",
    "model_desc = \"Multivariate MLP\"\n",
    "\n",
    "model = MultivariateNN(input_space_multi, output_space_multi)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "\n",
    "models_multi.append(model)\n",
    "model_descs_multi.append(model_desc)\n",
    "\n",
    "print(model_descs_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNI_INDEXES = [3190]  # 3190 for interpolation, 2897 for extrapolation\n",
    "MULTI_INDEXES = [index + 4 for index in UNI_INDEXES]\n",
    "EXCLUDED_MODELS = [\"AGP\", \"LR\"]\n",
    "X_RANGE = np.arange(2, 30, 1 / 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_descs_uni = [\"MLP\", \"FEMLP\", \"RDT\", \"LR\", \"AGP\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random.shuffle(field_indices_uni)\n",
    "\n",
    "dfs = [pd.DataFrame() for _ in UNI_INDEXES]\n",
    "tis = []\n",
    "cts = []\n",
    "\n",
    "for i, field_idx in enumerate(UNI_INDEXES):\n",
    "    plt.figure()\n",
    "    df = dfs[i]\n",
    "    df[\"x/D\"] = X_RANGE\n",
    "    init = True\n",
    "    # UNIVARIATE\n",
    "    for j, model in enumerate(models_uni):\n",
    "        model_desc = model_descs_uni[j]\n",
    "        if model_desc in EXCLUDED_MODELS:\n",
    "            continue\n",
    "        print(model_desc)\n",
    "        try:\n",
    "            ti, ct, _, wake_field, predicted_wake_field = (\n",
    "                test_dataset_uni.get_parameters_for_plotting_univariate(\n",
    "                    model, field_idx\n",
    "                )\n",
    "            )\n",
    "        except:\n",
    "            inputs, outputs = test_dataset_uni[\n",
    "                test_dataset_uni.slice_for_field(field_idx)\n",
    "            ]\n",
    "            inputs_fourier = rbf_sampler.transform(inputs)\n",
    "            ti, ct, ws, wake_field, predicted_wake_field = (\n",
    "                test_dataset_uni.get_parameters_for_plotting_univariate(\n",
    "                    model, field_idx, transformed_inputs=inputs_fourier\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # TODO move this stuff to plotting.py (?)\n",
    "        original_centerline = wake_field[:, 16:17].squeeze(1).detach().numpy()\n",
    "        predicted_centerline = (\n",
    "            predicted_wake_field[:, 16:17].squeeze(1).detach().numpy()\n",
    "        )\n",
    "        if init:\n",
    "            # plot original\n",
    "            # plt.plot(original_centerline, label=f'Original', color='black')\n",
    "            df[\"Ainslie\"] = original_centerline\n",
    "            tis.append(ti)\n",
    "            cts.append(ct)\n",
    "            init = False\n",
    "\n",
    "        # Plot predicted_centerline\n",
    "        # plt.plot(predicted_centerline, label=model_desc, color=MODEL_COLORS[j])\n",
    "        df[model_desc] = predicted_centerline\n",
    "\n",
    "for i, idx in enumerate(MULTI_INDEXES):\n",
    "    # MULTIVARIATE\n",
    "    df = dfs[i]\n",
    "    # TODO move this stuff to plotting.py (?)\n",
    "    for j, model in enumerate(models_multi):\n",
    "        model_desc = model_descs_multi[j]\n",
    "        if model_desc.split(\" \")[1] in EXCLUDED_MODELS:\n",
    "            continue\n",
    "        print(model_desc)\n",
    "        ti, ct, _, wake_field, predicted_wake_field = (\n",
    "            test_dataset_multi.get_parameters_for_plotting_multivariate(model, idx)\n",
    "        )\n",
    "\n",
    "        wake_field = wake_field.reshape(32, 224).T\n",
    "        predicted_wake_field = predicted_wake_field.reshape(32, 224).T\n",
    "\n",
    "        predicted_centerline = (\n",
    "            predicted_wake_field[:, 16:17].squeeze(1).detach().numpy()\n",
    "        )\n",
    "        # plt.plot(predicted_centerline, label=model_desc, color=MODEL_COLORS[j])\n",
    "        df[model_desc] = predicted_centerline\n",
    "\n",
    "\"\"\"    \n",
    "plt.xlabel('x/D (centreline)')\n",
    "plt.ylabel('Wind Deficit')\n",
    "#plt.title(f\"TI={ti:.2f}, CT={ct:.2f}\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig(f\"{ti:.2f}-{ct:.2f}.png\", dpi=300)\n",
    "plt.close()\n",
    "\"\"\"\n",
    "\n",
    "assert len(dfs) == len(tis) == len(cts)\n",
    "\n",
    "for df, ti, ct in zip(dfs, tis, cts):\n",
    "    \"\"\"\n",
    "    df.plot(x='x/D', y=[\"Ainslie\", \"FEMLP\", \"RDT\", \"MLP\"],\n",
    "        xlabel='x/D (centreline)',\n",
    "        ylabel='Wind Deficit',\n",
    "        color=[\"black\"]+MODEL_COLORS,\n",
    "        kind='line',\n",
    "        title=f\"TI={ti:.2f}, CT={ct:.2f}\"\n",
    "    )\n",
    "    \"\"\"\n",
    "    df.to_excel(f\"{ti:.2f}-{ct:.2f}.xlsx\", index=False)\n",
    "\n",
    "    # new plot for the paper (attempt 1)\n",
    "    \"\"\"\n",
    "    plt.rcParams.update({'font.size': 15})\n",
    "    fig, ax1 = plt.subplots()\n",
    "    fig.set_size_inches(8, 5)\n",
    "\n",
    "    ax1.set_xlabel('x [D]')\n",
    "    ax1.set_ylabel('wind speed deficit')\n",
    "    ax1.plot(df[\"x/D\"], df[\"Ainslie\"], label='Ainslie', linestyle='dashed', color='black')\n",
    "    ax1.tick_params(axis='y') #???\n",
    "\n",
    "    ax2 = ax1.twinx() # second axes that shares the same x-axis\n",
    "\n",
    "    ax2.set_ylabel('relative error in wind speed deficit', color='tab:blue')\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "    ax2.plot(df[\"x/D\"], df[\"MLP\"] - df[\"Ainslie\"], label='MLP', color=MODEL_COLORS[0])\n",
    "    ax2.plot(df[\"x/D\"], df[\"FEMLP\"] - df[\"Ainslie\"], label='FEMLP', color=MODEL_COLORS[1])\n",
    "    ax2.plot(df[\"x/D\"], df[\"RDT\"] - df[\"Ainslie\"], label='RDT', color=MODEL_COLORS[2])\n",
    "\n",
    "    fig.legend(ax1.get_legend_handles_labels()[1] + ax2.get_legend_handles_labels()[1],\n",
    "               loc=\"upper right\",\n",
    "               bbox_to_anchor=(0.907, 0.89))\n",
    "\n",
    "    plt.show()\n",
    "    \"\"\"\n",
    "\n",
    "    # new plot for the paper (attempt 2)\n",
    "    plt.rcParams.update({\"font.size\": 18})\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(8, 5)\n",
    "\n",
    "    ax.set_xlabel(\"x [D]\")\n",
    "    ax.set_ylabel(\"error in wind speed deficit\")\n",
    "    ax.tick_params(axis=\"y\")\n",
    "    ax.plot(df[\"x/D\"], df[\"MLP\"] - df[\"Ainslie\"], label=\"MLP\", color=MODEL_COLORS[0])\n",
    "    ax.plot(\n",
    "        df[\"x/D\"], df[\"FEMLP\"] - df[\"Ainslie\"], label=\"FEMLP\", color=MODEL_COLORS[1]\n",
    "    )\n",
    "    ax.plot(df[\"x/D\"], df[\"RDT\"] - df[\"Ainslie\"], label=\"RDT\", color=MODEL_COLORS[2])\n",
    "    ax.axhline(0, color=\"black\", linestyle=\"--\")  # 0 line\n",
    "\n",
    "    plt.legend()\n",
    "    plt.savefig(\"centreline_errors.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "fig.legend(\n",
    "    ax1.get_legend_handles_labels()[1] + ax2.get_legend_handles_labels()[1],\n",
    "    loc=\"center right\",\n",
    "    bbox_to_anchor=(0.85, 0.5),\n",
    ")\n",
    "plt.xscale(\"log\", base=2)\n",
    "plt.show()\n",
    "plt.savefig(\"results_size-dependence.pdf\", format=\"pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # univariate\n",
    "    for model, model_desc in zip(models_uni, model_descs_uni):\n",
    "        print(model_desc)\n",
    "        # if univariate\n",
    "        for field_idx in UNI_INDEXES:\n",
    "            try:\n",
    "                ti, ct, ws, wake_field, predicted_wake_field = (\n",
    "                    test_dataset_uni.get_parameters_for_plotting_univariate(\n",
    "                        model, field_idx\n",
    "                    )\n",
    "                )\n",
    "                # print(ti, ct)\n",
    "                # break\n",
    "            except:\n",
    "                # break\n",
    "                inputs, outputs = test_dataset_uni[\n",
    "                    test_dataset_uni.slice_for_field(field_idx)\n",
    "                ]\n",
    "                inputs_fourier = rbf_sampler.transform(inputs)\n",
    "                ti, ct, ws, wake_field, predicted_wake_field = (\n",
    "                    test_dataset_uni.get_parameters_for_plotting_univariate(\n",
    "                        model, field_idx, transformed_inputs=inputs_fourier\n",
    "                    )\n",
    "                )\n",
    "            plotting.plot_maps(\n",
    "                test_dataset_uni.X_grid,\n",
    "                test_dataset_uni.Y_grid,\n",
    "                wake_field,\n",
    "                predicted_wake_field,\n",
    "                ti,\n",
    "                ct,\n",
    "                ws,\n",
    "                error_to_plot=\"absolute\",\n",
    "            )\n",
    "            folder = f\"{ti:.2f}-{ct:.2f}/\"\n",
    "            if not os.path.exists(folder):\n",
    "                os.makedirs(folder)\n",
    "            filepath = os.path.join(folder, model_desc.split(\" - \")[0] + \".png\")\n",
    "            plotting.save_cut_maps(\n",
    "                test_dataset_uni.X_grid,\n",
    "                test_dataset_uni.Y_grid,\n",
    "                wake_field,\n",
    "                predicted_wake_field,\n",
    "                ti,\n",
    "                ct,\n",
    "                ws,\n",
    "                error_to_plot=\"absolute\",\n",
    "                filepath=filepath,\n",
    "            )\n",
    "\n",
    "    # multivariate\n",
    "    for model, model_desc in zip(models_multi, model_descs_multi):\n",
    "        print(model_desc)\n",
    "        for idx in MULTI_INDEXES:\n",
    "            ti, ct, ws, wake_field, predicted_wake_field = (\n",
    "                test_dataset_multi.get_parameters_for_plotting_multivariate(model, idx)\n",
    "            )\n",
    "            # print(ti, ct)\n",
    "            # break\n",
    "            plotting.plot_maps(\n",
    "                test_dataset_multi.X_grid,\n",
    "                test_dataset_multi.Y_grid,\n",
    "                wake_field,\n",
    "                predicted_wake_field,\n",
    "                ti,\n",
    "                ct,\n",
    "                ws,\n",
    "                error_to_plot=\"absolute\",\n",
    "            )\n",
    "            folder = f\"{ti:.2f}-{ct:.2f}/\"\n",
    "            if not os.path.exists(folder):\n",
    "                os.makedirs(folder)\n",
    "            filepath = os.path.join(folder, model_desc.split(\" - \")[0] + \".png\")\n",
    "            plotting.save_cut_maps(\n",
    "                test_dataset_multi.X_grid,\n",
    "                test_dataset_multi.Y_grid,\n",
    "                wake_field,\n",
    "                predicted_wake_field,\n",
    "                ti,\n",
    "                ct,\n",
    "                ws,\n",
    "                error_to_plot=\"absolute\",\n",
    "                filepath=filepath,\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
